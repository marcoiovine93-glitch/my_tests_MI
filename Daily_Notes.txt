Run job on boost partition (for GPU computing):
- insert lines in the job:
#SBATCH --account=ICT25_MHPC_0
#SBATCH --partition=boost_usr_prod
#SBATCH --gres=gpu:1
#SBATCH --cpus-per-task=1 // This is the number of threads per GPU, in the case of the GPUs!!
#SBATCH --ntasks-per-node=1 // Number of GPUs per node!!

Run job from debug node (partition?):
- add line to the script: #SBATCH --qos=boost_qos_db



Profiling with Nsys:
- run nsys: nsys-ui <name_of_the_file>
- how to see timings from Nsys: 
    - open --> then go to the menu (listview) on the central left --> Stats System View --> NVTX Range Summary
        - in this way we can compare the timings of the section on which we did the rodiling
        - profiling in the code:
            - header in code: #include <nvToolsExt.h>
            - how to include section in a profiling:
                nvtxRangePush("Copy matrix 1");
                ...code....
                nvtxRangePop();

- modules:
    - module load nvhpc (only for gpus??)
    - compile: nvcc exerc3_transpose.cu -arch=sm_80 -lnvToolsExt (we specify the architecture only for the gpu case???
      run with profiling: nsys profile --trace=cuda,nvtx ./a.out  (WE NEED CUDA ONLY FOR GPUS!!!)



- How to compile transpose matrix GPU code and RUN and PROFILING :
   module purge
   module load nvhpc
 
   nvcc exerc3_transpose.cu -arch=sm_80 -lnvToolsExt
 
   nsys profile --trace=cuda,nvtx ./a.out
   
   code headers:
   #include <cuda_runtime.h>
   #include <nvToolsExt.h>



- How to compile matrix multiplication GPU code:
    module load nvhpc
    module load hpcx-mpi
    module load cuda/12.2

    mpic++ -I./include -L/leonardo/prod/spack/06/install/0.22/linux-rhel8-icelake/gcc-8.5.0/nvhpc-24.5-torlmnyzcexnrs6pq4cccabv7ehkv3xy/Linux_x86_64/24.5    /math_libs/lib64/ -o matrix.x src/main.cpp -lcublas -lcudart

    srun ./code.x
  
    code headers:
    #include <cublas_v2.h>
    #include <cuda_runtime.h>




- How to compile jacobi (not sure it's correct) GPU code:
    module load nvhpc
    module load hpcx-mpi
  
    nvc++ -o exe -I./include src/main.cpp -acc -gpu=cc80,cuda12.4 -Minfo=acc -lcudart -lnvToolsExt
  
    code headers: 
    #include <openacc.h> //We include the header corresponding to openacc.
    #include <cuda_runtime.h>



- NO BLOCK condition for transpose code:
    - the threads tend to access the same bank in order to have chunks of 32 elements each(warp dimension)
    - REMEMBER!!: the warps have a number of elements (chunks of data accessed by threads) equal to 32!!!!!
    - to make the threads to not access the same data (there is this risk) and generate a latency in order to fullfill the warp, 
      we add an element to the array, even if it is not corresponding to the physical domain points --> in this way, we allocate more memory, 
      but the threads are able to access the data without trying to overwrite the same memory destination because there is more "space" in 
      memory that has been allocated?????


- GPU course (P1.7): why the shared memory access is faster than the global (default) one? Because in this way we make the threads access
                     the data shared in the same Block (we have, for GPUs, blocks containing threads). In GPUS, each Block of threads has 
                     a small shared memory. The ACCESS TO THIS MEMORY IS REALLY FAST --> THIS IS THE REASON WHY IT IS CONVENIENT TO USE SHARED
                     MEMORY APPROACH!!!!

- GPU course: why we need to do a Synchronize for the threads? If we don't do it, the Timer retrieves a really small time, why?
              - NOO!!! IN THIS CASE WE ARE CPNSIDERING MESSAGE PASSING, SO RANKS (NO MEMORY SHARED SYSTEM --> DISTRIBUTED ONE), WE DON'T HAVE THREADS!!!!!!
              - IT IS CORRECT TO SAY THAT WE NEED TO SNCHRONIZE THE DEVICES, WHY?
                  - If we compute the Timing without synchronizing, 
              - NOOOOOOOOOOOOOOOOOOO!!!! : This is totally wrong!!!!!
                - IN GPU COMPUTING (CUDA) THE THREADS ARE THE GPU THREADS!!! SO WE HAVE MULTIPLE GPUS --> FOR EACH GPU WE DEFINE A NUMBER OF THREADS
                  IN ORDER TO HAVE CHUNKS OF THREADS EQUAL TO 32 (USUAL SIZE OF A WARP OF THREADS) --> SINCE WE HAVE MULTIPLE GPUS PROCESSOR,
                  WE PROCEED IN THIS WAY (TO BE AS MUCH AS POSSIBLE IN LINE WITH THE HARDWARE): 
                    - WE SET A CERTAIN NUMBER OF RANKS WITH MPI: FOR EACH OF THEM WE SEND TO DEVICE --> WE DO THE COMPUTATION ON GPU --> WE RESEND
                      TO THE HOST --> WE ARRANGE THE RESULTS TO GET THE FINAL RESULT AND PRINT FROM A CPU RANK!!
                - SO IN GPU COMPUTING WE TALK IN GENERAL ABOUT THREADS!!!
                    - SO IT IS CORRECT TO ADFIRM THAT WE SYNCHRONIZE THREADS!!!
                    - WE DO A: cudaDeviceSynchronize() : 
                        - THIS FUNCTION STOPS THE CPU INSTRUCTION EXECUTION UNTIL THE GPU COMPUTATIONS ARE EFFECTIVELY TERMINATED
                          (BECAUSE KERNEL CREATION BY HOST(CPU) ARE NON-BLOCKING BY DEFAULT!!!)
                        - THIS IS IMPORTANT BECAUSE IN THIS CASE, WITHOUT INCLUDING IT, WE HAVE THAT THE ONLY TIME MEASURED IS THE TIME NEEDED 
                          TO COPY DATA FROM CPU TO GPU?????


- REMEMBER!!!!!: In Python is not possible to perform Shared memory parallelism!!!
    - IT IS NOT POSSIBLE TO INTRODUCE THREADS!!!



THINGS THAT I WANT TO KNOW:
- Cannon method for matrix multiplication
- How to make variables private in C++ without having compilation issues?
- FFT AND WHY THE AllToAll (Course P1.5)
- how to make Hdf5 work? (Course P1.5)
- how to make Jacobi work on GPUs with OpenACC??
    - consider that when we do the Memcopy(CUDA), we essentially need at first to allocate on the Device
    - this happens also in the Pragma structured region, but:
    - in a Pragma structured region OpenAcc authomaticly makes copy of the host buffers "calling" them in the same way of the host buffers!!!
    - so we don't need to introduce new device buffers!!!!
    - In a structured pragma region, OpenAcc at first copies data from Host to Device, but inside the pragma region we can refer to both the 
      host and device buffers!!!
    - at the closure of the structured data region, OpenAcc copies data from device to Host, BUT IS IMPORTANT TO OBSERVE THAT INSIDE THE DATA
      REGION WE CAN REFER TO EITHER HOST OR DEVICE BUFFERS!!!

- Balancing of the tree in Fortran
- Iterator in C++
- How to implement OpenAcc for Thermal bubble program (course 1.8)??
- How to make a module for NetCdf4?? (Thermal Bubble code for course 1.9)?
- How to compile with Intel on Leonardo?? 



- REMEMBER THE DIFFERENCE IN ARCHITECTURE between THE BOOST AND THE dcgp (CPU) architectures!!!!!
    - THEY HAVE DIFFERENT NUMA REGIONS!!!
        - dcgp : 2 sockets, each socket: 4 NUMA regions, each NUMA region: 14 cores (cpus per Task) !!!!
        - boost partition: cpus per task = 8, number of gres (gpus) = 4
            - number of gpu threads per block = 8 x 4 --> size of a wrap, which is: 32!!! (coincident with the size indicated on the 
              architecure Data Sheet!!!)




- COMPILERS MUST HAVE NO SECRETS!!!!!!!!!!!!!!!!!
    - WHY THE FLAGS? WHY THE PREPROCESSING DIRECTIVES???
    - WHY THE LINKING? 


- ASSEMBLY LANGUAGE!!!




- Shallow copy: 
    - what is a shallow copy? 
      Let's do an example: I have an array A and an array B
      - A will correspond to a certain memory address, the same can be said for B
      - I make B equal to A at some point.
      - Then I make a change to A --> what happens to B??? --> For a shallow copy, B will change as well, BECAUSE IT IS NOT A DEEP COPY!!!!
      - to make the copy fully work, we need a Copy Constructor, and usually also a Move Constructor!!!!






Architecture course:
    - how processor is intercating with external memory --> where we store program to be run!
    - we have in general the Von Neumann architecture: CPU with ALU and system and memory as a separated entity with respect to it!
    - we have the memory subsystem:
    - best performance: 
        - we have 2 different subsystem
        - the best speed is the lowest among the CPU and Memory!!!
        - we need to Load/store data into the machine --> if machine is fast, but the data are provided slowly, it will be slow!!
        - if memory is not able to provide enough amount of data to perform all the operations affordable by the CPU, the speed will be affected!
        - CPU: flop per second
        - Memory: Byte per second
        - How to define a single metric to estimate the peformance of a system?
            - Machine Balance (hardware): FLOP/s (peak) / Byte/s (peak) 
            - arithmetic intensity (software): OPerations/num. of bytes that algorithm needs to read/access to the memory.
            - if I is higher than MB : compute bound, otherwise: memory- bound !!!
            - given an algorithm , we can estimate alg intensity of it !!!
                - example: low arithmetic intensity:Lattice Boltzmann, high: Dense linear algebra (BLAS3 matrix multiplication)
            - The Roofline Model: visualize how algorithm is exected to behave in a single architecture
                - x-axis: computational intensity
                - y-axis: Attainable performance
                - vertical line: algorithm intensity set!!
                - we can try to have an extimation of the maximum performance we can achieve with the provided architecture!!
            - Example: 
                - inner product:
                        - inner product: 
                            - each iteration: 1 sum and 1 multiplication!! --> arithmetic complexity --> 2N FLOPS : we take into account only fl.
                              point operations!!
                            - memory accesses? : read one el. from a, one from b--> then we need to read 3 elements --> and write 1 element -->
                              we need to read 3N, write N elements 
                            - we calculate I: is is good? --> it depends on the machine!!
                            - Machine balance today is usually around 4!!
                            - so this computation will be memory bound for most of the machines!!
                            - between the principal memory and CPU there are different cache levels!!
                            - we can use Roofline also to compare different applications!!!
                            - FPGA: a lot of transistor not statically connected --> we can program them to perform the operation we want!!!
                            - we can optimize for either cpu performance or memory access!!!

            
            - Cache lines:
                - 2 subsystem: CPU, memory
                - external memory is slow (not into the processor --> distance, if we want a big memory, we need to address memory in some way
                                           we need system that searches where the element is stored in memory)
                                          - THIS IS IMPORTANT: WE NEED SOMETHING THAT DEFINES WHERE THE ELEMENT IS STORED IN MEMORY!!
                - memory access are random?
                    - how to speed-up process of accessing data?
                    - in general access is not random!
                    - in close amount of data we access multiple data:
                        - if we read a book multiple times, we keep it close to where is is often used
                        - we could store the small piece of data that we need!
                        - we have also a spacial locality, not only temporal locality
                        - we can try to exploit the pattern, we cannot store  all the matrix in the cache, because to have a faster 
                          memory close to cpu is more expensive than a memory external
                          (IT IS NOT POSSIBLE TO HAVE A HUGE MEMEORY CLOSE TO THE CPU FOR MONEY REASONS AND FOR MANIFACT, REASONS)
                        - caches are of standard sizes!
                        spacial locality:
                            - if we want to access buffers bigger than the vector?
                                - if we access often to data,we can try to read from extern. memory a sequence of element instead of a single element
                                - we assume that the other elements willl be used later on 
                                  (OBVIOUSLY WE CANNOT BE SURE THEY WILL BE NEEDED LATER)--> elements will be stored in the Cache
                            - the memory in cache is organized in cache lines
                        - when we read from the external memory, how we decide where to store in the cache memory?
                            - depends:
                                - direct mapping: we use a part of the external memory address
                                - page size: 4 kbytes
                        - if we store a given piece of data into cache line, if we want to read data that should be in the same place
                          How can we know if in the cache line there is a data or the other one?? 
                          - We introduce an index, with which we can check if the piece of data is in the cache line or not
                            - if data is in the line : cache hit, otherwise: cache miss
                                - THE CACHE MISS OR CACHE HIT IS REFERRED TO THE CHECK OF THE PRESENCEOF THE DATA IS IN THE CACHE LINE OR NOT!!!
                        - collisions: we are trying to store data where there is already data??? 
                        - Another approach to cache (Associative Cache Mapping):
                            - we don't have direct mapping, but we have Placement Policy
                            - we can choose where to store data into the Cache
                            - if we have one line filled, we can find the first line free and fill it with data
                                - drawback: how can we know where is data stored that we need? We need to go through all the data in the cache!!
                                - there is also other problem: if the cache is full how can I decide what line free??
                                - Associative caches: Good enough policies: Hot Potato:
                                    - The called cache line passes the hot potato to the next line
                                    - we start to eviction only when it is full
                                    - So THE HOT POTATO WILL BE ASSIGNED STATISTICALLY TO THE CACHE LINE CALLED LESS OFTEN
                                    - drawback: expensive in hardware, as many comparisons as cache lines

                        - 2 Approaches: 
                            - Direct mapping
                            - Associative cache mapping
                        - we introduce something in the middle:
                            - Set associative cache
                            - we can find the worst case: should not happen frequently
                        - We have several cache levels (usually 3)
                            - we have different machines, they can have really different cache configurations:
                                - certain caches can be shared only between certain cores 
                                - running the machine, we can check the architecture/design of the memory

                        - Pages:
                            - we move one page of memory to the hard drive (OS may decide to move one piece of memory to the hard drive)
                            - this happens under the hood, we don't know details and we don't have control on it (???) bout it
                            - why is it important to have data in more pages? We will see tomorrow.
                            - accessing the data from different pages can affect data and performance!!!!
                        
                        - MemHierarchy exercise:
                            - command : papi_mem_info (after having run the executable):
                                - let us to check the boundary from which we have the next memory location (L1, L2, L3 caches, and the DRAM memory)
                                - the number near "Associativity" is the number of bytes stored in a single Cache line: is the 
                                - line size: cache line size
                                - number of lines
                                - associativity: number of sets --> columns!!! Is it the number of sets (groups of columns) or the column 
                                  index in a certain set??


                        - How the cache behaves when we try to write something into the memory??
                            - what may happen if we try to modify that is in the cache line?

            

            Single Pocessor:
                - we have many steps when running a set of instructions
                - Resources: - memories that are not the registers: memory where we store the different instructions (example)
                                - difference with the register: is bigger--> slower to access
                                - we usually read one instruction per time --> more space required
                             - ALU
                             - Address generation unit: it is an ALU, but it is dedicated to memory generation task
                             - Single cycle execution:
                                - one instruction per clock cycle --> but to execute an instruction --> many steps 
                                    - steps coukld be longer or shorter
                                    - some instruct. perfectly fits in the clock cycle, other that need to wait 
                                        - pipelined execution: we split extraction in several steps
                                            - Instruction level parallelism:
                                                -  for every clock cycle a piece of hardware executes a small istruction
                                                - so several instruction at the same time: we cannot control (only suddenly)
                                                - it happens inside the processor
                                                - latency point of view: not positive:
                                                    - first result is get after a certain n of clock cycle: no benefit
                                                    - benefit: we can increase the clock frequency and the throughput:
                                                        - because after a certain number of clocks we get a lot of results
                                                        - so it is good if we have a lot of instruction to be executed!
                             - Problems with Pipeline:
                                - during the clock cycle we cannot use the same resource
                                - if 2 stages need to access the same memory, problem
                                - Data hazards: 
                                    - control/sequence hazards:
                                        - branch: we don't have yet the result because we are in a branch --> we need a way to throw away 
                                          what is being executed in the pipeline --> we need to solve the hazard!
                                          - Stall the pipeline (a Bubble) : if some instruct. depending on the instr. exec. before the 
                                            required instruction are executed --> we introduce a stall which will last a number of clock 
                                            cycle needed to make the required ijnstruction to provide the needed results for the current
                                            instruction!!!
                                            (we pause the pipeline)
                                    - Bypassing:
                                        - another way to solve hazards:
                                            - reduce stalls
                                    - Branches prediction: if we, after knowing the results, see the prediction is correct, we keep the branch
                                                           otherwise, we throw it away!
                                        - Speculative execution:
                                           - whenwe have instruction with condition,he starts to execute instruction and in case is wrong throw away
                                    - Why I cannot read/load 2 elements at the same time? --> higher complexity!!
                                        - I can have a pipeline for load/store, another for ALU, another for multiplication (more complexity)
                                        - if I need to access the outside memory, sometimes I don't know how much clock cycles I need
                                    - Out of order execution:
                                        - alter the order of execution of instructions: Dynamical scheduling
                                        - problems: change order --> write after write and write after read can be a problem (WHY??)
                                        - we use different registers (register renaming) 
                                    - If we add possibility Out of order and Speculative execution:
                                        - higher complexity
                                        - we keep the result in a register and we register or throw away depending on the result???
                            
                            - Modern processors:
                                - Intel Skylake (single core): yellow: ALUs
                                    - different warps inside alus are the different operations that can be done
                                    - theoretical Floating Point peak:
                                        - how to calculate: 


                            - Software techniques to perform instruction level parallelism:
                                - COMPILERS ARE REALLY GOOD AT THIS!!Software techniques to perform instruction level parallelism:
                                    - COMPILERS ARE REALLY GOOD AT THIS!!!!
                                    - avoid to have sequence of instruction where we have instructions depending from each other
                                    - try to have one single loop (not nested loops)!
                                    - avoid dependencies inside a single loop!
                                    - compiler can do loop unrolling
                                    - 


            Data parallelism:
                - Increasing single-core performance (vector operations): 
                    - vector of different items --> computational units able to perform operations on more elements at the same time
                    - so we can define vector instructions
                    - how data is managed
                    - example: same operation over different arrays
                    - for more iterations we can have similar instructions/operations
                        - we can try to merge and put together the same operation on different data
                        - one instruction on 2 elements of data
                        - drawback: if we want to perform operation on 2 elements for example: we need 2 ALUS (multiple hardware to work)
                        - newer processor: instructions can work on 8 or 16 elements arrays
                        - GPUs: they have long vector instructions (32 or more elements)
                        - the throughput: one instrcution per clock, but one instruction is divided into several tasks --> we have pipelines anyway
                        - the operation must be the same!!
                        - SIMD: single instruction, multiple data
                            - or Array processor --> but there is a difference between vector and array processor
                            - vector processor: - since having multiple Alus is costly, he keep one ALU and we move data into the Alus
                                - we don't need a for loop to control, but we think about a sequence of data put in the machine 
                                - we process one element at a time
                                - of course we have a vector register
                                - Alu processes one element at a time
                                - But if we have loops smaller than the defined one, we need control to stop the process!
                                - One element made of 2 items (it is a kind of this ...)
                                - We can consider registers with 2 or 4 elements --> depend on the architecture
                                - If we want to have a vector instruction, we want a vector loaded with elements closed in memory
                                    - but we will load an entire cache line into the cache (64 bytes) 
                                    - we are reading more elements than needed (for scalar CPU) --> we copy a chunk for each elements(for stride)
                                        - then we need to load data into a vector to perform for example the matrix multiplication
                                        - so the lack of performance is related also to the many operations to prepare elements to perform
                                          operations!
                                        - Virtual memory: a page is always multiple of a chache line : cannot happen that a part of line is in 
                                                          a page and another in different page
                                - Modern SIMD: Today vector and array computing are merged
                                - Cores of Coka: AVX2 (256 bits long)
                        - SO WE HAVE VECTOR INSTRUCTIONS (COMPUTATIONS AT THE SAME TIME) AND VECTOR REGISTERS
                        - GPU faster if we have a lot of data and we need to do the same operation on it            
                    - Vector arithmetic instructions:
                        - Reductions:
                            - we operate a vector on itself
                            - memory access: 
                                - HOW WE STORE DATA INTO MEMORY
                                    - IF WE HAVE STRUCT WITH X AND Y: IF WE HAVE TO ACCESS X ELEMENTS AND THEN Y ELEMENTS, WE ARE OBLIGED TO
                                      JUMP ONE MEMORY TO ANOTHER (STRIDED ACCESS --> REALLY BAD!!!!)
                                    - ALTERNATIVE: STRUCT OF ARRAYS --> MUCH MORE EFFICIENT!!!
                                    - INDEXED ACCESS: ARRAY FILLED WITH ADDRESSES USED FOR INDEXING ANOTHER ARRAY
                                        - VERY DIFFICULT FOR THE COMPILER TO UNDERSTAND THE CONTENT AND CANNOT PREDICT, PREFETCH, OPTIMIZE
                                        - AVOID ALWAYS!!!!
                                        - this can happen when we access to an object (C++) this happens --> we should avoid it!
                                - Conditionals in loops:
                                    - Vector masking:
                                        - evaluates conditions : conditional mask --> it computes both operations --> then the compiler merges
                                        - thr mask is created at run time
                                        - by computing the mask, it is avoided to havr branches!!
                                        - it does make sense if we have vector registers!!
                                        - there are cases where the compiler decides to not use vector instructions
                                        - avoid having conditionals inside loops
                                        - masks can be implemented in different ways:
                    - Limitations of SIMD:
                        - we cannot vectorize function calls
                            - we could inline the function calls??
                        - we need long loops to take advantage of it
                        - They are mixture Array computing and Vector computing!!!
                   
                   EXAMPLEs:
                    - Each p. of the grid contains 37 values (37 parameters)
                    - Lattice-Boltzmann simulation
                    - 2 functions: 1 compute bound and one memory bound
                    - for each point, 37 values for the state -->struct of 37 elements
                    - Matrix multiplication:
                        - we could try :- if we have large matrices, is happening that we need to read elements multiple times
                                        - small matrices: kept into the cache
                                        - big matrices (bigger than cache): sooner or later we will read element that will evict an element
                                            - to avoid this, what we can do?: cache blocking
                                                - we can decide a block dimension
                                                - we select a block and start to do computation just on that block
                                                - so we need to read blocks of the whole matrix
                                                - choose size in order to keep data into the cache memory
                                                - when we compute all the elements, we need to move to another block
                                                - again we will fill the caches and go on
                                                - we can try if is it possible to vectorize, or to implement a fma on the code
                                                - script to run the code multiple times to change the block size
                                                    - we don't know what is the best block size --> depends on the hardware
                                                    - we can at first find the best block size --> then we add compiler flags --> then we change the size another time
                                                    - we assume block size is a square
                                                    - 3 outer loops (through the dimensions)
                                                    - they're jumping one block at a time
                                                    - perf stat -a --per-core -e dTLB-load-misses ./main.c : 
                                                      - with -e we can set the metrics
                                                      - -a --per-core stands for the specific cores
                                                      - list of the different event that can be collected: perf list
                                                        - list of the metrics
                                                        - we can check how the metrics change with for example the block size or the matrix size 
                                                        - we can also check characteristics also by running PAPI:
                                                            - papi_mem_info
                    

                    - Shared memory:
                        - write and read order: depends on the order of operations and from the hardware point of view a lot of things can happen
                        - we can avoid things to happen from a software point of view (for example doing a synchronization)
                        - Difference between process and threads: both are software things
                                                                 - one process has its own memory space (from  a software p. of view)
                                                                    - from hardware p. of view can be shared resources 
                                                                 - thread:
                                                                    - if threads are running on symmetrycal multi processor, threads, in a given 
                                                                      array, can read what another thread wrote
                                                                 - we want to map the software concepts into the hardware, so we want to close
                                                                   from a logic point of view bind a MPI rank in a process, and then spawn
                                                                   a certain num. of threads. So we force the rank to access only the physical
                                                                   memory directly linked to physical process
                                                                   - we need to know which thread is running where
                                                                   - what is running where?
                                                                   - openmp: 2 environm. variables to Close or Spread: 
                                                                       - to check on which cores are running
                                                                       - when are they useful?
                    - Distributed memory:
                        - process = rank
                        - we need a way to make the processes, running on system with different memories, to interact with each other
                        - best way is to have one rank for each process --> then I spawn among threads to use the L3 shared memory


                    - Profiing tool:    
                        - if I want to collect cache miss of one thread, or for a single line?
                        - class of tools:
                            - profilers: intercepting events of our application
                            - made of 2 applications:
                                - extrae: 
                                - to view things: paraver --> files .prv : we copy files locally and we open them!


        - ATTENTION!!!!: HOW WE CALCULATE THE BANDWIDTH???? (Bytes per second) --> here we are referring to the effective data accessed, not 
          to the data sheet of the DRAM Bytes per second (which shoul be anyway the maximum admitted bandwith)
            - For a theoretical estimation we can refer to the calculation linked to the block size (edge), in particular it will be computed 
              as N^2 * (size of Double precision number (8 bytes)) * number of accesses to memory (in this case 3 reads and 1 writing) and 
              then we should multiply for N divided by the block size (WHY??????)
            - For an experimental estimation, we can use Perf:
                - we run per stat -d executable and we get the cache misses for each memory level. What we can observe is that 
                  in this case the cache misses for the last cache level (L3) are really smaller than the cache misses for L1
                - When we have a cache miss, it means that the system must access to data to retrieve it and make it available again
                - So the last level cache misses represent the data (band width) exchanged with the DRAM, while the L1 cache misses 
                  represent the bandwidth between L1 and L2. 
                - In spite of the standard analysis being referred to the DRAM bandwidth, here we calculate both DRAM and L1 to L2 Bandwidth
                - To do this, we multiply the size of the cache misses (L1 or ultimate cache level) for the size of the cache line 
                  (BECAUSE THE CACHE MISSES REPRESENT THE NUMBER OF CACHE LINE TO BE CALLED FOR DATA TO MAKE DATA AVAILABLE) and 
                  divide for the elapsed time!!!





Python Course:
    - Decorator:
        - it is important to have a return for the wrapper
        - it is important to have a return for each "Decorating action", like for example the printing
    - conda: python package manager (things already installed)
    - miniconda: better than anaconda: google how to do it
    - run conda init --> check the .bashrc --> strange stuffs --> when we open a new terminal, we see a new header for shell
      --> SO WHEN WE NEED TO RUN A COMPLICATED CMAKE OR MAKEFILE, IT'S BETTER TO DO: 
        conda deactivate
    - we create different environments
    - to check environments: conda env list
    - to install: conda install numpy (example for numpy)
    - to see packages: conda list
    - pip : pure Python --> conda brings the binary there are on different location, while pip takes more time
        - we use it on cluster
    - to install things: conda install -c conda-forge
    - we will use jupyter notebook
        - we need to use the one created for the environment we created?
    - Jupyter notebooks: good only for teaching (we don't want them)
    - we don't define data types (it decides)
    - there are also complex numbers
    - it is dynamically typed:
        - for python we could put at first string then integer --> it doesn't complain --> don't rely too much on this freedom!
    - no limit for integer:
        - where is it stored? --> complicated way (not efficient)
    - it is not github friendly
    - strings: we can concatenate with "+"
        - be careful with the assignments!!
    - function: pass by "association" --> it deciding if it calls by value or by reference
    - help function for built in objects
    - dir('class') : tells us which methods are available for a given class
    - blocks: done with indentation
    - loops: in range --> starts by default at 0
    - containers:
    - we can slice with containers: ls[2:7:2]  --> print from 2 to 7 with 2 step
        - -1 in the range --> we get a reverse
    - DON'T USE LIST OF LISTS!!!
        - bad idea, because we are not sure of how much elements we are changing
    - List: We can directly print for certain values of a variable 
            - sort function
            - deleting by index: del
            - deleting by value: remove
    - Sets:
        - each element is unique
        - mathematic operations
    - Tuples:
        - they are not the C++ tuples!!! ---> In C++ we used it for recursion!!!????
        - they are just like lists, but they are not changeable and are treated more fastly!!!
    - Commands that start with %: called Magic ----> ONLY WITH JUPITER NOTEBOOK???
        - important: timer:
            - %% : some strange results!!
            - times just the cell of the jupyther notebook!
    - TO STOP WRONG CODE:
        - go on Kernel --> stop Kernel --> correct --> Run all cells
    - Function:
        def name():  --> pass by value or reference depends on the passed type!!!
        - we can know it with help, or with: variable?
        - Interactive python
        - recursion
    - Errors: it is not easy to debug!!!
    - Lambda functions: we don't name a function
    - Exercises:
        - we can put the following: (we make sure that only from our location we can run the program??
          if __name == "__main__":
            #Call function
    
    - Class: 
        - we need to put self as first argument ---> WE NEED TO DO THIS FOR EVERYYY METHOD DEFINED FOR THE CLASS!!!
        - we can create class instance 
        - constructor already created for us 
        - dot notation to call class method
        - constructor: def __init__(self, )
        - assignment is not a copy!!!
        - SO WE USE OVERLOADING OF OPERATORS???
    
    - Modules:
        - import name of the module as <nameOfTheModule>
        - if we need only one function: from numpy import cos
        - to create one:
            - I can import it !!

    - Pandas
        - we will see it in the Machine Learning course

    
    - MPI 
        - not needed to do Init and Finalize
        - cluster: we cannot do conda install
            - instruction --> pip install and create mpi
            - ATTENTION!!!!!!!!! --> before installing pip, it is necessary to load openmpi --> Otherwise it won't work!!!
            - to install pip with no cache (on cluster):
                pip install --no-binary=mpi4py --no-cache-dir --force-reinstall mpi4py
            - be sure, when creating a new New Environment, to install the needed packages (like numpy, for example)
            - test that it works
            - so we need:
                - serial jacobi
                - serial game of life
                    - we need the serial versions because 
                - parallel of one of them
            - 
            
    - Pybind:
        - call some parts from another language??
        - in Python we don't have templates!!! --> It is more correct to state that we don't have Template functions or Template variables!!!



    - Numba:
        - nvhpc
        - we need nvidia
        - create new environment (at first deactivate the current one)
        - install pip
        - load nvhpc, cuda, what we used on gpus
        - jit: we are telling to compile things like c functions
        - we don't have control on it
        - timing: the second time we measure the true time??
        - jit parallel: parallel = true --> similar to OpenMp
        - we can use cuda with jit
        - WARNING !!!!: WE WERE NOT ABLE TO RUN NUMBA WITH POETRY LIBRARY (NUMERICAL ANALYSIS COURSE) --> WHY??        
    

    - MPI4PY INSTALLATION ON CLUSTER:
        - Why we install mpi4py with pip and not with conda?
            - essentially on a local machine we have mpi4py binary --> this binary is overwritten by conda and it is ok because the way conda installs
          is in line with the general laptops architecture
            - On cluster, we have a really specific architecture, so mpi4py which is on cluster, has a different configuration 
                - IN ORDER TO AVOID IT TO BE OVERWRITTEN, WE INSTALL WITH PIP --> WHEN PIP SEES A binary, it is able to recognize it and 
              it doesn't overwrite it
                - Risks:
                    - the best thing that can happen is that MPI doesn't work
                    - worst: it seems to work --> but it is slow and has bugs and issues!!!

        - RUN on Cluster for Parallel Jit (Multi-thread)
            - WE MUST CONFIGURE CPU-PER-TASK LIKE FOR OPEN MP!!!!!!!!
            - export THREADS PER NUMBA!!!!
            - remember that the outer loop where we put the @njit is the loop where we need to put the prange!!!!!!
            
    
    - REMEMBER !!! : 
        - THE WAY WE IMPLEMENTED THE CODE IN PYTHON DOES NOT MAKE ANY SENSE AT ALL!!
        - WE SHOULD HAVE BEEN USED 2 DIMENSIONAL ARRAYS INSTEAD !!!


    - Pybind:
        - it let us to introduce an interface to run C++ programs through Python code
            - what we do is to call directly the functions defined in the C++ scripts
            - in fact we introduce direct calls to:
                - C++ functions 
                - C++ Classes (with the corresponding functions as well)
            - How to run it?
                - what we do is to generate a sort of library originating from the "Interface C++ file":
                    - technically what we are planning to do is a "BINDING" of the functions, and of the class functions as well, from C++ to Python
                    - we compile in a way that gives us the creation of a Library: 
                        - WE CREATE A  .so file:
                            a compiled library file type following the ELF (Executable and Linkable Format) structure. They're used by Linux and Unix                            -based operating system like Ubuntu, Debian, and CentOS, and they allow applications to share functionality by 
                            dynamically loading various features at runtime --> so it works essentially at runtime!!!






OpenAcc :
- It is based on directives
- ATTENTION!!!: the directives are taken into account at compile, or at run time???
    - The directives are based on the #pragma acc (in C++) --> BUT ONE MOMENT: the Accelerators are applicable only for GPUs? Or also to CPUs?
                                                                               - it seems that it is applicable also to distributed comp. with
                                                                                 CPUs
    - ATTENTION!! : when we introduce a directive: #pragma acc parallel --> WE ARE NOT INTRODUCING ANY COMMAND RELATED TO PARALLELISL
        - WE ARE SIMPLY ALLOCATING MEMORY ON THE DEVICE (USUALLY THE GPU!!!!)(FROM THE HOST??)
        - IN ORDER TO PARALLELIZE A LOOP, TO MAKE IT EXECUTED BY DIFFERENT GANGS (THE GANGS ARE GROUPS OF THREADS) AT THE SAME TIME
          (BY MAKING COMPUTATION DISTRIBUTED FOR INSTANCE BETWEEN DIFFERENT PARTS OF THE MATRIX --> FOR MATRIX OPERATIONS/SOLVERS)
          WE NEED TO SPECIFY THE DIRECTIVE: loop !!!!
        - OF COURSE WE NEED TO CONSIDER, LIKE FOR OpenMP, that it is possible to MAKE THE VARIABLES "private", so to make data accessible by a proper
          thread and not from the other ones? --> NO!! EACH THREAD WILL HAVE ITS OWN COPY OF THE DATA TO ACCESS TO!!!!
        - Like OpenMP, it is possible to introduce the "Atomic" directive???? --> WE NEED TO RECALL THIS CONCEPT!!!
        - In general, when we introduce the #pragma acc directive for the loops in the code, what is done is an allocation of memory on the device,
          a copy of data from host to device, computation on the device, copy from host to device
            - we can make this more efficient by introducing STRUCTURED or UNSTRUCTURED data regions!!!
                - WHAT THE FUCK ARE THE DATA REGIONS??
                    - THEY ARE INTRODUCED TO AVOID TO : 
                        ALLOCATE-ON-DEVICE -> COPY DATA --> (AFTER COMPUTATION ON DEVICE) COPY TO HOST - FOR EACH #pragma acc
                        - we allocate and copy data just at the beginning and at the end of the Data Region (for STRUCTURED DATA REGIONS)
                        - For UNSTRUCTURED DATA REGIONS, WE FREE DEVICE MEMORY MANUALLY AND DELETE DATA ACCESSED FROM THE DEVICE AFTER THE COMPUT.
                             - IT IS LESS AUTHOMATIZED AND LET THE USER FOR INSTANCE TO COPY TO HOST BEFORE THE END OF THE DATA REGION!!!

- ATTENTION!!!!:
    - The data regions, delimited by the "{", "}" in C++, need to have these symbols after the line with the "pragma":
        - it is not possible to write: 
            #pragma acc ....... {
                NOOO!
            - correct way:
                #pragma acc ...
                {

- "present" statement:
    - it should be used to tell the compiler (?) THAT THE INDICATED DATA IS ALREADY ON THE DEVICE, AND THAT IT IS NOT NEEDED TO REALLOCATE
      MEMORY FOR DATA? BUT IS IT DONE AUTOMATICLY (BY DEFAULT) IN SOME CASES???










12/01/2026:
    NUMERICAL ANALYSIS COURSE:
        - Andrea Cangiani
        - Stefano Piani
        - program: - numeric diff equat.
                   - vector spaces, normes etc
                   - basic linear algebra
                   - iterative solution
                   - intro to Finite elements
                   - discretization of time dependent problems
        - Analysis:
            - to do scientific comouting: what we are doing??
            - code must do exactly what theory predicts!!
            - books: quarteroni and Mayers and Morton
                - Golub van Loan
                - Lloyd
        
        - Ordinary diff. equation: 
            - initial value problem --> assume uniqueness and existence of the solution
                - we discretize the time (subdivide in intervals)
                - we take into account the corresponding vector of discrete solution (for each time step)
                - Euler's method:
                    - Taylor formula:
                        - we remove the reminder --> we get the u(n+1) = u(n) + tau*f(tn, un)
                        - we set u' = f
                        - we get a divided difference
                - Convergence:
                    - and how fast is it converging??
                - Different derivation of Euler's method:
                    - by quadrature: integrating the ODE in the subinterval tn,tn+1]
                    - trapezoidal rule:
                        - is higher order method, but it requires the solution of a non-linear equation at every time step (WHY???) --> implicit scheme (WHY???)
                        - balance between costs and accuracy --> to decide what method to use
                            - implicit euler needs less steps to get a certain accuracy in the solution
                        - multistep: the current time solution doesn't depend only on the previous time step, but on also other previous steps!!!
                            - we usually use one step methods in order to get a sufficient number of solution needed for multi-step method!!
                - Various divided differences:
                    - what is the best for approximation??
                    - Divided difference error analysis:
                        - ERROR BOUNDS?????? (WHY????)
                        - they are a priori bounds (they depend on the function)
                        - how do we prove the formulas???
                            - on the notebook!!
                        - we can take also more time instants --> multi-step formulas!!!
                        - if we don't use simmetry (central schemes??) we need to ask for more evaluations to increase the order!!!
                        - Review the log log plot for the errors --> we should see that the 24 at the denominator makes the approximation better!!
                        - we can approximate the second derivative with a WHAT???? -->
                            - divided differences for higher derivatives:
                        
                 - First exercise:
                    - solution to -laplacian equal to f --> we will see why we add the minus
                    - python library:
                        - tools to manage how to build it
                                - folder: /dev/shm : poetry new <lib_name>
                                - ls -lh : I see the folder created 
                                    - I go inside:
                                        - pyproject : it is default created
                                            - name, version, our name, and some tools that can be used
                                        - install poetry : choose the curl one !!! --> - means that if the path is a minus, read from the default
                                            path !!!!
                                        - PROBLEM WITH poetry INSTALLATION:
                                            - WHEN TRYING TO RUN THE COMMANDS : poetry lock and poetry sync, I got the error related to Python:
                                                - [Errno 2] No such file or directory: 'python'
                                                - STEFANO SOLVED THIS BY INSTALLING PYTHON MAKING IT IN A WAY THAT Python3 must be seen as Python
                                                 (FOR SOME REASONS poetry wanted the folder to be named in this way):
                                                  - we run the command:
                                                    sudo apt install python-is-python3
                                        - PROBLEM WITH INSTALLATION:  
                                            - NOT ACTIVATE AND DEACTIVATE CONDA ENVIRONMENT BEFORE INSTALLING POETRY!!!!
                                            - IF YOU DID IT, TRY DO DO: onda config --set auto_activate_base false
                                                - THEN, UNISTALL POETRY, INSTALL AND SO ON!!! (PYLOCK AND PYSYNC)
                                        - poetry add numpy scipy --> poetry does : write in the dependency the packages, and it created a virtual
                                          env with my package in order to use it
                                        - poetry.lock -> it tells us what kind of packages we are using!
                                        - poetry helps to manage packages
                                        - test: small script to check the code works properly
                                            - small routine to check the code is working well
                                            - instead of throwing routine away, we can set tools to use it 
                                            - we use pytest
                                            - poetry sync --> ask to install the same virtual env of the file I blocked
                                            - poetry run ls --> I'm telling poetry to run on the virtual env that poetry created!!
                                            - poetry run pytest
                                            - Exercise:
                                                - is structured as a library (He already run poetry run for us)
                                                - function solve_poisson --> we need to implement it
                                                    - rhs: right hand side, Callable: it is a function --> in Python are first class citizen
                                                        - there is no difference between what we can do with a function or a variable
                                                        - we can pass a function as an argument for instance
                                                    - why we use solve_poisson
                                                        - we use it becausw the Bcond was hard coded --> in this situation we don't know how many 
                                                          points we will have on the boundary
                                                    - typing in python: we are telling: n_points is an integer for instance (it is like an integer)
                                                        - for instance boundary_condition is a dictionary
                                                    - Enumerator : it is introduced to make Side can assume only certain values!!
                                                    - Boundary conditions: are expected to be one-dimensional!!
                                                    - tuple: I want three arrays (WHAT ARE THEY??)
                                                    - What Pytest do?: it runs code with certain values!!
                                                    - IT IS VERY VERY SUBTLE HOW WE ENUMERATE THE NODES!!!!
                                                    - WE CAN DECIDE TO SET THE DIMENSION OF THE MATRIX AS 3X3 OR 5X5(REASONABLE CHOICE???)
                                                    - editor: use pysharm !!
                                                    - grid_x[0,0] : we can choose if it is the down_left or up_left, and if the first index stands
                                                      for x-axis or y-axis!!

                  - Second lesson (13/01/2026):
                        - Condition numbers and sparse matrices:
                            - if we add some convection to Poisson --> the tecnique is just again a linear system
                            - it could be necessary to solve multiple linear systems
                            - but it is always a matter of solving a linear system
                            - TRENT500: LINPAC --> computation is roughly a linear system!!
                            - CPU are designed to solve linear systems
                            - how do we solve linear system?
                            - how easy we do it?
                            - can a computer solve a linear system?
                                - we use floating point (I introduce an error)
                                - how many digits? 32 or 64 bits
                                    - how many digits is 32 bits? 6/7 digits
                                    - and 64? 16 digits
                                    - rappresentation error: if I put 1.00000001 in computer --> I get: 1 --> this is the concept of rappres. error?
                                    - 0.1 + 0.2 - 0.3 = 5.55e-17 !!!!!!!! 
                                        - floating point operations are independent on the language used!!!
                                        - why is not 0 ? it is a rappresentation error!
                                        - 0.1 = it is nice --> the numbers that are nice are the ones writable as number over power of 10
                                        - for example 1/3 is bad (periodical digits) --> rounding error!!!
                                        - these errors can grow and become even more complicated to manage
                                        - relative error: we compare the error to the initial error!! (it is a ratio that gives us the importanc eof the error!!
                                        - our computation can affect also vectors, not only scalars
                                        - given x, I compute sinx --> this is really well conditioned
                                        - well conditioned and ill conditioned problems !!!!


                            - Meshgrid function:
                                - 
                    
                    - Lecture 4:
                        - L-U decomposition: what are the limits of these methods?? Should we look other methods??
                            - if I have a big matrix which is not dense?
                                - computer needs to do a product and we can result in a fill in for example
                                - we cannot store a matrix really big --> too much big for storage
                                - so we need methods that can manage to solve problems with big matrix dimensions!!
                                - The alternative is by using iterative methods --> MADE OF A CERTAIN AMOUNT OF STEPS
                                    - we are solving a system which is symmetric and positive definite --> most simple case
                                    - we build a sequence of vectors such that the sequence converges to the solution
                                    -  
                    - Attention!!!:
                        - The Jacobi iterative method, as well as the Gauss Siedel, converge just if the matrices are strictly diagonally 
                          dominant!!! Otherwise we look the simulation run forever with really strange numbers come up !!!


                    - Convergence note:
                        - There is a huge and strange issue when trying to run the simulation of Jacobi with a matrix free approach (as we saw
                          during the course, this equivalent, with the exception of the right hand side of the equation, to solving the Jacobi 
                          seen during the previous courses!!
                        - The issue is the following:
                            - in the file CSolver.py, we have the class with a provided member function (jacoi_solver) which executes the iterations
                            - when we set a number of iterations as the stopping condition, we insert the error at each iteration as an element
                              of an allocated array of size equal to the n. of iterations
                            -
                          - The issue is the following:
                            - in the file CSolver.py, we have the class with a provided member function (jacoi_solver) which executes the iterations
                            - when we set a number of iterations as the stopping condition, we insert the error at each iteration as an element
                              of an allocated array of size equal to the n. of iterations
                                - in this case the error seems to decrease properly
                            - On the other hand, if I set the tolerance value as the stopping condition, and I introduce an evaluation of the error
                              for contiuning or not the iterative process, this time considering a scalar error recomputing through each iteration  
                                  - in this case the error seems to decrease properly
                            - On the other hand, if I set the tolerance value as the stopping condition, and I introduce an evaluation of the error
                              for contiuning or not the iterative process, this time considering a scalar error recomputing through each iteration
                              the process starts to go forever and, having the possibility to print the error at each step, we see that it
                              oscillates a lot between huge and really small numbers --> it seems that the evaluation of the condition gives a 
                              floating point problem
                            - HOW CAN WE SOLVE THIS ISSUE and WHY IS IT HAPPENING??? : 
                                - when I evaluate the error compared to the limit I set, I'm comparing a first estimate (for the first iterations) 
                                  which is quite huge with respect to the tolerance. As you noted, this error is in general affected by both the 
                                  order of the discretization (which depends on h) and on the numerical scheme,together with the boundary values 
                                  and so on. So comparing a huge number to a really small one, can result in floating point issues (lose of 
                                  precision) and strange oscillating behaviour. This comparison could make sense if we have for instance 
                                  we start from an initial guess that is not so great --> BUT ESSENTIALLY THIS SHOULD NOT CAUSE ANY ISSUE
                                  --> THE PROBLEM IS THAT, WE ARE COMPUTING A QUANTITY THAT IS NOT DECREASING MONOTHONICALLY THROUGH THE JACOBI
                                      ITERATION, BECAUSE IN GENERAL IS AFFECTED BY THE DISCRETIZATION ERROR, BOUNDARY ERRORS, INTERVAL LENGTH
                                      AND SO ON --> SO FOR ITERATIVE METHODS, DOES MAKE SENSE TO EVALUATE THE RESIDUAL, NOT THE COMPARISON 
                                      TO THE EXACT SOLUTION --> BECAUSE THE RESIDUAL WILL, OF COURSE DECREASE MONOTONICALLY!!!!!

                                                                                                                                

- Note on GIT:
    - to avoid possible issues when trying to pull new material to the main branch branch of our fork (which has been forked from a web Repo)
      it could be better to create a dev branch at beginning --> in this way we can avoid for exaple possible conflicts that happen when
      for example we modify resources that are then updated on the web Repo --> in that case the pull is forbidden, and could as well have 
      issue with the push --> in general we will have different commits locally and on the web repo --> otherwise with a proper branch,
      we will have the possibility to directly merge the developments that we do, with the new updated files on the web Repo!!!




- Tools to check for memory leaks and memory issues:
    - Not only Valgrid!!
    - we have also: Hellgrid, Ages Sanytizer
    - In C++ we don't have these problems, if we use vectors!! --> But C++ is more rich than C!!!
    - When choosing a programming language --> memory management and possible memory issues, compilation times!!!
    - C is a language simler semantically!!!







