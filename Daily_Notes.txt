Run job on boost partition (for GPU computing):
- insert lines in the job:
#SBATCH --account=ICT25_MHPC_0
#SBATCH --partition=boost_usr_prod
#SBATCH --gres=gpu:1
#SBATCH --cpus-per-task=1 // This is the number of threads per GPU, in the case of the GPUs!!
#SBATCH --ntasks-per-node=1 // Number of GPUs per node!!

Run job from debug node (partition?):
- add line to the script: #SBATCH --qos=boost_qos_db



Profiling with Nsys:
- run nsys: nsys-ui <name_of_the_file>
- how to see timings from Nsys: 
    - open --> then go to the menu (listview) on the central left --> Stats System View --> NVTX Range Summary
        - in this way we can compare the timings of the section on which we did the rodiling
        - profiling in the code:
            - header in code: #include <nvToolsExt.h>
            - how to include section in a profiling:
                nvtxRangePush("Copy matrix 1");
                ...code....
                nvtxRangePop();

- modules:
    - module load nvhpc (only for gpus??)
    - compile: nvcc exerc3_transpose.cu -arch=sm_80 -lnvToolsExt (we specify the architecture only for the gpu case???
      run with profiling: nsys profile --trace=cuda,nvtx ./a.out  (WE NEED CUDA ONLY FOR GPUS!!!)



- How to compile transpose matrix GPU code and RUN and PROFILING :
   module purge
   module load nvhpc
 
   nvcc exerc3_transpose.cu -arch=sm_80 -lnvToolsExt
 
   nsys profile --trace=cuda,nvtx ./a.out
   
   code headers:
   #include <cuda_runtime.h>
   #include <nvToolsExt.h>



- How to compile matrix multiplication GPU code:
    module load nvhpc
    module load hpcx-mpi
    module load cuda/12.2

    mpic++ -I./include -L/leonardo/prod/spack/06/install/0.22/linux-rhel8-icelake/gcc-8.5.0/nvhpc-24.5-torlmnyzcexnrs6pq4cccabv7ehkv3xy/Linux_x86_64/24.5    /math_libs/lib64/ -o matrix.x src/main.cpp -lcublas -lcudart

    srun ./code.x
  
    code headers:
    #include <cublas_v2.h>
    #include <cuda_runtime.h>




- How to compile jacobi (not sure it's correct) GPU code:
    module load nvhpc
    module load hpcx-mpi
  
    nvc++ -o exe -I./include src/main.cpp -acc -gpu=cc80,cuda12.4 -Minfo=acc -lcudart -lnvToolsExt
  
    code headers: 
    #include <openacc.h> //We include the header corresponding to openacc.
    #include <cuda_runtime.h>



- NO BLOCK condition for transpose code:
    - the threads tend to access the same bank in order to have chunks of 32 elements each(warp dimension)
    - REMEMBER!!: the warps have a number of elements (chunks of data accessed by threads) equal to 32!!!!!
    - to make the threads to not access the same data (there is this risk) and generate a latency in order to fullfill the warp, 
      we add an element to the array, even if it is not corresponding to the physical domain points --> in this way, we allocate more memory, 
      but the threads are able to access the data without trying to overwrite the same memory destination because there is more "space" in 
      memory that has been allocated?????


- GPU course (P1.7): why the shared memory access is faster than the global (default) one? Because in this way we make the threads access
                     the data shared in the same Block (we have, for GPUs, blocks containing threads). In GPUS, each Block of threads has 
                     a small shared memory. The ACCESS TO THIS MEMORY IS REALLY FAST --> THIS IS THE REASON WHY IT IS CONVENIENT TO USE SHARED
                     MEMORY APPROACH!!!!

- GPU course: why we need to do a Synchronize for the threads? If we don't do it, the Timer retrieves a really small time, why?
              - NOO!!! IN THIS CASE WE ARE CPNSIDERING MESSAGE PASSING, SO RANKS (NO MEMORY SHARED SYSTEM --> DISTRIBUTED ONE), WE DON'T HAVE THREADS!!!!!!
              - IT IS CORRECT TO SAY THAT WE NEED TO SNCHRONIZE THE DEVICES, WHY?
                  - If we compute the Timing without synchronizing, 
              - NOOOOOOOOOOOOOOOOOOO!!!! : This is totally wrong!!!!!
                - IN GPU COMPUTING (CUDA) THE THREADS ARE THE GPU THREADS!!! SO WE HAVE MULTIPLE GPUS --> FOR EACH GPU WE DEFINE A NUMBER OF THREADS
                  IN ORDER TO HAVE CHUNKS OF THREADS EQUAL TO 32 (USUAL SIZE OF A WARP OF THREADS) --> SINCE WE HAVE MULTIPLE GPUS PROCESSOR,
                  WE PROCEED IN THIS WAY (TO BE AS MUCH AS POSSIBLE IN LINE WITH THE HARDWARE): 
                    - WE SET A CERTAIN NUMBER OF RANKS WITH MPI: FOR EACH OF THEM WE SEND TO DEVICE --> WE DO THE COMPUTATION ON GPU --> WE RESEND
                      TO THE HOST --> WE ARRANGE THE RESULTS TO GET THE FINAL RESULT AND PRINT FROM A CPU RANK!!
                - SO IN GPU COMPUTING WE TALK IN GENERAL ABOUT THREADS!!!
                    - SO IT IS CORRECT TO ADFIRM THAT WE SYNCHRONIZE THREADS!!!
                    - WE DO A: cudaDeviceSynchronize() : 
                        - THIS FUNCTION STOPS THE CPU INSTRUCTION EXECUTION UNTIL THE GPU COMPUTATIONS ARE EFFECTIVELY TERMINATED
                          (BECAUSE KERNEL CREATION BY HOST(CPU) ARE NON-BLOCKING BY DEFAULT!!!)
                        - THIS IS IMPORTANT BECAUSE IN THIS CASE, WITHOUT INCLUDING IT, WE HAVE THAT THE ONLY TIME MEASURED IS THE TIME NEEDED 
                          TO COPY DATA FROM CPU TO GPU?????

- SWAPPING IN OPENACC:
    - once you do the swap on the host, you don't need to do the swap on the device, because the relation between the host and device 
      is still the same --> it is like pointing to a train or to another one: if you point to the forward part of it (the host of the other
      variable), the relation between that part and the vanes(the device address) is still the same !!!!!!





- REMEMBER!!!!!: In Python is not possible to perform Shared memory parallelism!!!
    - IT IS NOT POSSIBLE TO INTRODUCE THREADS!!!



THINGS THAT I WANT TO KNOW:
- Cannon method for matrix multiplication
- How to make variables private in C++ without having compilation issues?
- FFT AND WHY THE AllToAll (Course P1.5)
- how to make Hdf5 work? (Course P1.5)
- how to make Jacobi work on GPUs with OpenACC??
    - consider that when we do the Memcopy(CUDA), we essentially need at first to allocate on the Device
    - this happens also in the Pragma structured region, but:
    - in a Pragma structured region OpenAcc authomaticly makes copy of the host buffers "calling" them in the same way of the host buffers!!!
    - so we don't need to introduce new device buffers!!!!
    - In a structured pragma region, OpenAcc at first copies data from Host to Device, but inside the pragma region we can refer to both the 
      host and device buffers!!!
    - at the closure of the structured data region, OpenAcc copies data from device to Host, BUT IS IMPORTANT TO OBSERVE THAT INSIDE THE DATA
      REGION WE CAN REFER TO EITHER HOST OR DEVICE BUFFERS!!!

- Balancing of the tree in Fortran
- Iterator in C++
- How to implement OpenAcc for Thermal bubble program (course 1.8)??
- How to make a module for NetCdf4?? (Thermal Bubble code for course 1.9)?
- How to compile with Intel on Leonardo?? 



- REMEMBER THE DIFFERENCE IN ARCHITECTURE between THE BOOST AND THE dcgp (CPU) architectures!!!!!
    - THEY HAVE DIFFERENT NUMA REGIONS!!!
        - dcgp : 2 sockets, each socket: 4 NUMA regions, each NUMA region: 14 cores (cpus per Task) !!!!
        - boost partition: cpus per task = 8, number of gres (gpus) = 4
            - number of gpu threads per block = 8 x 4 --> size of a wrap, which is: 32!!! (coincident with the size indicated on the 
              architecure Data Sheet!!!)




- COMPILERS MUST HAVE NO SECRETS!!!!!!!!!!!!!!!!!
    - WHY THE FLAGS? WHY THE PREPROCESSING DIRECTIVES???
    - WHY THE LINKING? 


- ASSEMBLY LANGUAGE!!!




- Shallow copy: 
    - what is a shallow copy? 
      Let's do an example: I have an array A and an array B
      - A will correspond to a certain memory address, the same can be said for B
      - I make B equal to A at some point.
      - Then I make a change to A --> what happens to B??? --> For a shallow copy, B will change as well, BECAUSE IT IS NOT A DEEP COPY!!!!
      - to make the copy fully work, we need a Copy Constructor, and usually also a Move Constructor!!!!






Architecture course:
    - how processor is intercating with external memory --> where we store program to be run!
    - we have in general the Von Neumann architecture: CPU with ALU and system and memory as a separated entity with respect to it!
    - we have the memory subsystem:
    - best performance: 
        - we have 2 different subsystem
        - the best speed is the lowest among the CPU and Memory!!!
        - we need to Load/store data into the machine --> if machine is fast, but the data are provided slowly, it will be slow!!
        - if memory is not able to provide enough amount of data to perform all the operations affordable by the CPU, the speed will be affected!
        - CPU: flop per second
        - Memory: Byte per second
        - How to define a single metric to estimate the peformance of a system?
            - Machine Balance (hardware): FLOP/s (peak) / Byte/s (peak) 
            - arithmetic intensity (software): OPerations/num. of bytes that algorithm needs to read/access to the memory.
            - if I is higher than MB : compute bound, otherwise: memory- bound !!!
            - given an algorithm , we can estimate alg intensity of it !!!
                - example: low arithmetic intensity:Lattice Boltzmann, high: Dense linear algebra (BLAS3 matrix multiplication)
            - The Roofline Model: visualize how algorithm is exected to behave in a single architecture
                - x-axis: computational intensity
                - y-axis: Attainable performance
                - vertical line: algorithm intensity set!!
                - we can try to have an extimation of the maximum performance we can achieve with the provided architecture!!
            - Example: 
                - inner product:
                        - inner product: 
                            - each iteration: 1 sum and 1 multiplication!! --> arithmetic complexity --> 2N FLOPS : we take into account only fl.
                              point operations!!
                            - memory accesses? : read one el. from a, one from b--> then we need to read 3 elements --> and write 1 element -->
                              we need to read 3N, write N elements 
                            - we calculate I: is is good? --> it depends on the machine!!
                            - Machine balance today is usually around 4!!
                            - so this computation will be memory bound for most of the machines!!
                            - between the principal memory and CPU there are different cache levels!!
                            - we can use Roofline also to compare different applications!!!
                            - FPGA: a lot of transistor not statically connected --> we can program them to perform the operation we want!!!
                            - we can optimize for either cpu performance or memory access!!!

            
            - Cache lines:
                - 2 subsystem: CPU, memory
                - external memory is slow (not into the processor --> distance, if we want a big memory, we need to address memory in some way
                                           we need system that searches where the element is stored in memory)
                                          - THIS IS IMPORTANT: WE NEED SOMETHING THAT DEFINES WHERE THE ELEMENT IS STORED IN MEMORY!!
                - memory access are random?
                    - how to speed-up process of accessing data?
                    - in general access is not random!
                    - in close amount of data we access multiple data:
                        - if we read a book multiple times, we keep it close to where is is often used
                        - we could store the small piece of data that we need!
                        - we have also a spacial locality, not only temporal locality
                        - we can try to exploit the pattern, we cannot store  all the matrix in the cache, because to have a faster 
                          memory close to cpu is more expensive than a memory external
                          (IT IS NOT POSSIBLE TO HAVE A HUGE MEMEORY CLOSE TO THE CPU FOR MONEY REASONS AND FOR MANIFACT, REASONS)
                        - caches are of standard sizes!
                        spacial locality:
                            - if we want to access buffers bigger than the vector?
                                - if we access often to data,we can try to read from extern. memory a sequence of element instead of a single element
                                - we assume that the other elements willl be used later on 
                                  (OBVIOUSLY WE CANNOT BE SURE THEY WILL BE NEEDED LATER)--> elements will be stored in the Cache
                            - the memory in cache is organized in cache lines
                        - when we read from the external memory, how we decide where to store in the cache memory?
                            - depends:
                                - direct mapping: we use a part of the external memory address
                                - page size: 4 kbytes
                        - if we store a given piece of data into cache line, if we want to read data that should be in the same place
                          How can we know if in the cache line there is a data or the other one?? 
                          - We introduce an index, with which we can check if the piece of data is in the cache line or not
                            - if data is in the line : cache hit, otherwise: cache miss
                                - THE CACHE MISS OR CACHE HIT IS REFERRED TO THE CHECK OF THE PRESENCEOF THE DATA IS IN THE CACHE LINE OR NOT!!!
                        - collisions: we are trying to store data where there is already data??? 
                        - Another approach to cache (Associative Cache Mapping):
                            - we don't have direct mapping, but we have Placement Policy
                            - we can choose where to store data into the Cache
                            - if we have one line filled, we can find the first line free and fill it with data
                                - drawback: how can we know where is data stored that we need? We need to go through all the data in the cache!!
                                - there is also other problem: if the cache is full how can I decide what line free??
                                - Associative caches: Good enough policies: Hot Potato:
                                    - The called cache line passes the hot potato to the next line
                                    - we start to eviction only when it is full
                                    - So THE HOT POTATO WILL BE ASSIGNED STATISTICALLY TO THE CACHE LINE CALLED LESS OFTEN
                                    - drawback: expensive in hardware, as many comparisons as cache lines

                        - 2 Approaches: 
                            - Direct mapping
                            - Associative cache mapping
                        - we introduce something in the middle:
                            - Set associative cache
                            - we can find the worst case: should not happen frequently
                        - We have several cache levels (usually 3)
                            - we have different machines, they can have really different cache configurations:
                                - certain caches can be shared only between certain cores 
                                - running the machine, we can check the architecture/design of the memory

                        - Pages:
                            - we move one page of memory to the hard drive (OS may decide to move one piece of memory to the hard drive)
                            - this happens under the hood, we don't know details and we don't have control on it (???) bout it
                            - why is it important to have data in more pages? We will see tomorrow.
                            - accessing the data from different pages can affect data and performance!!!!
                        
                        - MemHierarchy exercise:
                            - command : papi_mem_info (after having run the executable):
                                - let us to check the boundary from which we have the next memory location (L1, L2, L3 caches, and the DRAM memory)
                                - the number near "Associativity" is the number of bytes stored in a single Cache line: is the 
                                - line size: cache line size
                                - number of lines
                                - associativity: number of sets --> columns!!! Is it the number of sets (groups of columns) or the column 
                                  index in a certain set??


                        - How the cache behaves when we try to write something into the memory??
                            - what may happen if we try to modify that is in the cache line?

            

            Single Pocessor:
                - we have many steps when running a set of instructions
                - Resources: - memories that are not the registers: memory where we store the different instructions (example)
                                - difference with the register: is bigger--> slower to access
                                - we usually read one instruction per time --> more space required
                             - ALU
                             - Address generation unit: it is an ALU, but it is dedicated to memory generation task
                             - Single cycle execution:
                                - one instruction per clock cycle --> but to execute an instruction --> many steps 
                                    - steps coukld be longer or shorter
                                    - some instruct. perfectly fits in the clock cycle, other that need to wait 
                                        - pipelined execution: we split extraction in several steps
                                            - Instruction level parallelism:
                                                -  for every clock cycle a piece of hardware executes a small istruction
                                                - so several instruction at the same time: we cannot control (only suddenly)
                                                - it happens inside the processor
                                                - latency point of view: not positive:
                                                    - first result is get after a certain n of clock cycle: no benefit
                                                    - benefit: we can increase the clock frequency and the throughput:
                                                        - because after a certain number of clocks we get a lot of results
                                                        - so it is good if we have a lot of instruction to be executed!
                             - Problems with Pipeline:
                                - during the clock cycle we cannot use the same resource
                                - if 2 stages need to access the same memory, problem
                                - Data hazards: 
                                    - control/sequence hazards:
                                        - branch: we don't have yet the result because we are in a branch --> we need a way to throw away 
                                          what is being executed in the pipeline --> we need to solve the hazard!
                                          - Stall the pipeline (a Bubble) : if some instruct. depending on the instr. exec. before the 
                                            required instruction are executed --> we introduce a stall which will last a number of clock 
                                            cycle needed to make the required ijnstruction to provide the needed results for the current
                                            instruction!!!
                                            (we pause the pipeline)
                                    - Bypassing:
                                        - another way to solve hazards:
                                            - reduce stalls
                                    - Branches prediction: if we, after knowing the results, see the prediction is correct, we keep the branch
                                                           otherwise, we throw it away!
                                        - Speculative execution:
                                           - whenwe have instruction with condition,he starts to execute instruction and in case is wrong throw away
                                    - Why I cannot read/load 2 elements at the same time? --> higher complexity!!
                                        - I can have a pipeline for load/store, another for ALU, another for multiplication (more complexity)
                                        - if I need to access the outside memory, sometimes I don't know how much clock cycles I need
                                    - Out of order execution:
                                        - alter the order of execution of instructions: Dynamical scheduling
                                        - problems: change order --> write after write and write after read can be a problem (WHY??)
                                        - we use different registers (register renaming) 
                                    - If we add possibility Out of order and Speculative execution:
                                        - higher complexity
                                        - we keep the result in a register and we register or throw away depending on the result???
                            
                            - Modern processors:
                                - Intel Skylake (single core): yellow: ALUs
                                    - different warps inside alus are the different operations that can be done
                                    - theoretical Floating Point peak:
                                        - how to calculate: 


                            - Software techniques to perform instruction level parallelism:
                                - COMPILERS ARE REALLY GOOD AT THIS!!Software techniques to perform instruction level parallelism:
                                    - COMPILERS ARE REALLY GOOD AT THIS!!!!
                                    - avoid to have sequence of instruction where we have instructions depending from each other
                                    - try to have one single loop (not nested loops)!
                                    - avoid dependencies inside a single loop!
                                    - compiler can do loop unrolling
                                    - 


            Data parallelism:
                - Increasing single-core performance (vector operations): 
                    - vector of different items --> computational units able to perform operations on more elements at the same time
                    - so we can define vector instructions
                    - how data is managed
                    - example: same operation over different arrays
                    - for more iterations we can have similar instructions/operations
                        - we can try to merge and put together the same operation on different data
                        - one instruction on 2 elements of data
                        - drawback: if we want to perform operation on 2 elements for example: we need 2 ALUS (multiple hardware to work)
                        - newer processor: instructions can work on 8 or 16 elements arrays
                        - GPUs: they have long vector instructions (32 or more elements)
                        - the throughput: one instrcution per clock, but one instruction is divided into several tasks --> we have pipelines anyway
                        - the operation must be the same!!
                        - SIMD: single instruction, multiple data
                            - or Array processor --> but there is a difference between vector and array processor
                            - vector processor: - since having multiple Alus is costly, he keep one ALU and we move data into the Alus
                                - we don't need a for loop to control, but we think about a sequence of data put in the machine 
                                - we process one element at a time
                                - of course we have a vector register
                                - Alu processes one element at a time
                                - But if we have loops smaller than the defined one, we need control to stop the process!
                                - One element made of 2 items (it is a kind of this ...)
                                - We can consider registers with 2 or 4 elements --> depend on the architecture
                                - If we want to have a vector instruction, we want a vector loaded with elements closed in memory
                                    - but we will load an entire cache line into the cache (64 bytes) 
                                    - we are reading more elements than needed (for scalar CPU) --> we copy a chunk for each elements(for stride)
                                        - then we need to load data into a vector to perform for example the matrix multiplication
                                        - so the lack of performance is related also to the many operations to prepare elements to perform
                                          operations!
                                        - Virtual memory: a page is always multiple of a chache line : cannot happen that a part of line is in 
                                                          a page and another in different page
                                - Modern SIMD: Today vector and array computing are merged
                                - Cores of Coka: AVX2 (256 bits long)
                        - SO WE HAVE VECTOR INSTRUCTIONS (COMPUTATIONS AT THE SAME TIME) AND VECTOR REGISTERS
                        - GPU faster if we have a lot of data and we need to do the same operation on it            
                    - Vector arithmetic instructions:
                        - Reductions:
                            - we operate a vector on itself
                            - memory access: 
                                - HOW WE STORE DATA INTO MEMORY
                                    - IF WE HAVE STRUCT WITH X AND Y: IF WE HAVE TO ACCESS X ELEMENTS AND THEN Y ELEMENTS, WE ARE OBLIGED TO
                                      JUMP ONE MEMORY TO ANOTHER (STRIDED ACCESS --> REALLY BAD!!!!)
                                    - ALTERNATIVE: STRUCT OF ARRAYS --> MUCH MORE EFFICIENT!!!
                                    - INDEXED ACCESS: ARRAY FILLED WITH ADDRESSES USED FOR INDEXING ANOTHER ARRAY
                                        - VERY DIFFICULT FOR THE COMPILER TO UNDERSTAND THE CONTENT AND CANNOT PREDICT, PREFETCH, OPTIMIZE
                                        - AVOID ALWAYS!!!!
                                        - this can happen when we access to an object (C++) this happens --> we should avoid it!
                                - Conditionals in loops:
                                    - Vector masking:
                                        - evaluates conditions : conditional mask --> it computes both operations --> then the compiler merges
                                        - thr mask is created at run time
                                        - by computing the mask, it is avoided to havr branches!!
                                        - it does make sense if we have vector registers!!
                                        - there are cases where the compiler decides to not use vector instructions
                                        - avoid having conditionals inside loops
                                        - masks can be implemented in different ways:
                    - Limitations of SIMD:
                        - we cannot vectorize function calls
                            - we could inline the function calls??
                        - we need long loops to take advantage of it
                        - They are mixture Array computing and Vector computing!!!
                   
                   EXAMPLEs:
                    - Each p. of the grid contains 37 values (37 parameters)
                    - Lattice-Boltzmann simulation
                    - 2 functions: 1 compute bound and one memory bound
                    - for each point, 37 values for the state -->struct of 37 elements
                    - Matrix multiplication:
                        - we could try :- if we have large matrices, is happening that we need to read elements multiple times
                                        - small matrices: kept into the cache
                                        - big matrices (bigger than cache): sooner or later we will read element that will evict an element
                                            - to avoid this, what we can do?: cache blocking
                                                - we can decide a block dimension
                                                - we select a block and start to do computation just on that block
                                                - so we need to read blocks of the whole matrix
                                                - choose size in order to keep data into the cache memory
                                                - when we compute all the elements, we need to move to another block
                                                - again we will fill the caches and go on
                                                - we can try if is it possible to vectorize, or to implement a fma on the code
                                                - script to run the code multiple times to change the block size
                                                    - we don't know what is the best block size --> depends on the hardware
                                                    - we can at first find the best block size --> then we add compiler flags --> then we change the size another time
                                                    - we assume block size is a square
                                                    - 3 outer loops (through the dimensions)
                                                    - they're jumping one block at a time
                                                    - perf stat -a --per-core -e dTLB-load-misses ./main.c : 
                                                      - with -e we can set the metrics
                                                      - -a --per-core stands for the specific cores
                                                      - list of the different event that can be collected: perf list
                                                        - list of the metrics
                                                        - we can check how the metrics change with for example the block size or the matrix size 
                                                        - we can also check characteristics also by running PAPI:
                                                            - papi_mem_info
                    

                    - Shared memory:
                        - write and read order: depends on the order of operations and from the hardware point of view a lot of things can happen
                        - we can avoid things to happen from a software point of view (for example doing a synchronization)
                        - Difference between process and threads: both are software things
                                                                 - one process has its own memory space (from  a software p. of view)
                                                                    - from hardware p. of view can be shared resources 
                                                                 - thread:
                                                                    - if threads are running on symmetrycal multi processor, threads, in a given 
                                                                      array, can read what another thread wrote
                                                                 - we want to map the software concepts into the hardware, so we want to close
                                                                   from a logic point of view bind a MPI rank in a process, and then spawn
                                                                   a certain num. of threads. So we force the rank to access only the physical
                                                                   memory directly linked to physical process
                                                                   - we need to know which thread is running where
                                                                   - what is running where?
                                                                   - openmp: 2 environm. variables to Close or Spread: 
                                                                       - to check on which cores are running
                                                                       - when are they useful?
                    - Distributed memory:
                        - process = rank
                        - we need a way to make the processes, running on system with different memories, to interact with each other
                        - best way is to have one rank for each process --> then I spawn among threads to use the L3 shared memory


                    - Profiing tool:    
                        - if I want to collect cache miss of one thread, or for a single line?
                        - class of tools:
                            - profilers: intercepting events of our application
                            - made of 2 applications:
                                - extrae: 
                                - to view things: paraver --> files .prv : we copy files locally and we open them!


        - ATTENTION!!!!: HOW WE CALCULATE THE BANDWIDTH???? (Bytes per second) --> here we are referring to the effective data accessed, not 
          to the data sheet of the DRAM Bytes per second (which shoul be anyway the maximum admitted bandwith)
            - For a theoretical estimation we can refer to the calculation linked to the block size (edge), in particular it will be computed 
              as N^2 * (size of Double precision number (8 bytes)) * number of accesses to memory (in this case 3 reads and 1 writing) and 
              then we should multiply for N divided by the block size (WHY??????)
            - For an experimental estimation, we can use Perf:
                - we run per stat -d executable and we get the cache misses for each memory level. What we can observe is that 
                  in this case the cache misses for the last cache level (L3) are really smaller than the cache misses for L1
                - When we have a cache miss, it means that the system must access to data to retrieve it and make it available again
                - So the last level cache misses represent the data (band width) exchanged with the DRAM, while the L1 cache misses 
                  represent the bandwidth between L1 and L2. 
                - In spite of the standard analysis being referred to the DRAM bandwidth, here we calculate both DRAM and L1 to L2 Bandwidth
                - To do this, we multiply the size of the cache misses (L1 or ultimate cache level) for the size of the cache line 
                  (BECAUSE THE CACHE MISSES REPRESENT THE NUMBER OF CACHE LINE TO BE CALLED FOR DATA TO MAKE DATA AVAILABLE) and 
                  divide for the elapsed time!!!

VIM:
- to delete all the lines from a certain line:
    :<line_number>$d



Python Course:
    - Decorator:
        - it is important to have a return for the wrapper
        - it is important to have a return for each "Decorating action", like for example the printing
    - conda: python package manager (things already installed)
    - miniconda: better than anaconda: google how to do it
    - run conda init --> check the .bashrc --> strange stuffs --> when we open a new terminal, we see a new header for shell
      --> SO WHEN WE NEED TO RUN A COMPLICATED CMAKE OR MAKEFILE, IT'S BETTER TO DO: 
        conda deactivate
    - we create different environments
    - to check environments: conda env list
    - to install: conda install numpy (example for numpy)
    - to see packages: conda list
    - pip : pure Python --> conda brings the binary there are on different location, while pip takes more time
        - we use it on cluster
    - to install things: conda install -c conda-forge
    - we will use jupyter notebook
        - we need to use the one created for the environment we created?
    - Jupyter notebooks: good only for teaching (we don't want them)
    - we don't define data types (it decides)
    - there are also complex numbers
    - it is dynamically typed:
        - for python we could put at first string then integer --> it doesn't complain --> don't rely too much on this freedom!
    - no limit for integer:
        - where is it stored? --> complicated way (not efficient)
    - it is not github friendly
    - strings: we can concatenate with "+"
        - be careful with the assignments!!
    - function: pass by "association" --> it deciding if it calls by value or by reference
    - help function for built in objects
    - dir('class') : tells us which methods are available for a given class
    - blocks: done with indentation
    - loops: in range --> starts by default at 0
    - containers:
    - we can slice with containers: ls[2:7:2]  --> print from 2 to 7 with 2 step
        - -1 in the range --> we get a reverse
    - DON'T USE LIST OF LISTS!!!
        - bad idea, because we are not sure of how much elements we are changing
    - List: We can directly print for certain values of a variable 
            - sort function
            - deleting by index: del
            - deleting by value: remove
    - Sets:
        - each element is unique
        - mathematic operations
    - Tuples:
        - they are not the C++ tuples!!! ---> In C++ we used it for recursion!!!????
        - they are just like lists, but they are not changeable and are treated more fastly!!!
    - Commands that start with %: called Magic ----> ONLY WITH JUPITER NOTEBOOK???
        - important: timer:
            - %% : some strange results!!
            - times just the cell of the jupyther notebook!
    - TO STOP WRONG CODE:
        - go on Kernel --> stop Kernel --> correct --> Run all cells
    - Function:
        def name():  --> pass by value or reference depends on the passed type!!!
        - we can know it with help, or with: variable?
        - Interactive python
        - recursion
    - Errors: it is not easy to debug!!!
    - Lambda functions: we don't name a function
    - Exercises:
        - we can put the following: (we make sure that only from our location we can run the program??
          if __name == "__main__":
            #Call function
    
    - Class: 
        - we need to put self as first argument ---> WE NEED TO DO THIS FOR EVERYYY METHOD DEFINED FOR THE CLASS!!!
        - we can create class instance 
        - constructor already created for us 
        - dot notation to call class method
        - constructor: def __init__(self, )
        - assignment is not a copy!!!
        - SO WE USE OVERLOADING OF OPERATORS???
    
    - Modules:
        - import name of the module as <nameOfTheModule>
        - if we need only one function: from numpy import cos
        - to create one:
            - I can import it !!

    - Pandas
        - we will see it in the Machine Learning course

    
    - MPI 
        - not needed to do Init and Finalize
        - cluster: we cannot do conda install
            - instruction --> pip install and create mpi
            - ATTENTION!!!!!!!!! --> before installing pip, it is necessary to load openmpi --> Otherwise it won't work!!!
            - to install pip with no cache (on cluster):
                pip install --no-binary=mpi4py --no-cache-dir --force-reinstall mpi4py
            - be sure, when creating a new New Environment, to install the needed packages (like numpy, for example)
            - test that it works
            - so we need:
                - serial jacobi
                - serial game of life
                    - we need the serial versions because 
                - parallel of one of them
            - 
            
    - Pybind:
        - call some parts from another language??
        - in Python we don't have templates!!! --> It is more correct to state that we don't have Template functions or Template variables!!!



    - Numba:
        - nvhpc
        - we need nvidia
        - create new environment (at first deactivate the current one)
        - install pip
        - load nvhpc, cuda, what we used on gpus
        - jit: we are telling to compile things like c functions
        - we don't have control on it
        - timing: the second time we measure the true time??
        - jit parallel: parallel = true --> similar to OpenMp
        - we can use cuda with jit
        - WARNING !!!!: WE WERE NOT ABLE TO RUN NUMBA WITH POETRY LIBRARY (NUMERICAL ANALYSIS COURSE) --> WHY??        
    

    - MPI4PY INSTALLATION ON CLUSTER:
        - Why we install mpi4py with pip and not with conda?
            - essentially on a local machine we have mpi4py binary --> this binary is overwritten by conda and it is ok because the way conda installs
          is in line with the general laptops architecture
            - On cluster, we have a really specific architecture, so mpi4py which is on cluster, has a different configuration 
                - IN ORDER TO AVOID IT TO BE OVERWRITTEN, WE INSTALL WITH PIP --> WHEN PIP SEES A binary, it is able to recognize it and 
              it doesn't overwrite it
                - Risks:
                    - the best thing that can happen is that MPI doesn't work
                    - worst: it seems to work --> but it is slow and has bugs and issues!!!

        - RUN on Cluster for Parallel Jit (Multi-thread)
            - WE MUST CONFIGURE CPU-PER-TASK LIKE FOR OPEN MP!!!!!!!!
            - export THREADS PER NUMBA!!!!
            - remember that the outer loop where we put the @njit is the loop where we need to put the prange!!!!!!
            
    
    - REMEMBER !!! : 
        - THE WAY WE IMPLEMENTED THE CODE IN PYTHON DOES NOT MAKE ANY SENSE AT ALL!!
        - WE SHOULD HAVE BEEN USED 2 DIMENSIONAL ARRAYS INSTEAD !!!


    - Pybind:
        - it let us to introduce an interface to run C++ programs through Python code
            - what we do is to call directly the functions defined in the C++ scripts
            - in fact we introduce direct calls to:
                - C++ functions 
                - C++ Classes (with the corresponding functions as well)
            - How to run it?
                - what we do is to generate a sort of library originating from the "Interface C++ file":
                    - technically what we are planning to do is a "BINDING" of the functions, and of the class functions as well, from C++ to Python
                    - we compile in a way that gives us the creation of a Library: 
                        - WE CREATE A  .so file:
                            a compiled library file type following the ELF (Executable and Linkable Format) structure. They're used by Linux and Unix                            -based operating system like Ubuntu, Debian, and CentOS, and they allow applications to share functionality by 
                            dynamically loading various features at runtime --> so it works essentially at runtime!!!






OpenAcc :
- It is based on directives
- ATTENTION!!!: the directives are taken into account at compile, or at run time???
    - The directives are based on the #pragma acc (in C++) --> BUT ONE MOMENT: the Accelerators are applicable only for GPUs? Or also to CPUs?
                                                                               - it seems that it is applicable also to distributed comp. with
                                                                                 CPUs
    - ATTENTION!! : when we introduce a directive: #pragma acc parallel --> WE ARE NOT INTRODUCING ANY COMMAND RELATED TO PARALLELISL
        - WE ARE SIMPLY ALLOCATING MEMORY ON THE DEVICE (USUALLY THE GPU!!!!)(FROM THE HOST??)
        - IN ORDER TO PARALLELIZE A LOOP, TO MAKE IT EXECUTED BY DIFFERENT GANGS (THE GANGS ARE GROUPS OF THREADS) AT THE SAME TIME
          (BY MAKING COMPUTATION DISTRIBUTED FOR INSTANCE BETWEEN DIFFERENT PARTS OF THE MATRIX --> FOR MATRIX OPERATIONS/SOLVERS)
          WE NEED TO SPECIFY THE DIRECTIVE: loop !!!!
        - OF COURSE WE NEED TO CONSIDER, LIKE FOR OpenMP, that it is possible to MAKE THE VARIABLES "private", so to make data accessible by a proper
          thread and not from the other ones? --> NO!! EACH THREAD WILL HAVE ITS OWN COPY OF THE DATA TO ACCESS TO!!!!
        - Like OpenMP, it is possible to introduce the "Atomic" directive???? --> WE NEED TO RECALL THIS CONCEPT!!!
        - In general, when we introduce the #pragma acc directive for the loops in the code, what is done is an allocation of memory on the device,
          a copy of data from host to device, computation on the device, copy from host to device
            - we can make this more efficient by introducing STRUCTURED or UNSTRUCTURED data regions!!!
                - WHAT THE FUCK ARE THE DATA REGIONS??
                    - THEY ARE INTRODUCED TO AVOID TO : 
                        ALLOCATE-ON-DEVICE -> COPY DATA --> (AFTER COMPUTATION ON DEVICE) COPY TO HOST - FOR EACH #pragma acc
                        - we allocate and copy data just at the beginning and at the end of the Data Region (for STRUCTURED DATA REGIONS)
                        - For UNSTRUCTURED DATA REGIONS, WE FREE DEVICE MEMORY MANUALLY AND DELETE DATA ACCESSED FROM THE DEVICE AFTER THE COMPUT.
                             - IT IS LESS AUTHOMATIZED AND LET THE USER FOR INSTANCE TO COPY TO HOST BEFORE THE END OF THE DATA REGION!!!

- ATTENTION!!!!:
    - The data regions, delimited by the "{", "}" in C++, need to have these symbols after the line with the "pragma":
        - it is not possible to write: 
            #pragma acc ....... {
                NOOO!
            - correct way:
                #pragma acc ...
                {

- "present" statement:
    - it should be used to tell the compiler (?) THAT THE INDICATED DATA IS ALREADY ON THE DEVICE, AND THAT IT IS NOT NEEDED TO REALLOCATE
      MEMORY FOR DATA? BUT IS IT DONE AUTOMATICLY (BY DEFAULT) IN SOME CASES???










12/01/2026:
    NUMERICAL ANALYSIS COURSE:
        - Andrea Cangiani
        - Stefano Piani
        - program: - numeric diff equat.
                   - vector spaces, normes etc
                   - basic linear algebra
                   - iterative solution
                   - intro to Finite elements
                   - discretization of time dependent problems
        - Analysis:
            - to do scientific comouting: what we are doing??
            - code must do exactly what theory predicts!!
            - books: quarteroni and Mayers and Morton
                - Golub van Loan
                - Lloyd
        
        - Ordinary diff. equation: 
            - initial value problem --> assume uniqueness and existence of the solution
                - we discretize the time (subdivide in intervals)
                - we take into account the corresponding vector of discrete solution (for each time step)
                - Euler's method:
                    - Taylor formula:
                        - we remove the reminder --> we get the u(n+1) = u(n) + tau*f(tn, un)
                        - we set u' = f
                        - we get a divided difference
                - Convergence:
                    - and how fast is it converging??
                - Different derivation of Euler's method:
                    - by quadrature: integrating the ODE in the subinterval tn,tn+1]
                    - trapezoidal rule:
                        - is higher order method, but it requires the solution of a non-linear equation at every time step (WHY???) --> implicit scheme (WHY???)
                        - balance between costs and accuracy --> to decide what method to use
                            - implicit euler needs less steps to get a certain accuracy in the solution
                        - multistep: the current time solution doesn't depend only on the previous time step, but on also other previous steps!!!
                            - we usually use one step methods in order to get a sufficient number of solution needed for multi-step method!!
                - Various divided differences:
                    - what is the best for approximation??
                    - Divided difference error analysis:
                        - ERROR BOUNDS?????? (WHY????)
                        - they are a priori bounds (they depend on the function)
                        - how do we prove the formulas???
                            - on the notebook!!
                        - we can take also more time instants --> multi-step formulas!!!
                        - if we don't use simmetry (central schemes??) we need to ask for more evaluations to increase the order!!!
                        - Review the log log plot for the errors --> we should see that the 24 at the denominator makes the approximation better!!
                        - we can approximate the second derivative with a WHAT???? -->
                            - divided differences for higher derivatives:
                        
                 - First exercise:
                    - solution to -laplacian equal to f --> we will see why we add the minus
                    - python library:
                        - tools to manage how to build it
                                - folder: /dev/shm : poetry new <lib_name>
                                - ls -lh : I see the folder created 
                                    - I go inside:
                                        - pyproject : it is default created
                                            - name, version, our name, and some tools that can be used
                                        - install poetry : choose the curl one !!! --> - means that if the path is a minus, read from the default
                                            path !!!!
                                        - PROBLEM WITH poetry INSTALLATION:
                                            - WHEN TRYING TO RUN THE COMMANDS : poetry lock and poetry sync, I got the error related to Python:
                                                - [Errno 2] No such file or directory: 'python'
                                                - STEFANO SOLVED THIS BY INSTALLING PYTHON MAKING IT IN A WAY THAT Python3 must be seen as Python
                                                 (FOR SOME REASONS poetry wanted the folder to be named in this way):
                                                  - we run the command:
                                                    sudo apt install python-is-python3
                                        - PROBLEM WITH INSTALLATION:  
                                            - NOT ACTIVATE AND DEACTIVATE CONDA ENVIRONMENT BEFORE INSTALLING POETRY!!!!
                                            - IF YOU DID IT, TRY DO DO: onda config --set auto_activate_base false
                                                - THEN, UNISTALL POETRY, INSTALL AND SO ON!!! (PYLOCK AND PYSYNC)
                                        - poetry add numpy scipy --> poetry does : write in the dependency the packages, and it created a virtual
                                          env with my package in order to use it
                                        - poetry.lock -> it tells us what kind of packages we are using!
                                        - poetry helps to manage packages
                                        - test: small script to check the code works properly
                                            - small routine to check the code is working well
                                            - instead of throwing routine away, we can set tools to use it 
                                            - we use pytest
                                            - poetry sync --> ask to install the same virtual env of the file I blocked
                                            - poetry run ls --> I'm telling poetry to run on the virtual env that poetry created!!
                                            - poetry run pytest
                                            - Exercise:
                                                - is structured as a library (He already run poetry run for us)
                                                - function solve_poisson --> we need to implement it
                                                    - rhs: right hand side, Callable: it is a function --> in Python are first class citizen
                                                        - there is no difference between what we can do with a function or a variable
                                                        - we can pass a function as an argument for instance
                                                    - why we use solve_poisson
                                                        - we use it becausw the Bcond was hard coded --> in this situation we don't know how many 
                                                          points we will have on the boundary
                                                    - typing in python: we are telling: n_points is an integer for instance (it is like an integer)
                                                        - for instance boundary_condition is a dictionary
                                                    - Enumerator : it is introduced to make Side can assume only certain values!!
                                                    - Boundary conditions: are expected to be one-dimensional!!
                                                    - tuple: I want three arrays (WHAT ARE THEY??)
                                                    - What Pytest do?: it runs code with certain values!!
                                                    - IT IS VERY VERY SUBTLE HOW WE ENUMERATE THE NODES!!!!
                                                    - WE CAN DECIDE TO SET THE DIMENSION OF THE MATRIX AS 3X3 OR 5X5(REASONABLE CHOICE???)
                                                    - editor: use pysharm !!
                                                    - grid_x[0,0] : we can choose if it is the down_left or up_left, and if the first index stands
                                                      for x-axis or y-axis!!

                  - Second lesson (13/01/2026):
                        - Condition numbers and sparse matrices:
                            - if we add some convection to Poisson --> the tecnique is just again a linear system
                            - it could be necessary to solve multiple linear systems
                            - but it is always a matter of solving a linear system
                            - TRENT500: LINPAC --> computation is roughly a linear system!!
                            - CPU are designed to solve linear systems
                            - how do we solve linear system?
                            - how easy we do it?
                            - can a computer solve a linear system?
                                - we use floating point (I introduce an error)
                                - how many digits? 32 or 64 bits
                                    - how many digits is 32 bits? 6/7 digits
                                    - and 64? 16 digits
                                    - rappresentation error: if I put 1.00000001 in computer --> I get: 1 --> this is the concept of rappres. error?
                                    - 0.1 + 0.2 - 0.3 = 5.55e-17 !!!!!!!! 
                                        - floating point operations are independent on the language used!!!
                                        - why is not 0 ? it is a rappresentation error!
                                        - 0.1 = it is nice --> the numbers that are nice are the ones writable as number over power of 10
                                        - for example 1/3 is bad (periodical digits) --> rounding error!!!
                                        - these errors can grow and become even more complicated to manage
                                        - relative error: we compare the error to the initial error!! (it is a ratio that gives us the importanc eof the error!!
                                        - our computation can affect also vectors, not only scalars
                                        - given x, I compute sinx --> this is really well conditioned
                                        - well conditioned and ill conditioned problems !!!!


                            - Meshgrid function:
                                - 
                    
                    - Lecture 4:
                        - L-U decomposition: what are the limits of these methods?? Should we look other methods??
                            - if I have a big matrix which is not dense?
                                - computer needs to do a product and we can result in a fill in for example
                                - we cannot store a matrix really big --> too much big for storage
                                - so we need methods that can manage to solve problems with big matrix dimensions!!
                                - The alternative is by using iterative methods --> MADE OF A CERTAIN AMOUNT OF STEPS
                                    - we are solving a system which is symmetric and positive definite --> most simple case
                                    - we build a sequence of vectors such that the sequence converges to the solution
                                    -  
                    - Attention!!!:
                        - The Jacobi iterative method, as well as the Gauss Siedel, converge just if the matrices are strictly diagonally 
                          dominant!!! Otherwise we look the simulation run forever with really strange numbers come up !!!


                    - Convergence note:
                        - There is a huge and strange issue when trying to run the simulation of Jacobi with a matrix free approach (as we saw
                          during the course, this equivalent, with the exception of the right hand side of the equation, to solving the Jacobi 
                          seen during the previous courses!!
                        - The issue is the following:
                            - in the file CSolver.py, we have the class with a provided member function (jacoi_solver) which executes the iterations
                            - when we set a number of iterations as the stopping condition, we insert the error at each iteration as an element
                              of an allocated array of size equal to the n. of iterations
                            -
                          - The issue is the following:
                            - in the file CSolver.py, we have the class with a provided member function (jacoi_solver) which executes the iterations
                            - when we set a number of iterations as the stopping condition, we insert the error at each iteration as an element
                              of an allocated array of size equal to the n. of iterations
                                - in this case the error seems to decrease properly
                            - On the other hand, if I set the tolerance value as the stopping condition, and I introduce an evaluation of the error
                              for contiuning or not the iterative process, this time considering a scalar error recomputing through each iteration  
                                  - in this case the error seems to decrease properly
                            - On the other hand, if I set the tolerance value as the stopping condition, and I introduce an evaluation of the error
                              for contiuning or not the iterative process, this time considering a scalar error recomputing through each iteration
                              the process starts to go forever and, having the possibility to print the error at each step, we see that it
                              oscillates a lot between huge and really small numbers --> it seems that the evaluation of the condition gives a 
                              floating point problem
                            - HOW CAN WE SOLVE THIS ISSUE and WHY IS IT HAPPENING??? : 
                                - when I evaluate the error compared to the limit I set, I'm comparing a first estimate (for the first iterations) 
                                  which is quite huge with respect to the tolerance. As you noted, this error is in general affected by both the 
                                  order of the discretization (which depends on h) and on the numerical scheme,together with the boundary values 
                                  and so on. So comparing a huge number to a really small one, can result in floating point issues (lose of 
                                  precision) and strange oscillating behaviour. This comparison could make sense if we have for instance 
                                  we start from an initial guess that is not so great --> BUT ESSENTIALLY THIS SHOULD NOT CAUSE ANY ISSUE
                                  --> THE PROBLEM IS THAT, WE ARE COMPUTING A QUANTITY THAT IS NOT DECREASING MONOTHONICALLY THROUGH THE JACOBI
                                      ITERATION, BECAUSE IN GENERAL IS AFFECTED BY THE DISCRETIZATION ERROR, BOUNDARY ERRORS, INTERVAL LENGTH
                                      AND SO ON --> SO FOR ITERATIVE METHODS, DOES MAKE SENSE TO EVALUATE THE RESIDUAL, NOT THE COMPARISON 
                                      TO THE EXACT SOLUTION --> BECAUSE THE RESIDUAL WILL, OF COURSE DECREASE MONOTONICALLY!!!!!

                                                                                                                                

- Note on GIT:
    - to avoid possible issues when trying to pull new material to the main branch branch of our fork (which has been forked from a web Repo)
      it could be better to create a dev branch at beginning --> in this way we can avoid for exaple possible conflicts that happen when
      for example we modify resources that are then updated on the web Repo --> in that case the pull is forbidden, and could as well have 
   
      issue with the push --> in general we will have different commits locally and on the web repo --> otherwise with a proper branch,
      we will have the possibility to directly merge the developments that we do, with the new updated files on the web Repo!!!




- Tools to check for memory leaks and memory issues:
    - Not only Valgrid!!
    - we have also: Hellgrid, Ages Sanytizer
    - In C++ we don't have these problems, if we use vectors!! --> But C++ is more rich than C!!!
    - When choosing a programming language --> memory management and possible memory issues, compilation times!!!
    - C is a language simler semantically!!!






HPC Cluster course:
- front end node to access
- we could have also commodity cluster
- needs:
    - mounted on rack
    - must be interconnected
    - software to connect
    - software to reserve resources to indiv users
    - cluster: environment to run jobs
    - node: where you're running
    - access node: let us access
    - master node : managm server
    - batch scheduler: schedules jobs
    - U: unit of thikness" for servers --> more U --> more thinkness!
    - we can have a blade system
    - to install cluster: Cobbler
    - we want the installation go automaticly
    - NETWORKING:
        - connections , packet, etc.
        - 2 computers -> connect with a cable (share information 
        - 3 comp. --> cable connection doesn't scale --> so we need swithcers
        - so we create a local area network ---> but we eed a device to route (router)
        - the roouter is connected through internet
        - the larger is the amount of devices to cross, the more latency we will have
        - physical network topologies --> they are usually point of possible failure __> if I fail in that point, everything fails
            - s during the cold war grids were built to isolate possibly in case of failure
        - topologies depend on application to run and on the hardware
        - Packet: send a letter --> we send infos to the destination --> we need to specify precisely the infos --> example: www. provides authom.
          the place we want to reach
        - Network stack: 
            - Protocols, network
            - ISO/OSI model : from physical/local to network and then Application layer
            - data can be transferred in several ways!!
            - TCP/IP model
       -  Email sending:
            - encapsulation : we increases size to add proper infos for destination and so on
                - receiver: de-encapsulation
                - ....
       - we have mutiple applic. running at the same time and more users :
            - I need the possibility to use the same IP address: PORTS
            - we can have priviledged ports
            - SSH : is using TCP (is a protocol??)
            - we have also UDP 
            - Internet protocol
            - IP address: identifies host on a network
                - network and host portion
                - assigned statically or dynamically
            - Routing
            - Some IP addresses are reserved --> cannot go directly to internet (private in our local are network)
            - First level domain (.it, .com : can be referred to country for example)
            - Second level : institution
            - third level
            - So we get the Fully qualified domain name (FQDN)
            - and we get DNS ???
            - static or dynamuc IP assignment : 
                - it can be assigned randomly from a pool for example (like when we connect to the Wifi
                - all the machines of the distributed hardware must be bound to the same IP address (for hpc cluster for example)
            - MAC address : Media access control access
                - physical address, unque
                - assigned by the manufacturer
            - Cables and connectors:
                - always a delay (also for fiber light) --> because the speed of light is finite
            - WE NEED TO KNOW THAT THINGS EXIST TO EVENTUALLY UNDERSTAND ISSUES 
       
       - LINUX COMMANDS:
            - network interfaces: 
                - Io (no play with this); ethX : physical ethernet interfaces; ethX:LABEL (virtual interface); wlan ??
                - PREDICTABLE Network Interface Names (NIC NAmes): consistent name regardless of hardware changes or system reboots
                - facilitates automation
                - LINUX network stack: ??
                - Command line utilities: ping, ssh, ethtool, ifconfig, ip, route, ....
                - ifconfig: old utility; ip: new standard
                - ping: elicits echo response from a host??
                - setup the client:
                    - we use network sniffer: 
                        - NIC that supports boot
                        - BIOS boot sequence: USB/Ext devices --> NETWORK --> Local Hard Disk (we don't have to invert the order?)
                        - Informatiom gathering:
                            - MAC address
                            - documentation, motherboard BIOS, NIC BIOS, BIOS for Sniffer
                        - Collecting MAC addresses: 
                        - Setting up DHCP:
                            - we define a subnet (we provide  master node .....)
                            - we need it for server and clients
                        - PXE client configuration
                            - 
                        - Setting Up NFS
                        - Setting Up KICKSTART
                - TROUBLE SHOOTING:
                    - logs for DHCP
                    - /var/log : every log message regarding everything is happening in the system?
                - We can work from home through VPN SISSA!!!
                - Board Management Controller (BMC):
                    - mini computer inside server 
                    - authomatically powers on when supplied
                    - access to it is done through a network port:
                        - this is really important, because let us understand why, to connect the nodes between each other, we need to connect
                          through network cables (ethernet cables)
                        - gives access to sensors and informs the user in case of failure
                        - we can see the text output of the system using a serial console access
                        - we could install on them operating systems from virtual disks, without a proper hardware support
                        - we van use it to upgrade the firmware
                        - we can access them by using standardized tools based on IPMI
                            - IPMI: 
                                - it stands for Intelligent Platform Management Interface is a set of computer interface specifications
                                  (specifically applied to computer subsystems)
                                - they provide management and monitoring capabilities with non dependance on the host system's CPU, firmware, 
                                  Operating system
                - Notes on OSI model:
                    - Network range: we have Local Area Network (LAN) (based on cables connected through Ethernet), Wireless Local Area (No wires)
                                     Wide Area Network: large, connected with Fiber, Cables, 4G, 5G
                    - the network is made of several layers (Network Layers) --> the model to describe this characteristic is the Open Systems 
                      Interconnect (OSI) model
                    - the OSI (it is important to observe that is referred to open systems) is a model that describes and standardizes the way 
                      network communication works!!



                LAB:
                - each station: 3 nodes
                - we need to connect everything from 
                - we need to connect all the cables
                - we can connect remotely through SISSA VPN
                - we can start compilation of gcc and let it go and check for installation the day after!
                - if we invert the LANS on the switcher, it doesn't really matter!!
                - we can check the connected cables after we have done it looking at the end of the room!!
                - VLAN - Segmentation
                    - we send information to same network? --> A lot of traffic --> we introduce a partition
                - DOcumentation online: temple.edu/mhpc/hpc-technology/hardware/setup.html
                - we can protect our ssh key through a protect key??
                - remote access to nodes: ssh, VNC interface card, remote desktop (OS dependent)
                - remote access (OS in-dependent):  
                - local access: local console, KVM, Serial console
                - it is important to know how to setup a cluster
                - afternoon: lessons, support --> morning: exercises
                - take a bunch of hardware, connect and setup as a cluster the must work!!
                - network basics --> setting up hardware --> installation through network --> cluster environment --> installing software -->
                  ......
                - READ!!!! : ROUTE commands: they will trust us --> no corrections from sort of compilers like in programming!!
                - DO NOT COPY PASTE COMMANDS!!!!!
                - Quizzes to answer before wednesday!!
                - ASK FOR HELP!! NO JUDGE!!
                - Reports: summarize what you did, the problems, how you fixed it --> they need to be complete --> describe how you 
                  completed the exercises, with what you struggled the most, summary of what we did, problems, solutions
                  - All must be pushed, also configuration files
                  - with no configuration --> 0 points
                  - some exercises don't need a report

                         - THE FUCKING POINT FOR DNS SYNTAX!!!!!!:
                             - THE POINT AT THE END OF THE WORDS
                               INDICATES THAT THE PREVIOUS WORD MUST BE TAKEN AS IT IS, WITHOUT ADDING THE DOMAIN, WHICH IS NORMALLY ADDED                                         BY DEFAULT!!!

                - Exercise 1:
                    - Transport layer: TCP, UDP
                    - Network Layer: IP Address SUbnet Mask, 
                    - Data-Link layer
                    - Master node (head node) --> compute nodes
                    - Set-up 2 different networks for each node --> they will be completely isolated
                    - Assignment IP addresses (management network) 
                    - We will need to cable everything before!!
                    - to connect to the rete nascosta :
                      ssh route IPaddress: this is: 172.16.1.2
                    REPORT:
                        - we connected the cables following the scheme depicted
                        - loopback: interface which works on kernel __> we see the first node! __> we see it inthe first node!!!
                            - it is assigned by marvin dynamically!!
                        - ip a: vedo anche l'ip address -- before we didn't see it!!
                        - try to understand what youre doing
                        - eno1,eno2, Io, eno3, eno4 : these are ports connected through network!!
                        - Supported link nodes? (After running ethtool eno2) : 
                            - Advertising: communication with the destination --> we communicate how much resources/requirements we have to the 
                              destination
                            - SUpported link modes: what the current port iscapable of (10base(and also the others) means 1000 MBytes)
                                - Full : they can send and receive at the same time; Half: they need to establish when one has to send and when
                                  the other has to receive
                            - Auto-negotiation: on -> the connection has been introduced with success
                            - Persistent dynamic IP configuration: every time we reboot the system , we don't have to call the DHC to generate
                              a new IP address explicitly --> it will bedone authomaticly at every reboot --> it is done in the config file
                              defined for the specific port!!! folder: /etc/sysconf ig/network-scripts --> configfile: ifcfg-eno1
                            - If we check the config file for the port on the master node, it doesn't have DCHP configured, because 
                              it is made in a way that the DCHP is called just on the first port, not on the other ones --> but the things can be
                              done in different ways --> in Temple cluster things are made in a different way --> the DHCP is set for each 
                              port, also the other ports
                            - DCHP: ip address is set dynamically!!
                            - Documenttation to set static persistent IP addresses: is reported to add a PREFIX (20) , WHY??:
                                - we set statically the ip address for eno2 and eno3, eno4 is not used!!! --> setting it statically 
                                  we can tell the computing nodes to communicate with the master always through the same IP!!!
                                - BMC = it is the node!!!i
                            - to retrieve the logs:
                              - tail -f /var/log/messages&
                            - BMC : in our case corresponds to the coloured cables connected to the cluster node chassis (no master node)!!!
                                - it is the green cable connected to the master node chassis!!
                                - the BMC works with access through IPMI -> it is managed through a microcontroller which is on the motherboard
                                  of te node, so it lets to have remote management also on the BIOS for example and to install OS etc...
                                - On the other hand, the cluster nodes are connected through SSH --> we can have access only through command line
                                - In fact, only after having installed the OS on the cluster nodes, we had access to them through SSH !!!!
                                - We can say that, from our configuration, each port corresponds to a connection with a proper IP address assigned!

                            - to restart the network interfaces (all of them)
                            - Do not forget to restart the DHCP server every time you modify the dhcpd.conf file: !!!!!!!!!!!!!!!
                            - ssh access: ssh Cluster2 --> password: mhpc2026 --> if you turn on : user: root, password: mhpc2026
                            - we have Domain Name System (DNS) Servers that perform lookups ()
                            - DNS server implementation is BIND also KNOWN AS named (Name Daemon)
                            - Forward lookup: when we resolve a domain name to its IP address!!
                            - We have domains and sub-domains
                                - they compose a FQDN (Fully qualified domain name)
                            - DNS query: client asks for a certain domain to DNS Server --> if it doesn't find the domain --> 
                                    - so DNS server will do another query to the Root Nameservers (Referral to TLD Nameservers)
                                    - then we have TLD (top level domain) nameservers for the top domains (.com, .net etc)
                                        - it responds with referral to the Authoritative Nameserver of a given subdomain.
                                    - or Authoritative Nameserver: responsible for a domain
                                    - IT IS LIKE A SORTING CENTER, NOTHING MORE
                                    - WHY IS USED?? : FOR REDUNDANCY, SECURITY, MANAGEMENT OF THE ISSUES
                                        - DNS syntax:
                                            - SOA: start of authority: indicates the Domain of the Authoritative Server DNS and the email of admin 
                                                   and other informations;
                                            - NS: name server --> is the name of the Authoritative Name Server we introduce (for example master.hpc.
                                            - A: it is used to map a domain, or subdomain to an IP address;
                                            - CNAME: Alias a subdomain to another subdomain;
                            - Reverse Lookup : the IP address itself is reversed
                            - SOA : Source of Authority --> it is present in 
                            - IPMI : it works through LAN connection!!!
                            - dig : command linux : tool to interrogate DNS name servers
                                - iptables-save:
                                    - -A : command lines that specify what rule I want:
                                            - input, output is initially all accepted --> what we want to do is to block every input, and let the
                                              input just from the local (private) ports
                                            - OUTPUT ACCEPT : IT lists the the IP that has passes through the installation --> we don't take
                                              into account these part --> is a sort of history
                                            - lo : accept, otherwise nothing works
                                            - tcp: ???
                                            - we set a DROP for INPUT and FORWARD : WE DO THIS BECAUSE WE FORBID EVERY COMMUNICATION IN INPUT
                                              EXCEPT THE ONE COMING FROM eno2 and eno3
                                            - we find the MAC addresses:
                                                - we find them by checking the traffic from the computing nodes
                                                - at first we saw just one MAC address when trying to retrieve the MAC address
                                                - then, to check the 2 MAC address, we detacthed the cables blue one per time 
                                                  in order to get the change of the MAC address after running the command to listen to the 
                                                  port eno3!!!
                                               - once we retrieved the MAC addresses, we need to set the Set up DHCP for the management network
                                                 to make the management node to set those MAC addresses !We set it through dhcp!!
                                               - Mac Addresses: 7c:c2:55:ed:2d:01 ; 7c:c2:55:ed:2d:d5  : 
                                                    - the MAC address 7c:c2:55:ed:2d:01 seems to be wrong!!
                                                        - the problem is not MAC address, it was one of the blue cables!
                                               - to retrieve the logs: 
                                                    - tail -f /var/log/messages&
                                               - when trying to rename, we get the error: 
                                                    systemctl start named
                                                    Job for named.service failed because the control process exited with error code.
                                                    See "systemctl status named.service" and "journalctl -xe" for details.
                                               - we run the command: systemctl started named, but we got:
                                                    systemctl status named.service
                                                       named.service - Berkeley Internet Name Domain (DNS)
                                                      Loaded: loaded (/usr/lib/systemd/system/named.service; enabled; vendor preset: disabled)
                                                      Active: failed (Result: exit-code) since Mon 2026-01-19 20:17:11 CET; 17s ago
                                                      Process: 55148 ExecStartPre=/bin/bash -c if [ ! "$DISABLE_ZONE_CHECKING" == "yes" ]; 
                                                      then /usr/sbin/named-checkconf -z "$NAMEDCONF"; else echo "Checki>
                                                      Jan 19 20:17:11 master systemd[1]: Starting Berkeley Internet Name Domain (DNS)...
                                                      Jan 19 20:17:11 master bash[55149]: /etc/named.conf:14: missing ';' before 'listen-on-v6'
                                                      Jan 19 20:17:11 master bash[55149]: /etc/named.conf:21: 'allow-query' redefined near 
                                                      'allow-query'
                                                      Jan 19 20:17:11 master systemd[1]: named.service: Control process exited, code=exited status=1
                                                      Jan 19 20:17:11 master systemd[1]: named.service: Failed with result 'exit-code'.
                                                      Jan 19 20:17:11 master systemd[1]: Failed to start Berkeley Internet Name Domain (DNS).
                                                      lines 1-11/11 (END)...skipping...
                                                       named.service - Berkeley Internet Name Domain (DNS)
                                                      Loaded: loaded (/usr/lib/systemd/system/named.service; enabled; vendor preset: disabled)
                                                      Active: failed (Result: exit-code) since Mon 2026-01-19 20:17:11 CET; 17s ago
                                                      Process: 55148 ExecStartPre=/bin/bash -c if [ ! "$DISABLE_ZONE_CHECKING" == "yes" ]; 
                                                      then /usr/sbin/named-checkconf -z "$NAMEDCONF"; else echo "Checki>
                                               - So we had to correct the config file: 
                                               - Now what we need to do it is to check if the foreard naming has been done correctly:
                                                    host c01.mgmt
                                               - but we get the error: Host c01.mgmt not found: 3(NXDOMAIN)i
                                               - HOW TO SOLVE? : we need to tell our network stack which name server to be used!
                                                - the domain name that is registered in the file /etc/resolv.conf is not related to our 
                                                  internal domain --> is an information that we need to specify and pass!
                                                - so we erase the line: nameserver 172.16.0.1 from the conf file and we leave just the other one
                                                  with the correct local IP address! we modified the file resolv.conf
                                                - to let us check with the host command without specifying the domain (.mgmt), we prepend 
                                                  the same conf file with the line : search mgmt
                                                - DNS is working (we checked), but we want to make sure that external(from outside)
                                                  DNS (Domain Name system) reqests to be forwarded to our DNS external SERVERS!!!!
                                                - So we add "forwarders" to the named conf file: 
                                                    - (we use the IP address 172.16.1.1 as forwarder) 
                                                    - file: /etc/named.conf
                                                - The changes we made to these last 2 config files, will not be persistent after reboot, so:
                                                    - if we remember, to make changes persistent after the reboot, we already modified the files in:
                                                        - /etc/sysconfig/network-scripts/
                                                        - in this case we modify the eno2 file config:
                                                         - we add the DNS
                                                         - we restarted just the eno2 interface, but after that, the resolv.conf is not the updated
                                                           one, is restarted!!
                                                         - I don't want this, how to solve it??
                                                            - I disconnect all the intrfaces (from 1 to 4) and I reconnect them (I reestart them)
                                                            - Now it works: 
                                                              - result in: resolv.conf_New
                                                - Now we want to do the reversed one:
                                                    - at first we check the name assigned to the IP address set during the previous procedure
                                                        - we see an error: ;; connection timed out; no servers could be reached
                                                        - we run the command for varlog and see that the error was a ; missing at the line 
                                                          14 of the forwarded line in the named conf file
                                                        - now we create the file : /var/named/192.168.1
                                                        - we modify the file /etc/named.conf adding: 
                                                          zone "1.168.192.in-addr.arpa." {
                                                              type master;
                                                              file "192.168.1";
                                                                };
                                                        - we restarted the interfaces verify the reversed naming:
                                                            systemctl restart named
                                                            [root@master ~]# host 192.168.1.1
                                                            1.1.168.192.in-addr.arpa domain name pointer c01.mgmt.
                                                        - Intelligent Platform Management Interface (IPMI)) installation:
                                                            - we have settings on the motherboard!!
                                                            - we installed it and we set a bash script to call the corresponding commands
                                                              in a more easy way: ipmiwrap
                                                            - we created also a password file:/etc/ipmi_password to avoid to expose it to the 
                                                              history !!
                                                            - Now we can set the Serial Console
                                                                - we enable the Serial Over Lan --> we can connect to a serial port of my BMC
                                                                - we run ipmiwrap c01 sol activate
                                                                - ipmiwrap c01 power cycle --> we run this connected on remote from the laptop
                                                                  --> the c01 will be rebooted : ipmiwrap c01 power cycle
                                                                  - pressing a lot of time delete to enter BIOS
                                                                  - They are already up to date
                                                            - So the configuration of the management network seems to be done!!
                                                        - We can do now the same configuration for the HPC network!! (HPC ports!!)
                                                        - REMEMBER!!!: 
                                                            - eno1 is the port connected to internet (to marwin)
                                                            - eno2 is the port for the management from the master 
                                                            - eno3 is the port on the master for the hpc connection to the nodes!!
                                                            - eno4 is not used!!
                                                    - 6th point: we need to re-do for HPC network
                                                            - the interface files for each port were already set, so we simply run a dump 
                                                              to check the traffic on the cluster port (eno3) on the master node and see the 
                                                              MAC address!!
                                                            - but to do this, we at first do a power cycle on c2: ipmiwrap c02 power cycle
                                                            - at the same time, on another terminal, we run the dump command to get the MAC address
                                                              tcpdump -i eno3 port bootps
                                                            - we do the same for the c1 host (the other slave node)
                                                            - we create a subnet in the /etc/dhcp/dhcpd.conf file referred to the hpc network
                                                            - we test with ping IP address c01-host and ping IP address c02-host
                                                            - NOO!! We cannot ping on the HPC!! WHY???
                                                            - how to verify? We need to activate the varlogs --> I do a power cycle of one host
                                                                - at the same time, I do a sol activate of the same host
                                                                - if the IP address that we see is the one we expect, everything is fine!!
                                                                - REMEMBER!!: DHCP and NAMING are totally different things!!!
                                                                - We can see that the logs give as IP addresses the IP addresses of the hpc 
                                                                - we do this because the changes seem to not be read by the port 
                                                                  --> so we do a sort of reboot!!
                                                            - what we do when we do a power cycle?? We simply power off and on through the BMC!!
                                                    - regarding the name resolution, we don't need to modify directly the file /etc/resolv.conf
                                                    - we need to modify the network interface for the eno3, like we did for the eno2
                                                      and then we do down and up of the eno3 and we should see the file /etc/resolv.conf modified
                                                      authomatically --> it worked, the file .conf was automatically updated
                                                    - But we had another issue: 
                                                        - by trying to check the IP addressed corresponding to the c01.hpc and c02.hpc, we don't
                                                          see anything
                                                            - how we solved? We tried to restart the network interface eno3 and to retrieve the logi
                                                            - but no errors from the logs
                                                            - SOlution: when we are doing the resolving name part, we are using essentially named
                                                              which is a specific tool, so to make it work, and see the changes, we need to 
                                                              restart as well named: systemctl restart named!!
                                                    - Reverse lookup 6th point:
                                                        - to do it, we need to add a new zone to the file: /etc/named.conf
                                                        - and we need as well to create a proper file: /var/named/192.168.1i
                                                        - we created /var/named/192.168.1 and we added the zone to the file: /etc/named.conf
                                                        - but we had an issue when trying to do the systemctl restart named -->
                                                            - so we did: notice that, in the zone added to the .conf file, there is the inverted
                                                              IP address!!!! --> so the right IP address to put is: 17.168.192 instead of 
                                                              1.168.192
                                                            - then we were able to run the systemctl restart named --> and to do: host 192.168.17.2
                                                              and host 192.168.17.1 with success!!:
                                                              1.17.168.192.in-addr.arpa domain name pointer c01.hpc.
                                                              2.17.168.192.in-addr.arpa domain name pointer c02.hpc.


                                      - EXERCISE 2:
                                        - installing operating system in a computing node
                                        - they are isolated
                                        - we cannot communicate between management and cluster network --> we can do it by passing through master
                                          but notin our case!
                                        - we can check temp., power, 
                                        - the network (especially the cluster) must work!!!
                                        - IPMI: sensor data, 
                                        - Core services: DHCP --> it will assign dynamically addresses
                                        - in hpc we usually don't do dynamical assignment!
                                        - TFTP: sending informations back and forth --> to be able to download files from server to a client
                                        - HTTP : serve CentOS to boot the installer
                                        - tcpdump: we can see the traffic
                                        - kickstart: to authomaticly install from network
                                        - Report:
                                            - we at first check the prerequisites
                                            - we do network booting! How? By making the DHCP server to send infos to PXE cient
                                                - so we need to tell the DHCP server which server will provide files (through TFTP)
                                                  and which boot program needs to be loaded
                                                - we add: allow booting;
                                                          allow bootp;
                                                - and we add the next-server and the filename
                                                - we add them to the subnet introduces for the management network
                                                - then of course we restart dhcp: systemctl restart dhcp
                                                - we enable and start tftp !!
                                                - we run tftp localhost  --> to run tftp locally!!
                                                - by running: systemctl status tftp, we see that the Tftp server service is active!!
                                                - we need to install syslinux and pxelinux:
                                                    - we run: yum install syslinux syslinux-tftpboot
                                                    - we copy some files: pxelinux.0, menu.c32, ldlinux.c32 to the library folder of tftp
                                                     (/var/lib/tftpboot)
                                                    - we go to the home directory and we try to download the pxelinux.0 file through tftp
                                                    - we verified that the pxelinux.0 file was present in the directory
                                                    - we create a PXE linux configuration folder:
                                                        - mkdir -p /var/lib/tftpboot/pxelinux.cfg
                                                        - we create default configuration file in: /var/lib/tftpboot/pxelinux.cfg/default
                                                        - we create a customized Boot Menu for the host c01:
                                                            - so we create a config file which will have as name the exadecimal version of 
                                                              the IP address of one of the compute (cluster) hosts (are they nodes or hosts?)
                                                            - we create a file in /var/lib/tftpboot/pxelinux.cfg/ named like the exadecimal
                                                              number
                                                            - we populate the file in similar way as the default file
                                                            - IMPORTANT!!: FOR EACH STEP, IT IS GOOD TO POWER OFF AND THEN POWER OFF THE HOSTS
                                                              IN ORDER TO BE SURE THE CHANGES ARE EFFECTIVE!!!!
                                                            - we run: ipmiwrap c01 power off, ipmiwrap c01 power on, ipmiwrap c01 sol activate  
                                                            - BUT we had an issue: CLIENT MAC ADDR: 7C C2 55 ED 2A B4  GUID: 00000000 0000 0000 0000                                                                7CC255ED2AB4 PXE-E53: No boot filename received
                                                            - WHY? --> there was an issue in the dhcp.conf file !!!
                                                                - the 2 lines needed to make DHCP server send info to PXE client and so on
                                                                  must be added to the computing (claster) submask, not to the managment one!!!
                                                                - so we fixed it and we run another time:
                                                                  ipmiwrap c01 power off --> ipmiwrap c01 power on --> ipmiwrap c01 sol activate
                                                                - But we had an issue: the slave node didn't recognize the preboot file (pxelinux.0
                                                                - WHY?? : Because we didn't activate tftp before running the procedure!!
                                                                    - so we a systemctl enable tftp --> systemctl start tftp -->
                                                                      I check that it is active: systemctl status tftp
                                                                      - Then I do a poer cycle (the power cycle turns off and on the node quickly)
                                                                      - And after the reboot I have the screen expected!!
                                                                - Then: we need to consider that: Compute nodes are not able to access 
                                                                        the internet in our configuration
                                                                - So we need to host the contents of a AlmaLinux ISO image locally 
                                                                  from our master node
                                                                - Then we installed the httpd package : yum install httpd
                                                                - We enable and start the service: 
                                                                    - THIS IS REALLY IMPORTANT!!!: BECAUSE IF WE DON'T DO IT, IT WILL NOT WORK!
                                                                - We Copy Almalinux distribution Data:
                                                                    - we download ISO to the scratch folder
                                                                    - we mount the image (we add permissions for it) in a specific folder 
                                                                      (/mnt/iso)
                                                                    - we copy contents to folder and we copy AlmaLInux Installer kernel and initrd
                                                                      for PXE boot into a directory almalinux8 in the folder: /var/lib/tftpboot/
                                                                    - after finishing, we Unmount ISO image
                                                                - Manual installation:
                                                                    - I modified the file: C0A81101 (lines added to get the installer location)
                                                                    - We restarted c01
                                                                    - we did the manual installation!!
                                                                - Now we need to do the proper stuffs for the Kickstarter
                                                                    - a kickstarter file is a file made to make an authomatic installation
                                                                    - to understand what to do, we need to create a file similar to one that
                                                                      is on the computing node c01 (on which we installed the OS)
                                                                    - so we connect to the node c01.hpc through ssh!! 
                                                                    - BUT, ONE MOMENT, IF WE TRY TO  REBOOT THE C01, WE SEE THAT AFTER THE BOOT
                                                                      NOTHING IS SHOWN --> THIS IS DUE TO THE FACT THAT WE DON'T HAVE A GRAPHICAL
                                                                      HARDWARE PHYSICAL INSTALLED
                                                                    - we take that file on the c01.hpc node --> we need a similar file on the
                                                                      master!
                                                                    - we create a file on the master node:
                                                                        - mkdir -p /var/www/html/kickstarts
                                                                    - we create a configuration file for the node c2, because we want to install
                                                                      through kickstart on the node c02!! C0A81102
                                                                    - we added a proper section for kickstarter in the pxe config file on the 
                                                                      node 2 (C0A81102)
                                                                    - in the same file we moved the MENU DEFAULT option from the local section to
                                                                      the kickstarter section --> in this way we have that the default entry
                                                                    - we modified the file: minimal.ks:
                                                                        - we set text instead of graphical at row 3 (to not have graphic interface)
                                                                        - we set kexec-tools, @core, chrony as the packages
                                                                            - wedon't want any graphical package for installation!!
                                                                        - we disbled the firstboot
                                                                        - we modified the row line 30 to clearpart --all --initlabel because
                                                                          we want that at each installation the disk is overwritten
                                                                        - we added a reboot (it is not important the position where we put it!!)
                                                                          because it is just a configuration file : the instructions are not 
                                                                          executed as they are depicted!
                                                                        - now we can try to verify that the c02 will be installed!!
                                                                        - but doing a power cycle, it doesn't work!! 
                                                                            - remember that, with no intention, we clicked on the booting botton!
                                                                            - to solve the issue, it was necessary to do a reset for the c02 node
                                                                            - but the problem is that, now we have that the installation in not
                                                                              authomaticly triggered, to do so, we need to add a Target to the 
                                                                              pxe config files (we can add the target also for the default one, it
                                                                              doesn't make so much sense because the default one will be always 
                                                                              run on local (not directly from a url)
                                                                            - He was still installing letting to select the installation at boot!1!:
                                                                              ONTIMEOUT local with 
                                                                              ONTIMEOUT installer-kickstart
                                                                            - Now we retry!!! with the only c02
                                                                            - the installation through KIckstarter starts authomaticly after 
                                                                            - at the end we changed the line another time to:
                                                                                ONTIMEOUT local
                                                                            - Now we can finally do: ssh c02.hpc !!! 

                                      
                                      - EXERCISE 3 :
                                        - we installed on the computing node from the master node
                                        - we have to change files for each node and each service --> nobody writes by hand
                                            - especially for errors
                                        - for example: frontier: 10000 nodes
                                        - tool: Cobbler:
                                            - cobblerd: 
                                            - created configurat. files for us using templates
                                            - template: general file --> they are not exactly the same
                                                - even though we are using the same nodes names, we have for example different MAC addresses
                                            - the changes will be made to the general file
                                            - backup all the files!!!!
                                            - delete just the files, not the folders!!
                                            - distribution: OS installat. media import in cobbler that does a network installation using
                                              a distribution
                                            - Profile:
                                                - settings and kickstart file.....
                                            - rebooting the nodes, it will reinstall the OS again and again --> it should be in this way
                                            - collect all MAC addresses for our BMCs and compute nodes
                                            - the files that we need to delete are the files we modified, not the other ones
                                        - START:
                                            - we do a backup of the folder where files we modified are stored
                                            - then we need to delete just the files WE modified
                                            - then we stop the services:  dhcpd, httpd, named
                                            - we delete /var/lib/tftpboot/pxelinux.0
                                            - we delete the files in /var/lib/tftpboot/pxelinux.cfg
                                            - we disable the dhcpd, httpd and named 
                                                - in this way we disable DHCP, HTTP communication and DNS !! --> because we want Cobbler to set
                                                  them automaticly
                                            - we install EPEL repositories: yum install epel-release
                                            - we have an error: Curl error (6): Couldn't resolve host name for https://mirrors.almalinux.org/mirrorl                                              ist/8/extras [Could not resolve host: mirrors.almalinux.org]
                                                - WHY? BECAUSE WHEN WE STARTED TO DO THE LAB, AT FIRST WAS MARVIN TO PROVIDE COMMUNICATON NETWORK
                                                  AND THE NETWORK ACCESS IS PROVIDED BY IT. SO IN EACH NETWORK INTERFACE WE SET PEERDN=No
                                                  WHY THIS? BECAUSE AFTER THAT, WE SET OUR SERVER DHCP AS THE IP ADDRESSES CORRESPONDING 
                                                  TO THE 2 PORTS OF THE MASTER NODE (an ip address of the server corresponding to each 
                                                  NETWORK THAT WE INSTALLED MANUALLY --> THIS IS WHY WE HAVE 2 CABLES CONNECTED TO THE EXTERNAL
                                                  LINE FOR EACH MASTER NODE???????
                                                - NOW THE NETWORKS WE CREATED ARE ERASED, SO WE NEED TO RESET THE EXERNAL INTERNET COMMUNICAT.
                                                  TO BE GIVEN BY MARWIN, AS IT WAS AT THE BEGINNING!!!
                                                - SO WHAT WE NEED TO DO IS TO RESET THE COMMUNICATION TO EXTERN TO HAPPEN
                                                - SO WE ASSIGN, IN THE FILE /etc/resolv.conf, the ip as the ip of Marwin!!!
                                                - we had an issue --> typo error!!! sever instead of server!!
                                                - we install cobbler
                                                - then we enable the services needed for Cobbler to make it work!!
                                                - but the check cobblerd gives some missing parts to be fixed!!!:
                                                    - we need to fix them!! 
                                                    - we need to set the server and next-server: we need to set them as the custer network subnet
                                                      which we conigured (192.168.16.1) --> so cobbler must use the master.hpc IP address
                                                      (remember that the master.mgmt IP address is: 192.168.0.1
                                                    - so we modified settings.yaml!
                                                    - we install yum-utils rsyncd 
                                                    - wwe install : yum install pykickstart (to run kickstart)
                                                    - we restarted cobblerd daemon : cobblerd sync
                                                    - we go to the directory: tftpboot: cd 
                                                    - we install tree: yum install tree
                                                    - now by running the command tree in the folder, we can see that cobbler has already
                                                      installed some files
                                                    - now we need to configure the DHCP and DNS through Cobbler:
                                                        - we change the file: /etc/cobbler/settings.yaml
                                                            - we modify from manage_dhcp: false to manage_dhcp: 1
                                                            - after doing it, we restart Cobbler using sync:
                                                              cobbler sync
                                                                - NOOO!! running cobbler sync we didn't restart the cobbler service, in fact
                                                                  after cobbler sync we didn't see any error!! (we should see a task failed):
                                                                  *** TASK COMPLETE ***
                                                                - to restart the cobbler service we should do: 
                                                                  systemctl restart cobblerd !!!
                                                                - Now if we run: cobbler sync , we see: (COBBLER SYNC APPLIED THE CHANGES MADE TO
                                                                  THE settings file of Cobbler !!!

                                                                  Exception occurred: <class 'cobbler.cexceptions.CX'>
                                                                  Exception value: 'Cobbler trigger failed: cobbler.modules.sync_post_restart_services'
                                                                  Exception Info:
                                                                  File "/usr/lib/python3.6/site-packages/cobbler/remote.py", line 98, in run
                                                                  rc = self._run(self)

                                                                  File "/usr/lib/python3.6/site-packages/cobbler/remote.py", line 212, in runner
                                                                  self.remote.api.sync(self.options.get("verbose", False), logger=self.logger)

                                                                  File "/usr/lib/python3.6/site-packages/cobbler/api.py", line 1302, in sync
                                                                  sync.run()

                                                                  File "/usr/lib/python3.6/site-packages/cobbler/actions/sync.py", line 139, in run
                                                                  utils.run_triggers(self.api, None, "/var/lib/cobbler/triggers/sync/post/*", 
                                                                  logger=self.logger)

                                                                  File "/usr/lib/python3.6/site-packages/cobbler/utils.py", line 924, 
                                                                  in run_triggers raise CX("Cobbler trigger failed: %s" % m.__name__)

                                                                  !!! TASK FAILED !!!

                                                            - This command will generate a new dhcpd.conf and reload the dhcpd service
                                                            - ONE MOMENT: HOW IT DOES THIS?
                                                                - cobbler uses configuration files templates
                                                                - these templates (generic files) are used to generate the new configuration file
                                                                - WHERE IS THE TEMPLATE FOR DCHPD? IT IS IN: /etc/cobbler/dhcp.template
                                                                - but this template file has wrong IP addresses:
                                                                    - we need to modify it in line with the dhcp.conf file that we had for 
                                                                      the Exercise 2!!!
                                                                    - so we modified the file : /etc/cobbler/dhcp.template
                                                                - we run: cobbler sync to udate the dhcp configuration
                                                                - ATTENTION!! : cobbler sync is a command that we use to recreate configuration
                                                                                files!! (from the template files!!!)
                                                                                - NOT ONLY THIS: IT ALSO RELOAD THE SERVICES FOR COBBLER
                                                                - DNS configuration:
                                                                    - we modify the settings.yulm file to activate dns management and to 
                                                                      modify the bind_master
                                                                    - we restart the cobblerd service: systemctl restart cobblerd
                                                                    - REMEMBER: when we modify settingd --> restart cobbler services
                                                                                modify templates --> cobbler sync
                                                                    - we do a cobbler sync to make cobbler generate a template file for Binding:
                                                                        - /etc/cobbler/named.template
                                                                        - we modify the file in line with what we have for 2 exercise
                                                                    - we reload the bind daemon (named) and we do a cobbler sync! 
                                                                    - now we need to configure a profile, because this let us configure groups
                                                                      of systems in the same way, for example compute nodes, login nodes
                                                                    - installation source for profiles: distributions
                                                                    - so we need to install both distribution and profile
                                                                    - we import thr Almalinux distribution from our ISO file
                                                                    - we do a wget of the Almalinux ISO image (like we did for the Ex.2)
                                                                    - we mount the iso image for AlmaLinux and we run a command to 
                                                                      to import a distribution from /mnt/iso into cobbler
                                                                    - we check with: cobbler distro list
                                                                    - we checked profile command with : cobbler profile reporti
                                                                    - we unmount the image: umount /mnt/iso
                                                                    - now we will introduce a system --> it is assigned to a profile
                                                                        - it is related to the network interface, to the physical nodes
                                                                    - we create a system and we verify that it has been created
                                                                    - ATTENTION!! : the nextserver line in the template file: dhcp.template
                                                                        - this line must be added to the cluster subnet part, not to the 
                                                                          management one!! --> because it is a line added for boot 
                                                                          configuration --> we tell that the boot will be done with 
                                                                          installation from network with the provided file pxelinux.0??
                                                                          In general, the installation from network is required for 
                                                                          the cluster network!!
                                                                    - Now we need to configure the Network inerfaces:
                                                                        - management:
                                                                            - now we check that the system has changed:
                                                                              cobbler system report
                                                                        - WARNING!!: how to check how much systems we have? :
                                                                          - cobbler system list
                                                                            - there is a new section: Interface
                                                                            - now we can do a sync to update the conf files
                                                                            - we do the same for both hosts (c01 and c02)
                                                                        - hpc: 
                                                                            - we simply add the interface to the system
                                                                                - the system is linked to the physical nodes,
                                                                                  so we will have a system for each node
                                                                     - We Need to tell Cobbler daemon to create zone for forward and reversed
                                                                       files for domains
                                                                        - so we edit the settings.yaml and we change a value to manage forward 
                                                                          zones. It will have added zones to the named.conf and also
                                                                          the files which the config file points to
                                                                        - after modifying the settings file, a file /etc/named.conf has been 
                                                                          created
                                                                        - the files are created from the template: /etc/cobbler/zone.template
                                                                        - instead of modifying this template, we create new template files
                                                                          in the folder /etc/cobbler/zone_templates/ --> mgmt and hpc
                                                                        - But we receive an error at the sync: !!! TASK FAILED !!!
                                                                            - How to solve it and why??:
                                                                                - we removed the line:
                                                                                  master                  IN      $cobbler_server.
                                                                                  from mgmt and hpc !!!
                                                                        - Now we do theprocedure also for the forward name resolution!!
                                                                            - we modify the manage_reverse_zones line in the settings!
                                                                            - we created files for 192.168.17 and 192.168.1 in templates_zones 

                                                                       - Now we create a system also for c02 and we configure as well a network
                                                                         interface :
                                                                       - PROBLEM:
                                                                            - I noticed that in both mgmt and hpc, the 
                                                                            IN      NS      192.168.16.1.  --THIS IP IS ALWAYS THE SAME FOR EVERY
                                                                                                             FILE FOR THE ZONES!!!
                                                                       - After fixing it, we has an issue: trying to ping or host, we saw:
                                                                         ping c01.mgmt
                                                                         ping: c01.mgmt: Name or service not known
                                                                       - WHY???? We checked the system created (the MAC addresses for each node)
                                                                         we corrected them but STILL THE ISSUE
                                                                            - WE CHECKED etc/cobbler/named.template --> there was an error
                                                                                - we updated the listen part:
                                                                                    listen-on port 53 { localhost; servicenets; };
                                                                                - we checked all the template files
                                                                                - but it didn't work
                                                                                - SOLUTION: the problem was due to one of the first steps,
                                                                                  where we modified the file /etc/resolv.conf:
                                                                                  the lines needed for searching the ip addresses of the hosts 
                                                                                  were commented!!!!!!
                                                                                - YOU NEED THOSE LINES!!!!!!!
                                                                                - This file is automatically generated by the dhclient-script 
                                                                                  uses our network configuration scripts.
                                                                     - Network Booting with Cobbler:
                                                                        - control PXE network boot : it generated TFTP and PXELINUX config files
                                                                          Netbooting: verify with cobbler system report
                                                                        - We use hpc network for provisioning 
                                                                        - we check the PXE configuration files (always in the folder which we saw
                                                                          for the exercise 2) :
                                                                              /var/lib/tftpboot/pxelinux.cfg/
                                                                        - inside we have a config file for each MAC address
                                                                        - We want to add Kernel options at Profile level, so we check the 
                                                                          profile: almalinux8-x86_64
                                                                        - we add serial console options with the terminal command
                                                                        - The Kernel options were added succesfully
                                                                        - PXElinux were updated!!
                                                                        - NOW We CONTROL Network boot:
                                                                            - we check the Netboot enabled or not:
                                                                              cobbler system report --name=c01 | grep Netboot
                                                                            - we try to set the Netboot to false from command line
                                                                            - we canfigure cobbler to disable network boot after the first 
                                                                              completion --> we modify preboot_justonce in the setings file
                                                                      - Kickstart Templates with Cobbler:
                                                                        - Cobbler uses a python based template system to generate the 
                                                                          Kickstart in the PXE boot 
                                                                        - so we can modify Template files, managed by Ceetah, instead of
                                                                          manually modifying the kickstart file
                                                                        - furthermore, this system let to assign a specifi kickstart 
                                                                          to a profile or a system for specific requirements
                                                                        - folder with templates: /var/lib/cobbler/templates the file is sample.ks
                                                                      - PROFILE CREATION:
                                                                      - Install Compute node:
                                                                            - we remove redhat_register line 66 from the sample.ks template file
                                                                            - we create a child profile of almalinux:
                                                                           cobbler profile add --name=almalinux8-compute --parent=almalinux8-x86_64
                                                                            - we insert compute.ks as the template for installation:
                                                                            cobbler profile edit --name=almalinux8-compute --autoinstall=compute.ks
                                                                            - we do :
                                                                                cobbler system edit --name=c01 --profile=almalinux8-compute
                                                                                cobbler system edit --name=c02 --profile=almalinux8-compute
                                                                            - to add our 2 systems to the new profile
                                                                            - we modify the compute.ks to add packages present in the kickstarter
                                                                              for the manual installation (2nd exercise)
                                                                            - we create a file /etc/motd to visualize a snippet at the login
                                                                            - we activate netboot for c01 and c02 systems:
                                                                                cobbler system edit --name=c01 --netboot=1
                                                                            - we do a power cycle of the c01 bmc: ISSUE: nothing happened
                                                                                - how we solved? --> the file pxelinux.0 wasn't in the folder:
                                                                                  /var/lib/tftpboot/
                                                                                - we added it --> installation seems to work, but
                                                                                  we are not able to access!! Password wrong!
                                                                                - we try to access from the master with ssh:
                                                                                    - we remove hostkeys:
                                                                                        rm ~/.ssh/known_hosts
                                                                                - also repeating the procedure on the node c02, we are not
                                                                                  able to access with the password 2026!!
                                                                                - HOW WE SOLVED:
                                                                                    - the password required is a default password, it is not 
                                                                                      a defined one, so we need to modify the default password
                                                                                      byencrypting the desired one with: openssl passwd -1
                                                                                    - then we put the result in the file setting.yml in the 
                                                                                      tag: default encrypted password
                                                                                    - to reinstall, we need to set the net boot to 1:
                                                                                      cobbler system edit --name=c01 --netboot=1
                                                                                      cobbler system edit --name=c02 --netboot=1
                                                                                    - but the password was still not accepted, why?
                                                                                      because we didn't do: 
                                                                                      systemctl restart cobblerd !!!!
                                                                                    - 
                                                                                
                                                                                                                
                                                    - EXERCISE 4:
                                                        - some commands are meant to be run on the compute nodes, pay attention!!
                                                        - root user should be able to access all compute nodes
                                                        - root should be able to jump from one node to the another one
                                                        - extra key to be used by SSH client to authenticate
                                                        - NTP
                                                        - NFS

                                                        - First we create a new key pair (public-private)
                                                            - we disabled password authentication and root login and tested it!!
                                                            - Now we need to configure the system to make possible for the root to move between the
                                                              nodes --> we generate a new SSH keypair to afford this!!
                                                            - I copied the new keys public and private into the .ssh folder of the computing nodes 
                                                              with scp 
                                                            - then we comcatenated the public keys to the authorized keys with: 
                                                              cat public key >> authorized keys
                                                            - Step 1.4.1 and 1.4.2: we need to add to the authorized_keys file the public key 
                                                              corresponding to the further internal key generated to give the root on 
                                                              the management node to have access to the comuting noded
                                                              --> to do this, it is essential that the internal key is added to the authorized_key
                                                              which is on the computing node -> it is not necessary to add it also in the 
                                                              management node, because in that way we would letalso the opposite (root access from 
                                                              comp. node to management node)
                                                            - 1.4.2: we do it authomaticly through a snippet Kickstart : 
                                                                - we launch the keygen
                                                                - we copy the keys to the computing nodes folder
                                                                - we concatenate on the authorized_keys
                                                                - But we need to pass manually the generated internal key on the master node
                                                                    - so we need to put the generated internal key in a snippet (like the logo)
                                                                    - use the snippet in the snippet that we created for the current purpose
                                                                    - to create a file, the command inside a snippet is:
                                                                        cat > /etc/motd <<EOF
                                                                        BODY
                                                                        EOF
                                                                        It will create a file at the specified path containing the body!!
                                                                    - So we did a first MISTAKE: WE CANNOT INCLUDE IN A SNIPPET THE COPY FROM
                                                                      master to nodes!!!! --> because the kickstart executes instructions on the 
                                                                      compute nodes, not on the master!!
                                                                    - the file we created is called rootkeys
                                                                    - we added reference to the snippet to the kickstart file for installation
                                                                    - we reboot the first node to check if the changes are implemented!!
                                                                        - cobbler system edit --name=c01 --netboot=0
                                                                    - when we connect to a host, a check is made on /etc/ssh/ssh_known_hosts
                                                                    - you will have to accept that server!! 
                                                                    - so the server will be added to the good fellas folder
                                                                    - we didn't have access through ssh because of the settings.yml!!
                                                                    - and we had also the problems with the hostkeys, which were not in the 
                                                                      known_hosts file on the Master!!!
                                                                    - after doing that, we observed that the problem ws still there:
                                                                        - to understand what happened, we can simply take the Anaconda.cfg file
                                                                          on the c01 node, which is accessible through IMPI (remember that the IPMI
                                                                          works through LAN connection)
                                                                        - the problem is that: 
                                                                            - the public key is truncated
                                                                            - the folder .ssh is not in the filesystem of c01 by default
                                                                        - After having fixed it, THE SSH DIDN'T WORK!!!!
                                                                        - WE CHECKED IN THE c01, and we saw that the files were the old ones!!!
                                                                        - SO THIS MEANS THAT WE FORCED THE BOOT BEFORE THE INSTALLATION WAS 
                                                                          FINISHED!!!
                                                                        - ATTEMNTION!!!!:
                                                                            - WHEN WE DO A POWER CYCLE WITH NETBOOT TRUE, WE CANNOT 
                                                                              CLICK SELECTING FROM THE BOOT MENU, WE NEED TO WAIT!!!!!!
                                                                            - WE REBOOTED CORRECTLY --> ANOTHER ISSUE --> DIRECTORY COMPLETELY 
                                                                              WRONG!!!
                                                                                - SO WE saw THAT THE PROBLEM IS THE TILDE IN THE KICKSTART!!!
                                                                                - REPLACE IT WITH /root/ !!!!
                                                                                - THIS IS DUE TO THE FACT THAT THE TILDE IS NOT YET DEFINED 
                                                                                  DURING THE INSTALLATION PROCESS!!!
                                                                            - TO CHECK WHAT WAS GOING ON DURING THE INSTALLATION WITH KICKSTART
                                                                              FILE --> WE CHECK the anaconda.cfg file
                                                                        - PAY ATTENTION TO THE REBOOT!!!!! WE NEED TO SET THE Autoinstall Netboot at                                                                          first to false and then to True!!
                                                                        - so we rwplace the tilde in the Kickstart file and check the result!!
                                                                        - we do a Net reboot to do a new installation --> it worked!!
                                                                     - Now we need to think about the ssh host key:
                                                                        - when we first do ssh node.hpc, we get a message of warning
                                                                          because the IP address of the host that asks for access (in this case
                                                                          is called server) is not known by the current machine
                                                                        - in order to solve the issue, we can do a key-scan:
                                                                          ssh-keyscan -H -t ecdsa c01 >> /etc/ssh/ssh_known_hosts
                                                                          ssh-keyscan -H -t ecdsa c01.hpc >> /etc/ssh/ssh_known_hosts
                                                                          ssh-keyscan -H -t ecdsa 192.168.17.1 >> /etc/ssh/ssh_known_hosts
                                                                        - with the keyscan, we populate the file /etc/ssh/ssh_known_hosts
                                                                        - So in this way the host key will be known
                                                                        - ATTENTION!!: when we try to connect through ssh, SSH checks 
                                                                          the /etc/ssh/ssh_known_hosts file --> this file is populated
                                                                          depending on the files in the same directory called: ssh_host*
                                                                        - PROBLEM: 
                                                                            - consider that I reinstall a compute node:
                                                                                - for the node will be created new keys --> so the hostNames
                                                                                  that include also the key of the specific node, will not
                                                                                  include anymore the key of the reinstalled node
                                                                                  --> so we will need to run again ssh-keygen
                                                                                - to avoid this, we can create 2 scripts:
                                                                                    - 1 script retrieve the files ssh_host* from the computing 
                                                                                      node that we need to reinstall (not from the master, where 
                                                                                      we are) and copy them in a folder related to the specific
                                                                                      node. This is important because these files are called
                                                                                      all in the same way, so the only way to have a distinction
                                                                                      is to add them to different folders;
                                                                                    - Once the installation is done, we need to run a second 
                                                                                      script, that will copy the files that were stored 
                                                                                      previously, in the folder /etc/ssh of the newly installed
                                                                                      node --> in this way, the host will be recoignized
                                                                                      also after the installation
                                                                                    - IT IS IMPORTANT TO NOTE THAT THE COPY IN THE SECOND SCRIPT
                                                                                      IS DOABLE THANKS TO THE StrictKeyChecking to false,
                                                                                      otherwise the copy from local to the comp. node is 
                                                                                      forbidden
                                                                                    - so we created the scripts:
                                                                                        - etrieve_script.sh and copy_script.sh 
                                                                                    - we created the folder:
                                                                                        - retrieve_keys
                                                                                    - but the second script run gave:
                                                                                        /copy_script.sh c01
                                                                                       ssh:Could not resolve hostname host:Name or service not known
                                                                                       @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
                                                                                       @    WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!     @
                                                                                       @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
                                                                                    - We had several issues:
                                                                                        ./copy_script.sh c01
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@    WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!     @
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!
Someone could be eavesdropping on you right now (man-in-the-middle attack)!
It is also possible that a host key has just been changed.
The fingerprint for the ECDSA key sent by the remote host is
SHA256:PRogHMymN0amndLdRtJAcqYrgX+0qpWPaHMGSW9tzoo.
Please contact your system administrator.
Add correct host key in /root/.ssh/known_hosts to get rid of this message.
Offending ECDSA key in /etc/ssh/ssh_known_hosts:1
Password authentication is disabled to avoid man-in-the-middle attacks.
Keyboard-interactive authentication is disabled to avoid man-in-the-middle attacks.
Last login: Sat Jan 24 02:27:09 2026 from 192.168.16.1
[root@c01 ~]# exit
                                                                                    - Strangely, despite the script didn't copy the files,
                                                                                      the system let access to the node, 
                                                                                      ma ssh c01.hpc didn't work
                                                                                    - SOLUTION:
                                                                                        - adding: UserKnownHostsFile=/dev/null:
                                                                                            - this temporarily disables known_hosts
                                                                                              for copying files to the node!!
                                                                                            - ASK MORENO!!!
                                                                                            - but after a new test, we got the message:
                                                                                              No ECDSA host key is known for c01.hpc and you have requested strict checking. Host key verification failed.
                                                                                              - despite being able to access
                                                                                              - HOW TO SOLVE:
                                                                                                - we added the lines: -o StrictHostKeyChecking=no \
    -o UserKnownHostsFile=/dev/null \
                                                                                                - to the ssh command in the copy script!!
                                                                                 - STEP 2:
                                                                                    - We need to introduce a NTP (Network time protocol)
                                                                                    - we need to synchronize clocks across different networks
                                                                                      or different nodes? Why is it necessary??
                                                                                    - we use our maste node as time source (taking it from the 
                                                                                      external internet connection) for the other nodes
                                                                                    - we make NTP server (the master) to use Global NTP Pool 
                                                                                      Project as reference clock:
                                                                                    - we provide time even when NTP is not synchronized with 
                                                                                      upstream network nodes:
                                                                                      we uncomment: local stratum 10
                                                                                    - we do a systemctl restart chronyd (after we check that it is
                                                                                      running running the line with status)
                                                                                    - we run: chronyc tracking OR chronyc sources
                                                                                        - for sychronization status
                                                                                    - the compute nodes are Chrony clients
                                                                                    - we remove Almalinux repos (ON NODES):
                                                                                    - NODES: 
                                                                                        - we install chrony and edit the config file
                                                                                        - we removed all the almalinux* files in the repo
                                                                                          and installed chrony
                                                                                        - we installed bind bind-utils
                                                                                        - but we are not able to see anything by doing:
                                                                                          chronyc source
                                                                                          because in the configuration file we specified that the 
                                                                                          source server NTP should have been:
                                                                                          ntp.hpc
                                                                                        - so we need to edit DNS configuration to introduce 
                                                                                          this name resolution:
                                                                                            - we modify the named.conf file
                                                                                            - we create a file hpc for the hpc domain
                                                                                            - but even modifying it, we had an issue when running:
                                                                                              host ntp.hpc: 
                                                                                              Host npt.hpc not found: 3(NXDOMAIN)
                                                                                            - WHY? we checked the file hpc:
                                                                                                we corrected the file with the line:
                                                                                                IN      NS      master.hpc.
                                                                                                npc                     IN      A       192.168.16.1
                                                                                            - this means that npc is an alias for master.hpc!!
                                                                                            - IF WE DON'T ADD THIS, THE HOST COMMAND WILL NEVER 
                                                                                              GIVE RESULTS!!
                                                                                            - WARNING!!!!: 
                                                                                                - WE MADE A HUGE MISTAKE!!!:
                                                                                                  - the name resolving is based on DNS, which 
                                                                                                    is a server who provides names/aliases, 
                                                                                                    SO IT IS INDEPENDENT FROM THE PARTICULAR
                                                                                                    NODE WHERE WE DEFINE IT, ONCE THE LOCAL
                                                                                                    NETWORK HAS BEEN ESTABLISHED, IT WILL BE
                                                                                                    POSSIBLE TO RESOLVE NAMES ON WHATEVER NODE!!!
                                                                                                  - SO TO ADD ntp.hpc AS A RESOVABLE NAME!!!
                                                                                                    WE NEED TO ADD IT ON THE master node!!
                                                                                                  - WE JUST NEED TO MODIFY THE FILE named.conf
                                                                                                  - SO WE TRIED TO RESTORE THE named.conf WE 
                                                                                                    MODIFIED ON NODES!!!
                                                                                                    ASK MORENO HOW TO RESTORE!!
                                                                                           - WAIT, WAIT, WAIT:
                                                                                             - THERE IS A SUBTLE HUGE PROBLEM:
                                                                                               THE FILES CREATED BY COBBLER IN THE folder:
                                                                                               ARE NOT CORRECT!!! WE ARE NOT ABLE TO DO A RESTART
                                                                                               OF named AFTER MODIFYING THEM!!!
                                                                                             - SOLUTION:
                                                                                                - WE WERE NOT ABLE TO RESTART FOR 2 PROBLEMS:
                                                                                                    - we must be aware that in the DNS syntax
                                                                                                      we need a NS, which is the authoritative
                                                                                                      Server for our DNS configuration (for it
                                                                                                      we usually define a name, like master.hpc,
                                                                                                      and a Host (A) for which we specify an IP
                                                                                                      address --> WE DIDN'T KNOW;
                                                                                                    - THE FUCKING POINT!!!!!!:
                                                                                                        - THE POINT AT THE END OF THE WORDS
                                                                                                          INDICATES THAT THE PREVIOUS WORD MUST BE
                                                                                                          TAKEN AS IT IS, WITHOUT ADDING 
                                                                                                          THE DOMAIN, WHICH IS NORMALLY ADDED
                                                                                                          BY DEFAULT!!!
                                                                                              - ATTENTION!! WE ADDED A MISTAKE ALSO IN THE COBBLER
                                                                                                FILES : THE PROBLEM IS THAT, IF THE COBBLER 
                                                                                                CONFIGURATION IS WRONG, THE GENERATED CONFIG FILES
                                                                                                ARE NOT CORRECT!!!
                                                                                              - WE MODIFIED THE FILE HPC IN /zone_templates/
                                                                                          - NOW, WE WERE ABLE TO RESTORE THE TEMPLATE FILES
                                                                                            AND TO ADD THE ALIAS, BUT WE HAVE ANOTHER ISSUE:
                                                                                            - WHEN WE RUN THE COMMAND: chronyc sources
                                                                                              WE DON'T SEE ANYTHING!!!
                                                                                            - REASON:
                                                                                                - The file /etc/named.conf has a problem:
                                                                                                  the forwarder has to be set to Marwin, not 
                                                                                                  to the one that has been introduced on Temple
                                                                                                - for Marwin, the right IP address is:
                                                                                                  - 172.16.0.1 , not 172.16.1.1 !!!!!!!!
                                                                                                  - in this way we are telling the system that
                                                                                                    the server sources for the NTP must be taken
                                                                                                    from the Network connected to Marwin!!!
                                                                                                  - SO THE PROBLEM WAS THE DNS!!!
                                                                                                - SO THE PROBLEM IS THE DNS!!!!
                                                                                                -SO WE FIX THE TEMPLATE:/etc/cobbler/named.template
                                                                                                - AND OF COURSE WE RESTARTED COBBLER
                                                                                           - NEW ISSUE: 
                                                                                                - Now when we run chronyc sources from the comp.
                                                                                                  nodes, we see: 
                                                                    MS Name/IP address         Stratum Poll Reach LastRx Last sample               
                                                                    ===============================================================================
                                                                    ^* _gateway                      3   6    17     7  +9704ns[  +49us] +/-   19ms

                                                                                                - we don't see: ^* master.hpc !!!
                                                                                                - WHY??: 
                                                                                                    - THE REVERSED ZONES NEVER WORKED PROPERLY!!!
                                                                                                        - IN PSRTICULAR, WE DIDN'T CREATE THE 
                                                                                                          TEMPLATES FOR THE REVERSED ZONES
                                                                                                          192.168.16 and 192.168.1 :
                                                                                                          WE VERIFIED THIS ERROR BY RUNNING:
                                                                                                          - host 192.168.16 and host 192.168.1
                                                                                                          - WE DIDN'T FIND ANYTHING!!!
                                                                                                        - HOW TO CREATE THE ZONES? AND WHY??
                                                                                                         - IN THE CASE OF THE 192.168.17,
                                                                                                           IT WAS SUFFICIENT TO INSERT $cname_record
                                                                                                           AND $host_record, SO COBBLER IS ABLE
                                                                                                           TO APPEND THE LAST PART OF THE IP
                                                                                                           ADDRESS (IN THIS CASE THE IP ADDRESS)
                                                                                                           IS REVERSED, SO IT WILL RESULT IN THE 
                                                                                                           FIRST PART, NOT THE LAST PART!!)
                                                                                                         - IN THE CASE OF 192.168.16 AND 192.168.1,
                                                                                                           WE NEED TO EXPLICITLY MAKE COBBLER TO 
                                                                                                           APPEND THE "1", WHICH WILL COMPLETE THE
                                                                                                           ip ADDRESS, SO FOR BOTH TEMPLATES,
                                                                                                           WE ADD THE LINE:
                                                                                                           1 IN PTR master.hpc.
                                                                                    - NOW WE CAN CONFIGURE AN NFS IN ORDER TO make the file
                                                                                        system available among the nodes!!
                                                                                            - HOME DIRECTORIES: /data/home AND 
                                                                                            - SOFTWARE: /data/opt
                                                                                        - we install nfs utils
                                                                                        - we enable and start
                                                                                        - we create a folder on the master that we want to share:
                                                                                            mkdir -p /data/opt
                                                                                        - we create a text file in the folder:
                                                                                            echo Hello NFS! > /data/opt/README
                                                                                        -  we want to share folder /data/opt/ with the cluster:
                                                                                            - is is done by the file: /etc/exports
                                                                                            - so we add the line to the file:
                                                                                                /data/opt    192.168.17.1(ro)
                                                                                            - to make changes effective:
                                                                                                exportfs
                                                                                        - Now we need to Set up a NFS client:
                                                                                            - so we need to do installation on the comp. nodes
                                                                                            - we install nfs-utils
                                                                                            - we need to mount NFS share:
                                                                                            - 3 ways to do it:
                                                                                                - with mount
                                                                                                - /etc/fstab
                                                                                                - using autofs
                                                                                            - mount procedure:
                                                                                                - we created the folder /data/opt/ on nodes
                                                                                                - we mounted and verified the mounting
                                                                                                - now we can see that the README file is 
                                                                                                  in the folder on both the nodes
                                                                                                - but we are not able to create or change a file
                                                                                                  in the NFS folder
                                                                                                - REMEMBER!!: in this case the NFS server is the 
                                                                                                  master node!!!
                                                                                                - this is due to the (ro) option specified!!
                                                                                                - so we try to modify access from the server:
                                                                                                    - but we are still not able to do:
                                                                                                    touch /data/opt/TEST
                                                                                                    - so now we create a new file from the server
                                                                                                      in the NFS folder with permissions 777
                                                                                                    - now we are able to access the file from the
                                                                                                      node c01, but if we check the permissions we 
                                                                                                      see:
                                                                                                      nobody as the user, not the root anymore
                                                                                                    - in this case we didn't change the access
                                                                                                      of the clients NFS, so we don't need to 
                                                                                                      do a exports-fs
                                                                                                    - so we see that,despite on the server we
                                                                                                      created the file as root, on the client it
                                                                                                      is registered as nobody user!!
                                                                                                      (ROOT ON A SYSTEM IS NOT ROOT ON ANOTHER
                                                                                                       SYSTEM BY DEFAULT!! --> ROOT SQUASH!!
                                                                                                 - we can do the same also for the other node:
                                                                                                    - we can add the IP address (.17.2) , or
                                                                                                      we can add directly the subnet!!
                                                                                                 - we unmount on both c02 and c01: umount /data/opt
                                                                                            - Mounting with /etc/fstab:
                                                                                                - we modify the file: /etc/fstab (we added a line)
                                                                                                - but doing: mount /etc/fstab, it didn't work 
                                                                                                  on the node
                                                                                                - ISSUE: how we solved?
                                                                                                    - THE ERROR IS THAT THE LINE MUST BE ADDED
                                                                                                      TO THE CLIENT fstab file, NOT TO THE
                                                                                                      SERVER fstab file!!!
                                                                                                    - now mount /data/opt works and also:
                                                                                                        mount | grep data
                                                                                            - Automount:
                                                                                                - mounts file systems on demand
                                                                                                - text files to list mapping of direct. to network
                                                                                                  filesystem location
                                                                                                - autofs mount directories automatically when 
                                                                                                  they are accessed
                                                                                                - we install autofs on nodes
                                                                                                - we did it on the c02 node, to have the
                                                                                                  possibility to make a backup if needed
                                                                                                - we edited the file /etc/auto.master after having
                                                                                                  deleted the folder /dev/opt
                                                                                                - the file /etc/auto.master is the file needed 
                                                                                                  to create mappings
                                                                                                    - we add the line:
                                                                                                    /data  /etc/auto.data  --timeout=1200
                                                                                                - so the mounts in /data are controlled by 
                                                                                                  the auto.data file and they are unmounts after
                                                                                                  1200 seconds of inactivity
                                                                                                - we create the file: /etc/auto.data
                                                                                                - we insert the line:
                                                                                                    - it is to map /data/opt to the NFS directory
                                                                                                      on the server: master.hpc:/data/opt
                                                                                                - we do a systemctl reload autofs
                                                                                                - we try to access the /data/opt
                                                                                        
                                                                                       - Managing and propagating local accounts:
                                                                                          - the goal is to propagate the creation or changes to
                                                                                            user accounts on the master node,to the computing nodes
                                                                                          - so we at first create a testuser local user account
                                                                                            on the master node
                                                                                          - Then we will need to propagate the updates to the
                                                                                            computing nodes:
                                                                                                - we synchronize /etc/passwd, /etc/group, 
                                                                                                  /etc/shadow, and /etc/gshadow from master to
                                                                                                  comp. nodes
                                                                                                - we need to update the autofs config. so that 
                                                                                                  the user direct. is updated
                                                                                          - USER/ACCOUNT CREATION:
                                                                                            - mkdir -p /data/home (we want the home direct. to be
                                                                                              in the /data/home folder
                                                                                            - useradd -b /data/home testuser (we add -b to specify
                                                                                              that the home directory is the different from the 
                                                                                              default)
                                                                                            - also group id (gid) is created (verified through
                                                                                              id testuser
                                                                                            - pinky -l testuser for info on user!
                                                                                            - we can change user infos, with chfn!
                                                                                                 chfn testuser
                                                                                            - pinky is really important because it tells us also
                                                                                              about the home directory of the user account!!
                                                                                            - test: tree /data/home -a, ls -lah /data/home/testuser
                                                                                                    and ls -lah /etc/skel/ should give the same
                                                                                                    result!!!!
                                                                                            - test OK!
                                                                                          - Now we need to propagate on the compute nodes!!
                                                                                            - we create the script file: push.sh to do the
                                                                                              push for a certain node!!
                                                                                            - we run it, but it didn't work!!
                                                                                            - WHY???: because we put c01 instead of c01.hpc!!!
                                                                                            - we verify the presence of the users accounts on the
                                                                                              computing nodes with pinky:
                                                                                              pinky -l testuser
                                                                                            - Test OK!
                                                                                            - Now we need to export the user home directories
                                                                                              through NFS!!
                                                                                              - we modify the folder to export by modifying
                                                                                                the NFS server configuration (the master node
                                                                                                config):
                                                                                                    - we make the exports file to be like:
                                                                                                      /data/opt    192.168.16.0/20(rw)
                                                                                                      /data/home    192.168.16.0/20(rw)
                                                                                              - we need to autofs setup of /data/home on 
                                                                                                computing nodes:
                                                                                              - BUT WE GET A STRANGE ERROR WHEN TRYING TO ACCESS
                                                                                                THE computing nodes!!
                                                                                                - ERROR: kex_exchange_identification: read: Connecti                                                                                                         on reset by peer
                                                                                                    - WHY??: because when we propagated the folders
                                                                                                      and files for the users, the group id for the
                                                                                                      folder/files pushed to computed nodes were
                                                                                                      not specified --> furthermore,
                                                                                                      the sshkeys were not installed ordered on the
                                                                                                      comp. nodes, so this made the group id 
                                                                                                      of the sshKey on the nodes to change on the 
                                                                                                      comp. nodes (994 on the master and 995 on
                                                                                                      the comp. nodes)
                                                                                                    - we changed the group for the files:
                                                                                                      ssh_keys ssh_host_rsa_key 
                                                                                                      ssh_host_ed25519_key 
                                                                                                      ssh_host_ecdsa
                                                                                                    - command: chgrp !! 
                                                                                                    - how we can avoid this? By specifying the
                                                                                                      group id as much high as possible, in order
                                                                                                      to not affect the lower than 100 ones
                                                                                                      (which are the one devoted to system
                                                                                                       entties/data)
                                                                                                      STARTING POINTS:
                                                                                                        - if we connect c01 with sol avctivate for
                                                                                                          instance, we see that the sshkey files 
                                                                                                          belong to 
                                                                                                          different group with respect to the 
                                                                                                          master node! So the groups are created
                                                                                                          specifically when ssh is installed
                                                                                                        - this is a consequence of connecting the 
                                                                                                          files from the master to the comp.
                                                                                                          nodes 
                                                                                                          --> Mismatch between user and group Ids!!
                                                                                                          (WHY THIS HAPPENED ONLY WHEN
                                                                                                          WE PUSH THE USER????)
                                                                                                    - to understand, run this line:
                                                                                                      ls -al /etc/ssh/
                                                                                                - Now we can configure automount to access a user
                                                                                                  home directory from a compute node
                                                                                                    - we added /data/home  /etc/auto.home  --timeout                                                                                                               =1200
                                                                                                      to the /etc/auto.master of the nodes
                                                                                                    - ISSUE: we did a copy error in the file 
                                                                                                             auto.data for the node c02 -->
                                                                                                             the folder testuser ws not 
                                                                                                             visible in the directiry
                                                                                                    - We corrected the isse. Now we have a problem:
                                                                                                        - the directory is not accesible 
                                                                                                          the root user in the nodes c01 and c02
                                                                                                        - HOW TO FIX IT??
                                                                                                            - THE REASON IS THAT WE DON'T HAVE
                                                                                                              ACCESS TO THE FOLDER BECAUSE ITS
                                                                                                              ACCESS IS LINKED TO THE USER
                                                                                                              GROUP??
                                                                                                            - YESS!!: if we access with the 
                                                                                                              user testuser, we see that the
                                                                                                              root directory (home directory)
                                                                                                              is testuser!!!
                                                                                                            - we did: su - testuser !!!
                                                                                                            - ATTENTION!!
                                                                                                                - in the Master node, the root
                                                                                                                  user has preiviledges on the 
                                                                                                                  directory, it's a super user
                                                                                                                - On the other hand, on the 
                                                                                                                  computing node, the root user
                                                                                                                  on the master is treated as a
                                                                                                                  nobody user, so it doesn't have
                                                                                                                  priviledges, this is why we 
                                                                                                                  need to access with the user
                                                                                                                  the folder belongs to!
                                                                                                                - IT IS AN EXAMPLE OF ROOT 
                                                                                                                  SQUASHING!!
                                                                                                     - BUT NOW we have another STRANGE THING 
                                                                                                       TO MAKE BETTER:
                                                                                                         - instead of having /data/home/testuser
                                                                                                           we want: /home/testuser,
                                                                                                         - We can do this in this way:
                                                                                                            - on the compute nodes, we 
                                                                                                              change the /etc/auto.master from
                                                                                                          /data       /etc/auto.data  --timeout=1200
                                                                                                          /data/home  /etc/auto.home  --timeout=1200
                                                                                                              to
                                                                                                          /data  /etc/auto.data  --timeout=1200
                                                                                                          /home  /etc/auto.home  --timeout=1200
                                                                                                         - What we need to do for the master node?
                                                                                                           In theory we could modify the
                                                                                                           configuration of the NFS Server, but
                                                                                                           it is not worth do a similar change
                                                                                                           on master because this change
                                                                                                           will impact only the local directory
                                                                                                         - So we use binf mounts to het the same
                                                                                                           effect:
                                                                                                           - we moddify the file auto.master
                                                                                                             on the master node, we add:
                                                                                                             /home  /etc/auto.home  --timeout=1200
                                                                                                           - we created the file: auto.data on
                                                                                                             the master node
                                                                                                           - we do a systemctl reload autofs (after
                                                                                                             having enabled and started it)
                                                                                                           - Test: we did a cd /home/testuser:
                                                                                               -bash: cd: /home/testuser: No such file or directory
                                                                                                           - SOLUTION: we didn't modify properly
                                                                                                                       the auto.data file
                                                                                                           - NOW WE NEED TO UPDATE THE USER's HOME
                                                                                                             DIRECTORY LOCATION!!!
                                                                                                            - HOW??
                                                                                                                - At first, we evaluated the curr.
                                                                                                                  testuser home directory by
                                                                                                                  running:
                                                                                                                  pinky -l testuser
                                                                                                                  homedirectory:/data/home/testuser
                                                                                                                - then we run:
                                                                                                               usermod -d /home/testuser -m testuser
                                                                                                                - and checked the change with pinky:                                                                                                                  home directory: /home/testuser
                                                                                                                - To propagate the change to
                                                                                                                  the computing nodes, we execute
                                                                                                                  the push.sh script!!!
                                                                                                                - ATTENTION!! :
                                                                                                                    - WHN WE DO CHANGES TO THE USER,
                                                                                                                      we push them by copying
                                                                                                                      the data from master to the
                                                                                                                      nodes;
                                                                                                                    - otherwise, when we modify the
                                                                                                                      home directories for example
                                                                                                                      for the general exporting
                                                                                                                      through NFS, we need to
                                                                                                                      change the /etc/exports file
                                                                                                                    - we do the push for c01 and 
                                                                                                                      c02!
                                                                                                                    - Test: we verify that the
                                                                                                                      home direct. on the nodes
                                                                                                                     for testuser is /home/testuser:                                                                                                                        pinky -l testuser
                                                                                                                    - Test: OK!!
                                                                                                        - Now we want to set up password-less
                                                                                                          SSH connectivity between Computing
                                                                                                          nodes, like we did for the root,
                                                                                                          but this time for specific users
                                                                                                          - This will let the Users to navigate
                                                                                                            among the computing nodes to check
                                                                                                            their computations related to folders
                                                                                                            that belong to them;
                                                                                                          - so we need keypairs that must be 
                                                                                                            connected to a SSH Server, but that
                                                                                                            at the same time readable by the user
                                                                                                          - We could, as we did for Root, 
                                                                                                            generate a password-less SSH Key-pair
                                                                                                            for each User account
                                                                                                                - the problem is that that 
                                                                                                                  procedure was done by the 
                                                                                                                  current user who made access
                                                                                                                  to the system
                                                                                                                - we cannot ask this to the user
                                                                                                                - we should make this happen in
                                                                                                                  another way:
                                                                                                                  1) generate key at account 
                                                                                                                     creation? And if the user
                                                                                                                     erases them?
                                                                                                                  2) add a .bashrc configuration
                                                                                                                     file in the /etc/skel to
                                                                                                                     generate the keys, also
                                                                                                                     in this case, at the user
                                                                                                                     creation --> but the user is
                                                                                                                     still capable to manipulate
                                                                                                                     this configuration;
                                                                                                                  3) we could modify files that are
                                                                                                                     executed at the login of
                                                                                                                     the user and that are managed
                                                                                                                     by the system administrator:
                                                                                                                     /etc/profile and /etc/profile.d                                                                                                                     - These files in profile.d
                                                                                                                       are executed in alphabetic 
                                                                                                                       order, so we need to include
                                                                                                                       a script that must be run
                                                                                                                       as the last one
                                                                                                                     - to do so, we call it:
                                                                                                                       z01_firstlogin.sh
                                                                                                                     - so we generate this file:
                                                                                                                    /etc/profile.d/z01_firstlogin.sh
                                                                                                                     - we tested it by accessing
                                                                                                                       with the user testuser:
                                                                                                                       su - testuser
                                                                                                                       We observed that the .ssh
                                                                                                                       folder was correctly 
                                                                                                                       created





                                                    - EXERCISE 5 :
                                                            - ATTENTION: from the folder /etc/yum.repos.d we erased every reference to almalinux!
                                                              BECAUSE WE DON'T WANT TO CONNECT THROUGH THE LOCAL NETWORK!!!

                                                            - we need Repo's from internet
                                                            - but if we don't have connection?
                                                            - setting our own repositories
                                                            - yum: red-hat based distros
                                                            - they are not connected to internet!
                                                                - only the master has connection because Marwin provides connection!! 
                                                                - it is for security (no possibility to track traffic for every node)
                                                                - all the packages are not official
                                                                - upates: we download packages from management node --> transfer to 
                                                                  compute nodes
                                                                - install. with cobbler provides base repository located in the web server
                                                                    - create foo repository
                                                                    - install package in foo
                                                                    - create local repos on nodes and update them
                                                                    - install a non-base package
                                                                - PAY ATTENTION TO THE REBOOT!!!!! WE NEED TO SET THE Autoinstall Netboot at first
                                                                  to false and then to True!!
                                                                -  EXERCISE:
                                                                    - we create a repos inside /var/www/html
                                                                    -  inside the folder we download htop
                                                                    - we downloaded htop with yumdownloader
                                                                    - we created the repo: createrepo
                                                                        - we can use it every time we can to check that the package
                                                                          didn't break everything
                                                                        - we create a .repo in the node c01:
                                                                            - we delete files almalimux-*.repo:
                                                                            - we ceated it in /etc/yum.repos.d
                                                                        - we checked the list of packages of the foo repo
                                                                            - yum update --> yum repo-pkgs foo list
                                                                            - yum install htop
                                                                            
                                                                        - I Created a folder in the WEB SERVER (master node)
                                                                        - I downloaded htop with yum dowloader (master node)
                                                                        - create a folder on the comput. node
                                                                        - we checked the available packages on the comp. node
                                                                        - we installed htop on the folder in the compute node
                                                                    - Updating base packages:
                                                                        createrepo: creates and populates the repo database
                                                                        - we ceate the folder /root/minimal on the master
                                                                        - we create the file names.rpms with the command:
                                                                        ssh c01 yum list installed | awk 'NR > 1 {print $1}'  > names.rpms
                                                                        - we download the set contained in name.rpms with yumdownloader
                                                                        - we create the repository database by doing:
                                                                          createrepo .
                                                                        - instead of adding the repository to be exposed in the web server
                                                                          (/var/www/html), we make Cobbler do it for us!!
                                                                        - synchronization: cobbler reposync --> repository folder copied into
                                                                          web server: /var/www/cobbler/repo_mirror
                                                                        - if we want to add the repo minimal to Cobbler's database, we do:
                                                                            cobbler repo add --name=minimal --mirror=/root/minimal
                                                                        - expose the repo to the web server:
                                                                            cobbler reposync --only=minimal
                                                                        - we check that /var/www/cobbler/repo_mirror/minimal exists
                                                                          /var/www/cobbler/repo_mirror/minimal
                                                                        - I did a mistake : I did the download in the home directory instead of
                                                                          the minimal one!!!
                                                                          - We fixed deplacing the files in another directory
                                                                        - The action which determines the presence of the repository on the comp.
                                                                          nodes is the creation of metadata for local RPM repositories, enabling                                                                              nodes in an air-gapped or restricted network to install package
                                                                        - so this happens when we do: createrepo .
                                                                        - to make the repository available also on the compute nodes, we need also
                                                                          to expose the repository to the  
                                                                        - ATTENTION!!! THERE ARE 3 COMMANDS THAT ARE FUNDAMENTAL:
                                                                            - The repos on the fucking nodes are stored in /etc/yum.repos.d/ (I can
                                                                              create the file for example foo.repo inside it
                                                                            - Then I verify that the foo repository exists: yum repolist
                                                                            - packages of the foo repository:
                                                                              yum repo-pkgs foo list
                                                                            - once we found the package , we can install it: yum install htop
                                                                         - AND WITH COBBLER??
                                                                            - we don't need to expose the repositories explicitly
                                                                            - we can do it with certain commands!!!
                                                                            - we at first add the repo in Cobbler: 
                                                                              cobbler repo add --name=minimal --mirror=/root/minimal
                                                                            - then we expose to the web:
                                                                              cobbler reposync --only=minimal
                                                                            - ATTENTION!! ON the compute node we need to do same thing we did
                                                                              for the smallest example: we need to create a file: minimal.repo
                                                                              inside the folder: 
                                                                                 /etc/yum.repos.d/
                                                                            - But in this way I need to add the file every time I do an install.
                                                                              of a package for instance, and I have to do it manuallly
                                                                            - we can automatize it by adding the minimal inside a profile:
                                                                               - the configuration files for the profiles are added in deployment
                                                                                 time, so we do: 
                                                                                 cobbler profile edit --name=almalinux8-compute --repo=minimal
                                                                               - WHY WE NEED THAT FILE? BECAUSE IT TELLS THE NODE WHERE IT NEEDS
                                                                                 TO TAKE THE FILE!!!!
                                                                            - so every time that the comp. nodes are rebooted, the file is added
                                                                              to the configuration and thw node is already provided with the most
                                                                              recent packages!!
                                                                            - even if we add the file to the profile, we don't verify it works
                                                                              because we should do a reboot!!!

                                                                            
                                                                            - ADDITIONAL TASKS:
                                                                              FIRST EXERCISE:
                                                                              - commands:
                                                                                - we create a folder exposed to the web server:
                                                                                  mkdir /var/www/html/ganglia
                                                                                - we delete the folder, because it will be exposed by Cobbler!!:
                                                                                  MISTAKE!!
                                                                                  rm -r /var/www/html/ganglia
                                                                                - We create the folder in the root:
                                                                                  mkdir /root/ganglia
                                                                                - We download the package with its dependencies in the folder:
                                                                                  cd /root/ganglia
                                                                                  repotrack ganglia-gmond
                                                                                - We create the repository:
                                                                                  createrepo .
                                                                                - We add to the Cobbler database:
                                                                                  cobbler repo add --name=ganglia --mirror=/root/ganglia
                                                                                - We synchronize the repo:
                                                                                  cobbler reposync --only=ganglia  --> TASK COMPLETE!
                                                                                - We want the repository to be added at development time, so we add
                                                                                  it to the profile almalinux8-compute :
                                                                          cobbler profile edit --name=almalinux8-compute --repo="minimal ganglia"
                                                                                - We synchronize cobbler:
                                                                                  cobbler sync --> TASK COMPLETE
                                                                                - we verify that the repo has been added to the profile:
                                                                                  cobbler profile report --name=almalinux8-compute
                                                                                  --> Repos                          : ['minimal', 'ganglia']
                                                                                - NOW IT'S TIME TO ACCESS TO THE NODES:
                                                                                    Because we don't want to reinstall the nodes:
                                                                                    - ssh c01.hpc
                                                                                    we do the access to the folder with the config files needed for
                                                                                    the yum.configurations of the nodes:
                                                                                    - cd /etc/yum.repos.d/ 
                                                                                    We create a new file.repo:
                                                                                    - vi ganglia.repo
                                                                                    We inserted the text:
                                                                                    [ganglia]
                                                                                    name=ganglia
                                                                                    baseurl=http://192.168.16.1/cobbler/repo_mirror/ganglia
                                                                                    enabled=1
                                                                                    gpgcheck=0
                                                                                    priority=i
                                                                                    - cd
                                                                                    
                                                                                    - yum update --> we have an error!!!
                                                                                      BUT THIS ERROR IS DIFFUSED AND MAKES IMPOSSIBLE TO UPDATE!!
                                                                                    - WE COMMENTED THE PART RELATED TO ganglia IN THE FILE:
                                                                                      cobbler-config.repo
                                                                                    - we also modified the folder yum.repos.d:
                                                                                        - we included the file ganglia described above in 
                                                                                          the file: 
                                                                                          cat ganglia >> cobbler-config.repo
                                                                                        - this was an error!!!!!
                                                                                    - SO THE PROBLEM SEEMS TO BE RELATED SPECIFICALLY TO ganglia!
                                                                                    we check the available packages:
                                                                                    - yum repo-pkgs foo list
                                                                                    - yum install ganglia-gmond
                                                                                    - NOW we repeat the installation on the c02 node:
                                                                                        - exit 
                                                                                        - ssh c02.hpc
                                                                                        - cd /etc/yum.repos.d/cobbler-config.repo
                                                                                        - we added the text:
                                                                                        [ganglia]
                                                                                        name=ganglia
                                                                                        baseurl=http://192.168.16.1/cobbler/repo_mirror/ganglia
                                                                                        enabled=1
                                                                                        gpgcheck=0
                                                                                        priority=1
                                                                                        - we added the file minimal.repo (we forgot to do it before)                                                                                          to the folder /etc/yum.repos.d/ :
                                                                                        - cd /etc/yum.repos.d/
                                                                                        - we put: 
                                                                                        [minimal]
                                                                                        name=minimal
                                                                                        baseurl=http://192.168.16.1/cobbler/repo_mirror/minimal
                                                                                        enabled=1
                                                                                        gpgcheck=0
                                                                                        priority=1
                                                                                        - inside the file minimal.repo
                                                                                        WE come back to the master node:
                                                                                        - exit
                                                                            - ssh c02.hpc yum list installed | awk 'NR > 1 {print $1}'  > names.rpms
                                                                                (we want to update the packages already installed on c02.hpc!!
                                                                            - yumdownloader `cat names.rpms`
                                                                            - we still have to install the 10 packages!!! 
                                                                            - how can we find them??
                                                                                - we need to find the packages already installed in the master 
                                                                                  node --> they belong to the repository baseos
                                                                                - so we need to check what packages are in this repository:
                                                                                - We exit from the node
                                                                                - exit
                                                                                we check the repos in the master node:
                                                                                - yum repolist
                                                                                we check the packages installed for the baseos repository:
                                                                                - yum repo-pkgs baseos list | awk 'NR > 1 {print $1}'
                                                                                In particular we check the packages targeted as "Installed"
                                                                                We copied them in the file baseos_pack.txt
                                                                                - we connected to one of the nodes:
                                                                                ssh c01.hpc
                                                                                we checked the packages installed in the minimal repo:
                                                                                - yum repo-pkgs minimal list | awk 'NR > 1 {print $1}'
                                                                                We copied the installed packages in the file nodes_pack.txt 
                                                                                We did a comparison between the 2 to check what packages are 
                                                                                placed in the first file, but not in the second
                                                                                - we found the packages:
                                                                                  elfutils.x86_64: OK
                                                                                  source-highlight.x86_64 : OK
                                                                                  elfutils-devel.x86_64 : OK
                                                                                  efi-filesystem.noarch : OK
                                                                                  yum-utils.noarch: OK
                                                                                  mtools.x86_64: OK
                                                                                  fwupd.x86_64 : OK
                                                                                  libfabric.x86_64 : OK
                                                                                  strace.x86_64 : OK
                                                                                  libquadmath.x86_64 : OK
                                                                                  libsmbios.x86_64 : 
                                                                                We go on the master node:
                                                                                - exit
                                                                                We go into the minimal directory already created
                                                                                - cd minimal
                                                                                We download the packages one after the other:
                                                                                - youmdownloader shim-ia32.x86_64 ....
                                                                                We sybchronize the repo minimal with cobbler:
                                                                                - cobbler reposync --only=minimal
                                                                                - ssh c01.hpc
                                                                                - yum install shim-ia32.x86_64 and for each package to install
                                                                                - exit
                                                                                - ssh c02.hpc
                                                                                - yum install shim-ia32.x86_64 and for each package to install
                                                                                - we repeat the procedure for each package!!

                                                                                                                                             

                                                                                

                                                                   
                                                    - EXERCISE 6:
                                                        - Software modules:
                                                            - from the 5th we had the possibility to install only the last version of a software
                                                            - package:
                                                                - binaries,
                                                                - libraries
                                                                - headers
                                                                - pages
                                                                - configuration files
                                                            - we need dspecific folders related to the specific files 
                                                            - we use CMake
                                                            - how can we make a software package locally?
                                                            - headers : we need include diectives
                                                                - or export
                                                            - Software modules:
                                                                - make the changes needed to the  environment to use a software
                                                                - multiple versions of softwares can be available
                                                                - we don't need admin permission, we can expose the module files in our user space 
                                                            - LET'S START WITH THE 6TH EXERCISE!!
                                                                - create dedicated install user
                                                                - we donot install with root or sudo priviledges, dedicated install user
                                                                - we create the user amd propagate it to the nodes!!
                                                                - useradd install
                                                                - pinky -l install
                                                                - I did a mistake when creating the user!!!
                                                                - I created it in the wrong way!!
                                                                - We fixed it by recreating the user --> now I have just to propagate on the 
                                                                  nodes!!!
                                                                - now I'm able to do the access with the user install!!
                                                                - we should have created the user install and propagated it to the compute nodes
                                                                - we run : chown -R install:install /data/opt
                                                                    - we change the owner of /data/opt
                                                                - let's install Python3 and Python2:
                                                                    - gcc is already installed (we verified it by doing: gcc --version)
                                                                    - we configure with the prefix:
                                                                        - in this way we can define where to install:
                                                                            Python 2.7.18 (in what repository!!!!)
                                                                        - so the make install will copy all the files in the defined directory!!!
                                                                    - make -j 12 --> is this the compilation?
                                                                    - YES!! AND THERE ARE SOME MISSING DEPENDENCIES!!
                                                                        - what misses?? 
                                                                        - WHAT PACKAGES WE NEED TO INSTALL???
                                                                    - ATTENTION!!!:
                                                                      - WE DIDN'T CREATE THE FOLDER: build-python-3.14.2
                                                                      - mkdir build-python-3.14.2 --> cd build-python-3.14.2
                                                                      - and from here, we start to configure!!!
                                                                      - ./Python-3.14.2/configure --prefix=/data/opt/tools/python-3.14.2
                                                                      - make -j 12:
                                                                        - we get the following message: 
                                                                        The following modules are *disabled* in configure script:
                                                                        _sqlite3

                                                                        The necessary bits to build these optional modules were not found:
                                                                        _ctypes               _ctypes_test          _curses
                                                                        _curses_panel         _dbm                  _gdbm
                                                                        _zstd                 readline
                                                                      - To install the missing headers and libraries, we do: 
                                                                        yum install zlib zlib-devel (zlib contains the libraries and zlib-devel
                                                                        the headers !!!
                                                                      - So we come back to the root:
                                                                        - exit
                                                                      - We download and install with yum all the packages that are missing
                                                                        - In the minimal folder??
                                                                            - NO!! --> IT IS SUFFICIENT TO INSTALL ON THE MASTER, BECAUSE
                                                                              PYTHON HAS TO BE INSTALLED ON BOTH MASTER AND COMPUNTING
                                                                              NODES!!!!!
                                                                            - so on the root user we do:
                                                                              yum search nameofthemodule
                                                                            - in this way we find the name of the headers,libraries and packages
                                                                              to be installed for that module!!!! 
                                                                            - modules: ctypes, ctypes_test, curses, _curses_panel, dbm, gdbm, 
                                                                                       zstd, readline
                                                                              - dependencies:
                                                                                python3-libnacl, zstd
                                                                                gdbm-devel, gdbm-libs, ncurses-c++-libs, 
                                                                                ncurses-compat-libs,
                                                                                ncurses-devel, ncurses-libs, notcurses-devel, notcurses-static,
                                                                                notcurses-utils, ncurses-base, ncurses-term, notcurses, 
                                                                                perl-MLDBM, libzstd, libzstd-devel, perl-Compress-Stream-Zstd,
                                                                                fedora-repo-zdicts, readline-devel, tcl-tclreadline, 
                                                                                tcl-tclreadline-devel, compat-golang-github-chzyer-readline-devel,
                                                                                golang-gopkg-readline-1-devel, lua-readline, 
                                                                                perl-Term-ReadLine-Gnu
                                                                              - WE installed all the dependencies --> we access with the install
                                                                                user --> we re-run the python configuration!!!
                                                                              - but we do make -j 4 : because our machines have 4 cores!!
                                                                                                      - we don't want hyperthreading
                                                                                                      - we don't want the CPU to request
                                                                                                        more accesses to the DRSM for data!!!
                                                                                                      - we want 1 process per core!!
                                                                              - but we still see:
                                                                                  The following modules are *disabled* in configure script:
                                                                                  _sqlite3
                                                                                  The necessary bits to build these optional modules were not found:
                                                                                  _ctypes               _ctypes_test          _zstd
                                                                              - so we run:
                                                                                  - dnf builddep python3 on the root user
                                                                                  - it wasn't useful :
                                                                                  - we installed with yum : libzstd.i686, libzstd-devel.i686
                                                                              - we searched on the web:
                                                                                  - cname should require libffi, but it is already installed!!
                                                                                  - and also: libffi-devel
                                                                         - we rerun configuration and make:
                                                                    The necessary bits to build these optional modules were not found:
                                                                     _zstd
                                                                    - So we concentrate on the disabled sqlite:
                                                                      - we do: yum install libsqlite3x.x86_64 libsqlite3x-devel.x86_64 (master node)
                                                                     - we reconfigure and do make with the install user!!
                                                                     - we get still:
                                                                     The necessary bits to build these optional modules were not found:
                                                                     _zstd
                                                                     - THE MESSAGE DEPENDS ON THE PACKAGE: libzlib, but it seems optional!!!
                                                                     - and we cannot find zstdlib or libzstd --> but they in a way would override
                                                                       pkg.config!!
                                                                   - SO WE RUN make install fron user install in the folder of Python 3.14.2
                                                                   - we see a warning:
                                                                   WARNING: The scripts pip3 and pip3.14 are installed in
                                                                   '/data/opt/tools/python-3.14.2/bin' which is not on PATH.
                                                                   Consider adding this directory to PATH or, if you prefer to suppress
                                                                   this warning, use --no-warn-script-location.
                                                                   - so what we will do, is to add 2 paths:
                                                                     - export PATH=/data/opt/tools/python-3.14.2/bin:$PATH
                                                                 - export LD_LIBRARY_PATH=/data/opt/tools/python-3.14.2/lib:$LD_LIBRARY_PATH
                                                                   - python3 : to check that the path change made python install. useful
                                                                   - we update man to make possible to find man pages for Python:
                                                                     - export MANPATH=/data/opt/tools/python-3.14.2/share/man:$MANPATH
                                                                   - if we need to make headers visible while compiling:
                                                                        - export CPATH=/data/opt/tools/python-3.14.2/include/python3.13:$CPATH
                                                                        
                                                                - PYTHON 2:
                                                                    - same done for python3
                                                                      Python build finished, but the necessary bits to build these modules were no                                                                        found:
                                                                         _bsddb             bsddb185           dl
                                                                         imageop            nis                sunaudiodev
                                                                         - we do yum search on the modules to find the headers and libs to be
                                                                           installed
                                                                         - libdb-dev, dlopen, libnsl-dev
                                                                         - we reconfigured and we did make -j 4
                                                                         - same error!!!
                                                                         - we do : yum builddep python2   
                                                                      - we go ahead anyway with the make install
                                                                      - how to check that the make install will be successfull:
                                                                        after having run the make -j , do: ./python in:
                                                                        /home/install/build-python-2.7.18
                                                                        and if you don't see errors, you can go ahead!!!
                                                                      - so we do make install
                                                                      - we add the paths, and we check by running python2
                                                              
                                                              - ENVIRONMENT MODULES:
                                                                - environ. module are important to check what environ. variables need to change
                                                                  for a softw. package to work and the paths that need to be added
                                                                - to implement them: LMOD
                                                                - we install lmod
                                                                - we configure it
                                                                    - to do it, we need packages
                                                                    - to install packages, we need to install 
                                                                    - we enable the AlmaLinuxs PowerTools repository:
                                                                        dnf config-manager --set-enabled powertools
                                                                    - then we install the needed packages on both master and compute nodes
                                                                  [root@master ~]# yum install tcl tcl-devel lua lua-devel lua-posix lua-filesystem
                                                                    - we build with configure choosing a folder
                                                                    - we do a pre-make: 
                                                                        - we install everything but the symbolic links
                                                                        - by default they have root priviledges, we don't want this!!!
                                                                        - the symbolic links are virtual files (maybe)
                                                                        - we create the sym. link manually
                                                                        - we create them as root: (we exit the install user)
                                                                            - the lmod symbolic link is created because, when we need to update
                                                                              Lmod to a new version, we will need to update the symbolic links
                                                                              also on all the compute nodes. 
                                                                              SO WE ADD THIS SIMB. LINK !!
                                                                            - the first argument is the file/dir we want to link to
                                                                            - sec. argument is file/dir. is the link name that will point to the 
                                                                              target
                                                                            - we define the second and third simb. links so that they are saved
                                                                              at profile level
                                                                            - SO WE CREATE THE FOLDER: /data/opt/apps/lmod/
                                                                            - we go inside and we create the sim. links
                                                                            - WE DO THE SAME FOR THE COMP. NODES!!!
                                                                            - it seems that the folders and the links have been already created
                                                                              on the nodes!!
                                                                            - we are able to type: module avail!!!
                                                                            - AND ON THE COMPUTE NODES??
                                                                                - AM I ABLE TO RUN module avail???
                                                                                    NOOOO!!
                                                                                    - WE DIDN'T CREATE THE SYMBOLIC LINKS!!!
                                                                                    - SO WE CREATE THEM ON BOTH c01 and c02!!!
                                                                                    - BECAUSE I NEED TO INSTALL LUA ALSO ON THE NODES!!!!
                                                                                    - SO WE INSTALL LUA ALSO ON THE NODES!!!!
                                                                                    - BUT WAIT!!! What we need to do is to install the dependencies
                                                                                      from the packages included in the Install prerequisites!!
                                                                                    - We do a yumdownloader from the minimal directory
                                                                                      --> we download the rpms!!
                                                                                    - Now we do the yum install from the nodes!!!!
                                                                                    - BUT WE ARE NOT ABLE TO FIND THE PACKAGES ON MINIMAL 
                                                                                      FOLDER ON THE NODES!!!! WHY??
                                                                                      - BECAUSE, FUCKING SHIT, YOU NEED TO RUN A COMMAND, MANAGED
                                                                                        BY COBBLER, TO SYNCHRONIZE THE REPO WITH THE NODES!!:
                                                                                        - cobbler reposync!!!!!!
                                                                                    - WE INSTALLED, BUT WE HAVE STILL ISSUES!!
                                                                                    - module avail is not found!!!!
                                                                                    - SO WE CHECKED AN NOTICED THAT:
                                                                                        - THE LINKS WERE NOT CORRECTLY SET ON THE NODES!!!!
                                                                                        - WE NEED TO RUN module avail ON THE INSTALL USER!!!

                                                                         - we create a folder hierarchy:
                                                                            - we  create a folder modulefiles/
                                                                              for the modules, and we place it inside
                                                                              the /data/opt/tools --> remember that the folder /data/opt is shared
                                                                              among master and comp. nodes!!!
                                                                            - I create the folder as install user!
                                                                            - NONO!! we create a modulefiles folder for each folder in the 
                                                                              opt/ directory!!!
                                                                          - we run: mkdir -p /data/opt/{apps,base,libs,tools}/modulefiles
                                                                            - I get: mkdir: cannot create directory modulefiles: Permission denied
                                                                            - The error is due to the fact that the folder "apps" belonged to 
                                                                              the root user, not the install one
                                                                            - so it was necessary to run: 
                                                                                - chown install /data/opt/apps/ -Rh from the root user!!
                                                                            - now we can create the folder
                                                                            - we add the Path!!
                                                                                - but the problem is that we need that the change is permanent
                                                                                - so we create a script inside the folder 
                                                                                    /etc/profile.d/modules.sh
                                                                                - so when the system is reinstalled, it will be available again!!
                                                                                - nooo!! We WANT THAT EVERY TIME WE LOGOUT AND IN AGAIN WITH THE 
                                                                                  USER, THE PATH
                                                                                  HAS ALREADY BEEN CREATED!!
                                                                                    - SO WE SIMPLY MODIFY IT!!! we add the line!!
                                                                                - it was not possible to modify the file, so we 
                                                                                  modify the owner from the root user by running:
                                                                                  chown install /etc/profile.d/modules.sh -Rh
                                                                                - then we logged in again with the install user
                                                                                - It is still not possible to save the changes
                                                                                    - SOLUTION: we create another file called:
                                                                                      module.sh (without the s)
                                                                                    - NOT POSSIBLE TO DO IT!!! --> WE MODIFY THE FILE modules.sh
                                                                                      AS ROOT USER INSTEAD!!!
                                                                                    - IT IS PROBABLY DUE TO THE FACT THAT THE USER installl
                                                                                      CANNOT ACT ON THE PROFILES, THAT ARE MANAGED BY A 
                                                                                      SUPER USER!!!
                                                                                    - we created the same file on the nodes!!!!
                                                                                    - we verified at the login by running:
                                                                                      echo $MODULEPATH
                                                                                - ATTENTION!!! WE FORGOT TO DO: make pre-install!!!!
                                                                            - MODULE FILES:
                                                                                - SOO: 
                                                                                    - LMOD searches in the directories if there are .lua scripts
                                                                                    - AND IT INTERPRETS THEM AS MODULES!!
                                                                                    - ATTENTION!!
                                                                                        - for the directory system that we included, a module
                                                                                          will be intended as .lua file and it will be 
                                                                                          visible with the command module avail only if 
                                                                                          it will be included in this way:
                                                                                          /data/opt/tools/modulefiles/python/3.14.2.lua
                                                                                        - THE CHANGES WILL BE EFFECTIVE ONLY AFTER EXITING
                                                                                          AND ACCESSING AGAIN!!!!! (VERIFY THAT THIS WORKS 
                                                                                          AS EXPECTED!!!)
                                                                                     - so we create the folder for python
                                                                                     - after exiting and re-accessing to install, 
                                                                                       the python module is visible with module avail
                                                                                     - now we do: module load
                                                                                     - then module list
                                                                                    - NOW we create a new modul file for Python 2.7.18:
                                                                                        - touch /data/opt/tools/modulefiles/python/2.7.18.lua
                                                                                        - LD is capable of checking what is the last version
                                                                                        - but could choose to have another behaviour:
                                                                                            - if we want that our default is Python 2,
                                                                                              we can introduce a symbolic link:
                                                                                       ln -s /data/opt/tools/modulefiles/python/2.7.18.lua 
                                                                                       /data/opt/tools/modulefiles/python/default
                                                                                            - NOW WE NEED TO MAKE PYTHON USABLE
                                                                                            - IN WHAT SENSE?
                                                                                                - IN THE SENSE THAT, WHEN PYTHON MODULES ARE 
                                                                                                  LOADED, THE EXPORT OF THE binary and of the 
                                                                                                  library files must be available to all
                                                                                                  subshells or child processes!!
                                                                                                - THIS IS NOT REQUIRED ON LOCAL MACHINES
                                                                                                  BECAUSE ITS INSTALLATION IS DONE
                                                                                                  IN A WAY THAT ALREADY INCLUDES THE PATH
                                                                                                  CHANGE OF THE ENVIRONMENT VARIABLES!!
                                                                                                - WE NEED TO CHANGE THE MODULE FILES
                                                                                                  TO DO THESE EXPORTS AT THE LOADING
                                                                                                  OF THE MODULE!!
                                                                                                - OTHERWISE WE CAN USE PYTHON ONLY FROM 
                                                                                                  THE MODULE SUBFOLDER???
                                                                                                - after modifying the module, we 
                                                                                                  were able to see the rghr screen after
                                                                                                  running python3!!!
                                                                                              - we do the same for Pyhon2!!
                                                                                  - We need to install the Install Build Enviornent:
                                                                                    - we install it on the root!!!
                                                                                  - GCC compiling:
                                                                                    - module:
                                                                                        - we create the folder:
                                                                                        mkdir -p /data/opt/tools/modulefiles/gcc
                                                                                        - we create the module file for GCC
                                                                                        - we re run the build
                                                                                        - we do make install
                                                                                  - CMAKE:
                                                                                        - we will configure it after gcc will have compiled 
                                                                                        - we do make -j 4 and then make install !!!  


                                                       EXERCISE 7: CONFIGURATION WITH ANSIBLE:
                                                            - automation
                                                            - do all configs in different way
                                                            - ad-hoc task execution tool: let us send any command to the cluster nodes
                                                            - uses open ssh
                                                            - uses python
                                                            - ansible master is th emaster node
                                                            - invetory and playbook 
                                                            - proper ansible -m service -o <smr
                                                            - autoatic checks
                                                            -plsybook: contains plays
                                                                - 
                                                            
                                                            EXERCISE 7 :
                                                                - install Ansible from: yum -y install ansible.noarch
                                                                - ansible tries to connect with a host
                                                                - initially ansible is able to see only the localhost, in fact, if we run:
                                                                  ansible localhost -m ping
                                                                - it will succes, but for any other name it will fail
                                                                - so we need to ceate an inventory to include groups or other hosts
                                                                - default host is in: /etc/ansible/hosts
                                                                - buy we can create host wherever we want by specifying the location path!!
                                                                - we create the file: inventory in /root/ (master node):
                                                                    - we populate it with:
                                                                        [master]
                                                                        localhost    ansible_connection=local
                                                                    - we run: ansible master -m ping -i inventory --> SUCCESS !!!
                                                                - we add: 
                                                                [compute]
                                                                c01
                                                                c02
                                                                - we created an example of playbook:
                                                                    - master.yml created in the home!!!! 
                                                                - variables can be stored in vars folder inside a role!!!
                                                                - facts: variables got as a result of the interaction with the host, not set by
                                                                         the user!!!
                                                                - we add: - debug: var=ansible_facts to the playbook master.yml
                                                                    - we run the playbook to test the change
                                                                    - in this way we can see what facts are available (they are like member
                                                                      variables of built-in classes)
                                                                - Loops:
                                                                   in the with_items I can show the "variables" to apply the tasks to all 
                                                                   of them!!
                                                                - Modules:
                                                                    - to check them: ansible-doc -l
                                                                    - service is a module!!
                                                                    - they have inside sort of function usable in tasks!!
                                                                - Conditionals:
                                                                    - THE PROPER CHARACTERISTIC OF A STATEMENT BLOCK IN ANSIBLE CAN BE 
                                                                      SEEN IN THE LAST PART OF THE TASKS!!!!
                                                                    - the when statement must be followed by a boolean !!!!
                                                                 NTP server installation:
                                                                      -
                                                                     - NTP client installation:
                                                                           -
                                                                     - ROLES:
                                                                           - we need the folder roles in the same path of the
                                                                             playbook
                                                                           - introd. to provide vars files, tasks, handlers,
                                                                             based on a certain file structure
                                                                     - CONNECT COBBLER WITH ANSIBLE:
                                                                           - we configure the compute management class
                                                                           - ATTENTION!!! WE DO THIS ONLY ON THE MASTER NODE!!!
                                                                           - we add the class to the system with 
                                                                             cobbler system edit --name=c01 --mgmt-classes=compute
                                                                           - we want to map the cobbler managm. classes to the Ansible groups
                                                                             for example 
                                                                           - after the creation on managm. classes, we see the correct result 
                                                                             for
                                                                         cobbler system report | awk '/Management Classes|Name/ && !/DNS|Servers/'
                                                                           - We want to introduce a mapping!!
                                                                           - for this, we need a file cobbler.py!!
                                                                           - we run Cobbler commands only on master node!!
                                                                           - we want to make the inventory changes to be shared authomaticly
                                                                             to all the system??
                                                                           - WE WANT TO MAP ALL THE CLSSES RELATED TO THE DNS WITH AN INVENTORY
                                                                             ANSIBLE
                                                                           - IN THIS WAY THE INVENTORY WILL BE UPDATED FOR ANY CHANGE OF THE 
                                                                             CLUSTER!
                                                                           - INITIALLY the command:
                                                                                - ansible-playbook -i inventory ansiblemaster.yml
                                                                           - DIDN'T WORK: WE GOT THE ERROR:
                                                                                - ansible-playbook -i inventory ansiblemaster.yml
[WARNING]: Unable to parse /root/dynamic_inventory/inventory as an inventory source
[WARNING]: No inventory was parsed, only implicit localhost is available
[WARNING]: provided hosts list is empty, only localhost is available. Note that the implicit localhost does not match 'all'
                                                                        [WARNING]: Could not match supplied host pattern, ignoring: master
                                                                            - BECAUSE THE INVENTORY WAS NOT INCLUDED IN THE FOLDER 
                                                                              dyna  mic_inventory
                                                                            - WE APPLY THE MAPPING WE COULD CHANGE THE INVENTORY HOST FILE,
                                                                              BUT WE DO IT A SCRIPT!!
                                                                            - WE CAN NOTE THAT FOR THE SECOND RUN THE THE REMOVE HOSTS FILE IS
                                                                              SKIPPED BECAUSE HAS ALREADY BEEN DELETED WITH THE FIRST RUN!!
                                                                          - AD HOC COMMANDS:
                                                                            - THEY ARE USED TO EXECUTE A SINGLE COMMAND ON THE COMPUTE NODES
                                                                            - THEY ARE SINGLE TASKS!
                                                                            - flag -m : stands for module (the default is the module command)
                                                                            - flag -a : to pass arguments!!
                                                                            - example: ansible compute  -a 'hostname':
                                                                                - WE GOT THE ERROR: NO HOST FOUND --> WHY?
                                                                                    - BECAUSE THE hosts FILE IN THE FOLDER OF THE DYNAMIC
                                                                                      INVENTORY, DIDN'T CONTAIN THE REFERENCES TO THE compute 
                                                                                      GROUP!!!!  
                                                                                - ansible master  -m service -a "name=httpd state=started 
                                                                                  enabled=yes" : OK!!
                                                                                - ansible compute -o -m yum -a "name=smartmontools state=latest"
                                                                                  : OK!!
                                                                        
                                                                     - TASK 1:
                                                                        - We created the file: master_1_task.yml 
                                                                        - we run the command to execute the playbook:
                                                                          ansible-playbook -i inventory  master.yml
                                                                        - we need to execute with a certain sequence all the tasks!!!

                                                                     - TASK 2: we created the file doubts with the questions
                                                                               - I don't need to reboot the machines!!
                                                                               - just run ansible!!
                                                                               - IT SEEMS TO WORK!!!!
                                                                     - TASK 3 and 4 : we created the roles on the master node
                                                                         Add a role to the compute nodes playbook called accounts_propagation:
                                                                         DO I NEED TO CREATE THE RULE BY ACCESSING TO THE COMPUTE NODES?
                                                                         OR I CAN DO IT BY DEFINING DIFFERENT HOSTS???
                                                                         - Yes: I define different hosts
                                                                         - I have to synchronize from master to nodes!!! 
                                                                         - So I am on the compute nodes -> I want to synchronize them with the 
                                                                           master node!!
                                                                         - we created user: 
                                                                            - we checked id by running: id new_userhp                                                                                                           - we created user: 
                                                                                - we checked id by running: id new_userhpc
                                                                         - For the second part of the 3rd task, we choose as user the 
                                                                           install user!!!!
                                                                         
                                                                         - PAY ATTENTION TO THE INDENTATION AND TO THE SPACES!!:
                                                                            - BLOCK DOESN'T WANT A VALUE AND NEITHER A SPACE BEFORE THE : COLUMN!!
                                                                         
                                                                         - UNTAR with ansible: if we don't specify that the source is the remote
                                                                           host , we assumes to search the file on the current triggering playbook

                                                                         - SYMBOLIC LINKS IN ANSIBLE: 
                                                                            - the src is the link towards I want to point to
                                                                            - the dest is the starting path
                                                                            - if the file to point to is in the same directory of the dest,
                                                                              it it sufficient to write its name!!
                                                                         
                                                                         - MAYBE, NOW IT WORKS!!!
                                                                        
                                                                     - TASK 3:
                                                                       - to do the push to the compute nodes, it is necessary at first to 




                                                       EXERCISE 8:
                                                            - no report?????
                                                            - job scheduler:
                                                                - make decisions based on parallel
                                                                - complex
                                                                - impl. poliies to ex. jobs 
                                                                - instruct resource manag. to allocate jobs
                                                                - communication between Scheduler and Resource Manager
                                                                - wall time: maximum time you can spend (doesn't mean that each jobs need 24 hours)
                                                                - checkpoints:   
                                                            - for hpc- ai the most important thing is: storage and bandwidth
                                                            - job priority: it is calculated in several ways!!! 
                                                            - job accumulates queue time 
                                                                - expansion factor
                                                            - how to know if we have the correct configuration??
                                                            - FIFO scheduling:
                                                                - we want scheduling to avoid idle resources!!(waste cycles and time)
                                                                - we can avoid it by doing backfill
                                                            - we need to install and configure the Slurm!!
                                                                - create queues and submit jobs
                                                                - configure slurm for MPI
                                                            
                                                            - EXERCISE:
                                                                - User acconts creation:
                                                                    - we create another role for the current exercise
                                                                    - we create another playbook called slurm_play.yml
                                                                    - for the first part (User creation) we create a role (user_account)
                                                                      in the folder roles
                                                                    - in the playbook, we call at first the user_account, ad then 
                                                                      the account_propagation to copy the users on the compute nodes
                                                                    - the Munge key is generated by running a command on the master node
                                                                        dd if=/dev/urandom bs=1 count=1024 > munge.key
                                                                        (is generated in the root/ folder for the root user)
                                                                        (we copy the munge key in the files of the Munge authentincation
                                                                         role)
                                                                        - the munge auth. task will be run only on the master node
                                                                        - on the compute nodes, we propagate just the munge.key!
                                                                    - after running the role for the Munge key, we need to create a new 
                                                                      one to propagate the munge key on the compute nodes
                                                                    - so we created the role: copy_mungekey (must run on the host!!)
                                                                    - WE CREATE A NEW REPO FOR SLURM:
                                                                        - we call it slurm
                                                                        - from the master node:
                                                                            - mkdir slurm
                                                                            - we yumdownload the rpms and we do createrep .  and 
                                                                              we add the new repo with cobbler
                                                                            - we do it manually
                                                                            - we do a reposync
                                                                            - we add the repo to the profile almalinux-compute
                                                                            - we do a cobbler sync
                                                                            - we should add a piece of code to the file 
                                                                        - ATTENTION!!: the files in the directory etc/yum.repos.d 
                                                                          must be present only for the compute nodes!! Not for the 
                                                                          master node!!!
                                                                            - I modified by hand the file cobbler-config.repo in the
                                                                              folder for c01 and c02
                                                                    - WE ADD A role for the installation of the Packages required for Slurm
                                                                    - WE NEEDE tO TO A REPOTRACK ON THE SLURM REPO TO MAKE
                                                                      THE TASKS RELATED TO THE INSTALLATION WORK!!!
                                                                    - we generated the slurm.conf
                                                                    - we run the task of the System wide for both compute and master!!: OK!!1
                                                                    - MASTER CONFIGURATION:
                                                                        - OK
                                                                    - CLIENT CONFIGURATION:
                                                                        - it seems it doesn't see the packages for slurm:
                                                                            - it has been necessary to do a makecache and before a cobbler
                                                                              reposync --> now they are visible from the comp. nodes?
                                                                            - we also changed the first task related to the packages
                                                                              adding update_cache: true
                                                                    - ATTENTION!!!:
                                                                        - UNFORTUNATELY sinfo on master ad nodes had issues:
                                                                        - BUT PROBABLY 1 issue can be related to the hostname of c01 and c02
                                                                        - c02 has hostname: c02.hpc
                                                                        - c01 has c01 WHY???
                                                                 - SO WE CHANGED THE c01 hostname with the command: hostnamectl set-hostname c01.hpc
                                                                  
                                                                 - ANOTHER IMPORTANT THING:
                                                                    - WE ARE STARTING TO RUN OUR PLAYBOOKS with the command:
                                                                        - ansible-playbook -i inventory slurmplay.yml
                                                                        - IN THIS WAY, we have more control on what inventory we are running on
                                                                        - otherwise, if we have a dynamic inventory, it is the hosts defined
                                                                          by it that is run!!
                                                                 
                                                                 - File for system-wide configuration (changes):
                                                                    - SlurmCtIdHost = master (it is the hostname of the master node --> find it
                                                                      by running "hostname" on the master node)
                                                                    - Memory, CPUs, Sockets, Cores, Threads  EXPRESSED EXPLICITLY
                                                                      - BECAUSE otherwise it shows an error linked to the architecture!!
                                                                    - We set explicitly the IP addresses of the compute nodes!!!
                                                                        
                                                                 - LOGS for SLURM: tail -n 50 /var/log/slurmctld.log
                                                                    - error: _slurm_rpc_node_registration node=c02: Invalid argument
                                                                    - but if we do: scontrol ping (REALLY IMPORTANT), we get: 
                                                                        - IS UP: this means that the master and the nodes are 
                                                                          sending and receiving
                                                                        - so WHY the nodes are down?
                                                                            - MAYBE THE FIREWALL!!: IT IS ACTIVATED ON THE COMPUTE NODES!!!
                                                                            - NOW WE DEACTIVATE IT IN THE FINAL ROLE OF THE PLAYBOOK!!!
                                                                   - REMEMBER!!!!!
                                                                        - YOU NEED TO FUCKING RESTART NAMED, SLURMCTLD , SLURM IN ALL THE UNITS!!
                                                                          BOTH MASTER AND COMPUTE NODES!!!!!!
                                                                        - OTHERWISE THEY REST IN STATE: DOWN!!!!!
                                                                   - TO SEE THE LOGS:
                                                                        - tail -n 50 /var/log/slurmctld.log

                                                                   - Now both the compute nodes are in status: drained!!!
                                                                        - we modified the host name of both the compute nodes: c01 and c02
                                                                   
                                                                   - AFTER A LOT OF CHANGES, WE RESET THE HOSTNAME FOR THE COMPUTE NODES AS 
                                                                     c01 and c02
                                                                        - c01 and c02 cannot resolve the "master", but only "master.hpc"
                                                                        - so they are not able to connect
                                                                        - the strange thing is that if I change the host names to c01.hpc and 
                                                                          c02.hpc, they resolve also "master"
                                                                        - WE ADDED MANUALLY THE IP address of the master with the name "master"
                                                                          for both the nodes!!!!

                                                                   - THE MASTER CONTINUES TO SEE THE NODES IN DRAINED STATE, BUT SINCE 
                                                                     THE SYSTEMCTL SEEMS ACTIVE AND STABLE, WE RUN.
                                                                     scontrol update nodename=c[01-02] state=resume
                                                                     - NOW THEY SEEM IDLE
                                                                     - IF THEY KEEP BEING IDLE, OK, IF THEY IMMEDIATELY TURN TO DRAINED,
                                                                       THERE IS A PROBLEM!!
                                                                     - for us, it happens for the reason retrievable in this way:
                                                                       scontrol show node c01 | grep -i reason
                                                                     - THE REASON IS: LOW REAL MEMORY
                                                                        - so in the slurm.conf file, we set a memory lower than the real (less 
                                                                          than 3387!!
                                                                        - we reduced the memory to 3000
                                                                  - TASKS:
                                                                    - prologue and epilogue are intructions executed before the run of the job
                                                                     and after it has finished
                                                                    - we need a cgroup: a ASCII file which defines parameters used by cgroup
                                                                      plugins --> WE NeeD TO PUT IT IN THE SAME FOLDER OF THE FILE:
                                                                      slurm.conf
                                                                    - we create the file cgroup.conf in the folder: /etc/slurm/cgroup.conf?
                                                                    - WE CREATE IT WITH ANSIBLE!!
                                                                    - WE CREATE IT ON EVERY NODE!!!
                                                                    - then we need to restart slurmctld (master) and slurmd (nodes)
                                                                    - cgroup defines the boundaries for the job, its limits!!
                                                                    - WE ADD THE cgroup in the role: sys_wide_config
                                                                    - this is the configuration for the system wide configuration role:
                                                                     files
                                                                          cgroup.conf
                                                                          slurm.conf
                                                                          task_epilogue
                                                                          task_prologue
                                                                         tasks
                                                                         main.yml
                                                                    - Now let's create an example job
                                                                    - we did a test with the user: testuser
                                                                    - job: test_job.sh
                                                                    - result: OK!! Prologue:OK! Epilogue:OK!



                                                        
                                                       EXERCISE 9:
                                                            - Infiniband
                                                                - one type that can be put in a cluster
                                                                - in couples, on temple cluster
                                                                - 2 compute nodes connected
                                                                In the instructions:
                                                                    - master: c02
                                                                    - c01 : is c03
                                                                - RDMA: bypasses OS and CPU to copy directly into remote memory
                                                                    - this is way has a low latency!
                                                                - Layer 2 : Data-link layer
                                                                - low cpu utilization
                                                                - how to manage the network?
                                                                - can be applied to many topoogies
                                                                - Throughput and latency
                                                                - what workflow for infiniband??
                                                                - we need HCA adopter
                                                                    - it connects not only a cable, but also a chip inside
                                                                    - each card has a sort of MAC address
                                                                - very expensive
                                                                - DAC : copper cables --> but wehave problems with longer cables
                                                                - DOC : optical cable --> more expensive and delicate
                                                                - we don't need to install IP like in non infiiband (because we don't pass through 
                                                                  the cpu and OS
                                                                - opt. tranceivers ..> more than 10 km
                                                                - subnet manager:
                                                                    - fabric
                                                                    - config. endpoints nd 
                                                                - Our switches are managed 
                                                                - SCale : 
                                                                - Blocking ratio: 1:1 --> means that everything has a link!!!!
                                                                - we need to do a report!!!!
                                                                - 






EXERCISE 9:

INSTALLATION OF INFINIBAND SUPPORT:
- we reboot nodes 2 and 3, wedon't need to reboot masyer node!!
- the nodes 2 and 3 are the only one with infiniband connection
- ssh c02 "yum install -y @infiniband" (we install on compute
  nodes)
- so we do also: 
  ssh c03 "yum install -y @infiniband"
- we do it without Ansible
- Verify that netbooting is False in cobbler to avoid any reinstallations. (Netbooting must be set to false)
- cobbler system report | grep Netboot
- ssh c02 "shutdown -r now"
- ssh c03 "shutdown -r now"
- we do a sol activate to check that it is rebooting:
    - it seems to reboot
- ibstat command from the root master 
  (checks the state of the adopter):
    -  Node GUID: 0x0002c903000b86f8 : it is the MAC address of the 
       infiniband????
    - we have a diff. between GUID of the port and the node
    - Linkup: it means it is iitializing connection (connected just via      cable, but not with our network;

- SUBNET MANAGER
    - the GUID is the equivalent of the MAC address for Ethernet
      connection
    - we set it as a service on the node c02 (not as a managed IB network swithes;
    - we ignore the warning to not install the subnet manager
    - we install it only on the c02 (we insert a ssh c02 before and "
      signs!!!):
        yum install opensm
        systemctl enable opensm
        systemctl start opensm
        systemctl status opensm
    - Active: active
    - We check the status again: ibstat
        - State: Active
        OK!

- TESTING IB NETWORK: 
    - Discovering all devices on IB network
        - there are several commands built-in by the @infiniband group
            - ibnetdiscover: it shows the devices on IB fabric
            - ibdiagnet : to diagnostics utility
                - it doesn't work 
                - we run from c02: yum whatprovides */ibdiagnet
                    - it doesn't work 
                    - it doesn't work neither where running on the
                      master node, that should have all the packages
                      available
                - WE WILL ASK MORENO: the package is ibutils

            - iblinkinfo: it tells us the nature of the connection,
              it doesn't work only in case there is the switch!!
            - ibqueryerrors: to check errors in the connection 
                - we get threshold error for one of the ports (c02)!!
                - WE ACCEPT IT....WE CHECK THE NEXT STEP
        - we check the IB PING PONGcnnectoin between nodes:
            - we create 2 terminal: one for c02, one for c03:
            - on the master(c02) we launch the server:
              ibv_rc_pingpong -d mlx4_0 -g 0 -i 1
            - on the client (c03) :
              ibv_rc_pingpong -g 0 -d mlx4_0 -i 1 192.168.16.1
                - it doesn't work: the IP address should be 
                  192.168.17.2 ??
                - NOOO!!! The firewalld is active again!!!
                - we disable the firewall on c02, c03
                  systemctl stop firwalld
                - it was not working because of the firewall!!
                - after the fix: OK!!
              running:ibv_rc_pingpong -g 0 -d mlx4_0 -i 1 192.168.17.2
        - We test the bandwidth using ib_write_bw. It will give 
          the bandwidth we can achieve with IB links
          - ib_write_bw -F --report_gbits : active the client (run from
            master.hpc
          - from the c02 and c03: 
            ib_write_bw -F --report_gbits -a master
            - NOO! it is an error: for us the master is c02, so we
              run:
            ib_write_bw -F --report_gbits -a c02
            - the pick for Bancdwidth is around 27.17 Gb/sec
         - Now we test the latency:
          - we run the server accessing to c02 (because the c02 is the 
              master!!)
          - we run the ib_write_lat -F --report_gbits -a c02 from the
            c03 (which is the client)
          - the loss of bandwidth start at the same point of the 
            loss of perform. of the bancdwidth, because the size 
            of the data becomes too big to be loaded in a single
            call (clock cycle??)
         - OpenMPI:
            - MPI library must be compiled and it is part of a stack,
              so the other softwares compiled with mpi should be linked              to a version
            - Compilation:
               - We install OPenMPI and compile with Slurm and 
                  Infiniband
               - we check that the headers are installed on the master
                 (in this case is the master, not c02!!)
               - yum install slurm-devel (from the master)
               - we access with the install user:
                - we choose the version: 5.6.8
                - we do a scontrol to put the nodes again in idle 
                  state
                - We download the OpenMPI from github
                - we run the commands for the configuration
                - before we do rm -rf openmpi...(yo have a new install)
                - with-pic: is independent of the position
                  POSTION INDEPENDENT CODE
                - we have base instead of tools for configuration
                - Resource Managers: ssh/rsh must be equal to: yes
                    - BUT WE DON'T SEE THEM!!!!
                    - we installed the 5th version
                    - fot this version the with-verb option (for 
                      infiniband) is not implemented, this makes
                      not useful the installation for our case
                    - So we re-installed openmpi with version 4.1.6
                    - we generate the configure file again
                    - we create the environment variables
                    - we run the configure on a chosen folder
                     (base/openmpi/)
                    - error: we miss 2 libraries!!!
                    - we install libraries on root user onmaster node
                      - rdma-core-devel
                      - libibverbs-devel
                    - we come back to install user and we retry the 
                      configure
                    - CHECK: OK!!
                    - we compile and install with make:
                        - make -j 12 (on Temple we can do 12)
                        - make install
                  - MODULE FILES:
                   - mkdir -p /data/opt/base/modulefiles/mpi/openmpi 
                   - /data/opt/base/modulefiles/mpi/openmpi/4.1.6.lua
                     (we created the module)
                   - INSTALL AS DEFAULT SOFTWARE MODULE:
                    - we want the Module loaded by default at the 
                      login --> so we need to do scripts
                      to load mpi/openmp
                    - we create 3 script:
                        - one for the load command and 2 for the
                          shells (z01_StdEnv.sh and z01_StdEnv.csh)
                    - we call it z01 and z02 so it will be loaded afte 
                      other loads of the modules in general 
                    - we do it from the root user to extend the
                      change to every user and node!!!
                    - by accessing again with install user, we should
                      be able to see the list of modules by
                      running:
                      module list
                    - the modules must be available also on the 
                      compute nodes, so we check that the 
                      /etc/profile.d/z01_StdEnv.sh is also on the 
                      compute nodes!!! The environment must be 
                      uniformly configured!!!
                    - we can send the scripts z01 and z02 to the
                      compute nodes by using ansible:
                ansible compute -b .... (we add the -b to make it
                available to all the users ) CHECK THE REST OF THE
                COMMAND!!!!
                    - if we login with a user, we shoulf see the 
                      module already loaded!!!:
                  - OPENMPI AND RDMA configuration:
                    - remote direct memory access
                    - it is used by infiniband
                    - we apply it only to c02 and c03 that have
                      the infiniband connection
                    - We need to pin some phsical memory to prevent
                      kernel to swap out these memory sections
                    - in this way, IB network devices can write 
                      into memory without the CPU
                    - the pinning operation is a priviledged 
                      operation
                    - the configuration is managed by the files
                      in the folder: /etc/security/limits.d/
                      that must be created on the nodes!!
                    - on c02 and c03 (in this case the master is 
                      intended as the node c02!!!)
                    - I insert in the file rdma.conf:
                     * soft memlock unlimited
                     * hard memlock unlimited
                    - we check the ulimit from the "real" master node:
                      ulimit -a
                  - OPENMPI use INfiniband by default:
                    - we need to add a cofiguration setting in the
                      config file: 
                      $INSTALL_PREFIX/etc/openmpi-mca-params.conf
                    - so we need to do it by making a proper
                      installation
                    - su - install
                    - we add the lines to the file (ASK FOR THE FILE)
                        btl = openib,vader,self
                        btl_openib_allow_ib = 1
                    - ib: means infiniband
              - Testing InfiniBand with LAMMPS_
                 - IMPORTANT!!: we do access with pippo user
                   (that is a test user)
                 - we download resources from Github
                 - we do it on a test user, not on the root, 
                   in particular on pippo user 
                 - we enter in the folder, and we load CMake
                 - we create a build directory, we enter and we do
                 cmake -C ../cmake/presets/minimal.cmake 
                 -D CMAKE_INSTALL_PREFIX=$HOME/opt/lammps ../cmake
                 - compile and install
                 - make -j 12 --> error linked to stdlib.h
                    - WHY????
                    - we try to add the CPATH to the openmpi
                      module that we have created!!
                    - but the issue is still showing up
                    - problem: module file of the GCC --> IT ADDED
                      ALSO THE PATH FOR THE USER AND IT WAS NOT 
                      NEEDED
                 - make install
                 - we test on the node "master" by exporting the
                   environment variables
                 - we copy the exports in a script 
                 - we do a source of the script
                    - it worked and set the env. varibles
                 - we launch the in.melt from 
                   lammps/examples/melt in serial mode
                    - the result is what we expected!!
                 - Now in parallel with mpirun:
                    - 1 by 2 by 2 MPI processor grid: this means
                      4 processes!
                    - Warning: is due to the fact that we are 
                      running on the "master", that does not have
                      the infoniband connection!!!
                 - RUNNING LAMMPS ON CLUSTER:
                    - We launch more LAMMPS jobs and we increase 
                      the size of the grid of 8 times (number of
                      atoms)
                    - we use torq to run the job
                    - we need to add to the job the line to tell that we refer to c02 and c03!!! (ASK!!!)
                    - WE HAVE AN ISSUE --> WHEN RUNNING THE JOB, IT SEEMS THE 2 NODES ARE NOT COMMUNICATING BETWEEN EACH OTHER!!!! 
                    - WE want to run the job a number of processes equal to: 3, 6, 12, 24!!!
                    - WE WRITE FERNANDO ABOUT THE PROBLEM!!!!! 
                    - SOLUTION:
                        - WE SET UNLIMIT memory FOR c02 and c03 --> BUT WE RUN THE BATCH FROM THE REAL master node
                        - IT WORKED BY CHANGING THE LIMIT ALSO IN THE MASTER NODE --> IT SEEMS THAT THE RUNNING OF THE JOB IN A WAY INHERITS 
                          OR USES, THE LIMITS OF THE RUNNING NODE, DESPITE THE FACT THAT THE COMMUNICATION BETWEEN THE COMPUTE NODES IS 
                          ON GOING WITH INFINIBAND!!!

                    

                 - IP over InfiniBand (IPoIB):
                    - we can modify the address assigned to the port (ib0) and to assign the address o the infinband (192.168.32.1/20)
                    - so we modify the interface and we do a ping of the port: 192.168.33.1 to verify the infiniband connection!!
                    - We can run ip -a we can see a new device, that is the device corresponding to the infiniband
                    - from the "master" we can give a IP address static via DHCP --> in the ifcfg-ib0 we insert: 
                    - ip a : gives ib0 is down
                    - so we add the address statically:
                        - from c02 (master) I add the ip address for the master 192.168.48.1/20 (the subnet)
                        - then I add the address for the c03 (the node)  --> 192.168.49.1/20
                        - with ib0 at the end 
                        - THEN IT WORKED!!!!
                    
        





TMAX :
    - can be really useful
    - srun --pty bash -i --> to do changes to the shell??
    - we can share our terminal with someone
    - for system files: do not use vim, use less!!! So we don't modify files!!
    - view : read only
    - vim -R : read only
    - nano -v : read only
    - pipe command: | grep  ether | awk '{print $2}' | cut -f: -d-3
    - awk: useful for data processing
    $NF : number of words?? 
    - try to use these commands!!

    - Latency: time delay between time frame starts to leave a device and the time the first bit of that frame reaches the destination
        - we have software , circuit, media delays (physical matter)
    - One way latency: is nor related to the amounr of data used
        - 0 size
    - bandwidth: theoretical maximum data trasnsfer capacity of a network
    - traceroute : to check where network arrives and sends messages
    - VOIP: we use it for phone because we need low latency!!
    - Throughput:
        - amount of data we can transmit in real world context!!
        - in a perdiod f time
        - best case: bandwith = throughput
        - Performance: 
            - T = time = size/throughput
            - benchmarking network performance
            - Sometimes it is more useful to send data physically if the th size is really huge!!!
            - a track needs less time than Network!!
            -  
        - 
 
    
 





Storage course:

- 2nd lesson:
    - it's not jus doncifentiality, also integribilty
    - it is not guaranted that data has not been intercepted 
    . cases of interente providers injedting adds on website because cmoning throough routers --> confidentiality affected with no intention
    - TLS and SSH
        - TLS: laid security
            - we can encapsulate whatever we want in it
            - additional signature -> the key must belong to a certain user!! 






AFS module version problem:
- when we try to upgrade, we receive an error
- checking the logs, we get:
    - In file included from rx_kmutex.c:24:
/var/lib/dkms/openafs/1.8.10/build/src/afs/LINUX/osi_compat.h: In function grab_cache_page_write_begin:
/var/lib/dkms/openafs/1.8.10/build/src/afs/LINUX/osi_compat.h:168:12: error: implicit declaration of function __grab_cache_page; did you mean read_cache_page? [-Werror=implicit-function-declaration]
 168 |  return __grab_cache_page(mapping, index);
   |      ^~~~~~~~~~~~~~~~~
   |      read_cache_page
/var/lib/dkms/openafs/1.8.10/build/src/afs/LINUX/osi_compat.h:168:12: warning: returning int from a function with return type struct page * makes pointer from integer without a cast [-Wint-conversion]
 168 |  return __grab_cache_page(mapping, index);
   |      ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
cc1: some warnings being treated as errors
make[6]: *** [/usr/src/linux-headers-6.17.0-14-generic/scripts/Makefile.build:287: rx_kmutex.o] Error 1
make[5]: *** [/usr/src/linux-headers-6.17.0-14-generic/Makefile:2016: .] Error 2
make[4]: *** [/usr/src/linux-headers-6.17.0-14-generic/Makefile:248: __sub-make] Error 2
make[4]: Leaving directory '/var/lib/dkms/openafs/1.8.10/build/src/libafs/MODLOAD-6.17.0-14-generic-SP'
make[3]: *** [Makefile:248: __sub-make] Error 2
make[3]: Leaving directory '/usr/src/linux-headers-6.17.0-14-generic'
FAILURE: make exit code 2
make[2]: *** [Makefile.afs:283: openafs.ko] Error 1
make[2]: Leaving directory '/var/lib/dkms/openafs/1.8.10/build/src/libafs/MODLOAD-6.17.0-14-generic-SP'
make[1]: *** [Makefile:188: linux_compdirs] Error 2
make[1]: Leaving directory '/var/lib/dkms/openafs/1.8.10/build/src/libafs'
make: *** [Makefile:15: all] Error 2
DKMSKernelVersion: 6.17.0-14-generic
Date: Fri Feb 6 16:13:31 2026
DuplicateSignature: dkms:openafs-modules-dkms:1.8.10-2.1ubuntu3.4:/var/lib/dkms/openafs/1.8.10/build/src/afs/LINUX/osi_compat.h:168:12: error: implicit declaration of function __grab_cache_page; did you mean read_cache_page? [-Werror=implicit-function-declaration]

- The error above is linked to the version of openafs, that should be updated.
- So we try to update manually by downloading the .tar.gz file and untaring it (tar -xzvf openafs-1.8.15pre1-src.tar.gz)
- we then run the command to configure it (inside the folder get from the untar) :
    ./configure --enable-transarc-paths=no --with-linux-kernel-headers=/usr/src/linux-headers-$(uname -r)
- Then make -j 10
- Eventually we run:
    make install
- TO CHECK THE INSTALLATION:
    - /usr/local/bin/fs --version --> if we get an error, we run: sudo ldconfig (this command makes the system to link the new installed libraries)
    - sudo modprobe openafs (this command loads the afs module) --> if gives error --> it means that the secure boot is enabled! 
        --> disable it (from the bios) and rerun the command
    - check openafs is active as a service: sudo systemctl status openafs-client
NOOOOOOOOOOOOOOOOOOOOO!!!!!!!!!!: THIS IS SO RISKY!!! 







ADVANCED GPU COURSE:

- COKA cluster access: ssh miovine@coka.fe.infn.it
- Alessandro Gabbana
- port the code to makeit run on GPU and optimize it with the hpc devices
- performance enhance and how to do it
- potential things to work on --> day after day increment performance and analyze --> final presentation
- list of things to take into considerations
- Outline
    - exascale systems are based on GPU nowadays --> wasn't true 10 years ago
    - more and more GPUs
    - INTEL Xeon 5 : marconi cluster based on it!
    - from single core oriented to parallel programming 
    - a lot of ALUs to compute efficiently
    - 2007: CUDA --> extention of c - c++
        - cuda only for GPU
- GPU: bandwidth is larger than CPUs tipically
- architecture:
    - many light cores
    - smaller memory
    - high BW to memory
    - parallelizable
    - CPU: I launch program on CPU and iI manage to synchronize with the GPU!!
    - GPU: number of proc. elements order of 1000
    - fraction dedicated to memory management more developed on CPU than GPU
    - streaming multiprocessor for gpus are like cores for CPUs?
    - GPU: much simpler structure
    - programming model:
        - CPU orchestrate kernel and launching --> allocate memory on the GPU
        - transfer data to the GPU (bottleneck) (copy of data)
        - kernel execution in parallel
        - GPU threads: slow access to global and texture memory
            - each threads has registers (faster) and local memory (slow)
        - GPU as separate memory but Unified virtual address and UM (unified memory)
        - Hide latency:
            - we can have access directly from a GPU to another on the same node
    - Flynn's Taxonomy:
        - classification based on how instruction and data are combined
            - SIMD: same instruction to multiple elements
                - in one clock with vectorize execution
                - GPU is similar to this except for the fact that we have different threads on different data and also branches inside
                - SIMT: single instruction multiple threads
                    - threads run in parallel on different pieces of data
                    - single instruction
                    - warp: 32 threads execute the same intruction on different data
                    - threads in a warp can diverge if they meet cond. branches, but effect has been mitigated with last versions
            - CPU: optimized for latency
            - GPU: optimized for throughput --> we can pipeline at large scale
                - we can have multiple streams, each one with multiple threads!!
                - low per thread performance
                - amount of memory is limited (handle transfer between CPU and GPU)
                - Extension card??
            - Nvidia developed its own CPU ARM based
    - GPU approaches:
        - Libraries
        - Directives:
            - OpenMP
            - OpenACC --> pragma based languages
                - copin (I copy before the execution)
                - produces the code to be run on GPU
            - Programming languages:
                - CUDA
                - OpenCL
                - ROCm: equivalent of CUDA for AMD
                - HIP
            - CUDA programming model:
                - extension of c language
                - host: CPU
                - device: GPU
                - we need to take care of allocation of memory:
                    - cudaMalloc(pointer_to_somth_living_on_GPU, size)
                    - cudaFree(pointer_to_somth_living_on_GPU)
                    - cudaMemcpy : copy from CPU to GPU!
                    - cuda Kernel:
                        - __global__ : qualifier specific for CUDA --> kernel will be exec. on GPU and called from the Host
                        - __device__ : qualifier specif. function executed on the device and called within the device only (can be called inside
                                       a kernel???)
                        - __host__ : executable only on the host!!
                        - we cannot combine __host__ and __device__, but we can combine them with __global__
                        - mandatory parameters for kernels:
                            - nthr: num. of threads per block
                            - nblck: number of blocks???
                            - Threads, blocks, grids
                                - We count from left to right, and from bottom to top
                            - Block and grid sized depend on the hardware
                            - grid size:
                                - not easy to define what is the right choice for block and grid size
                         - EXAMPLE:
                            - axpy.cu : axpy operation : a*x + y
                            - .cu : openMP program done on CPU??
                            - load Nvidia compiler:
                                - nvcc
                            - add the cuda_runtime.h header file
                            - add the gpu axpy kernel, naming it gpu_axpy()
                            - function similar to ualloc()
                            - sm_XX : depends on the target GPU architecture!! We need to search it!!
                            - launch the kernel: it is asynchronous
                            - we are measuring just the overhead for launching the kernel
                            - CUDA stream: 
                                - cudaMemcpy: implies the synchronization --> so we need to do synchronization, to force it!!!
                                    - it is important to get the actual execution time
                                - different element in kernel will be devoted to computation for different chunks  of data!!
                                - different threads can handle different part of my data?
                            - We need to optimize memory access
                                - we want that contiguous threads to access contiguous memory loactions!!
                                - CPU: increase data locality
                                - GPU: different approach --> 
                                - in this way perf. ijncreased
                                - but we can have more improvments: --> increasing the number of blocks!!!
                                - we fix the number of threads and recompute the number of blocks
                                - start defining a strategy to evaluating every time the code !!!!! 
                                - for the moment we do it incrementally!
                            - Error checking:
                                - to understand where the problem comes from
                                - cudaGetErrorString(...)
                                - axpy program!!
                        - EXAMINATION:
                            - optimization:
                                - loop unrolling
                                - compiler fla
                                - use of shared memory and tiling techniques
                                - memory coaleshing and data layout
                                - register usage optimization and occupancy tuning
                                - asynchronous execution and overlapping data transfers with computation
                            - small presentation (some slides, screenshots, parts of code)
                                - the story of what we've been doing in the last 4 days
                                - chritical thinks --> why things works or not?? --> analyze, try, understand!!!
                            - possible Projects:
                                - Lattice Boltzmann Method:
                                    - Boltzmann equation on a grid layer --> restricted to small set of directions --> integrating things over a
                                      stencil can ....
                                    - it is extremely suitable for parallel architecture --> updates each point independently with respect to 
                                      each other!!
                                    - we apply collition operator!! 
                                    - precollision to postcollision state!
                                    - all the grid can be computed in parallel
                                    - grid from which we read and grid from which we write
                                    - we can visualize the solution --> allows dissipation --> vorices decay in time (how kinetic energy varies
                                      with time)
                                - 1D Fast Fourier Transform:
                                    - compute in python after porting from c in order to verify the correctness of the result 
                                    - cuFFT library
                                - Jacobi Solver
                                    - he wants also a Hybrid MPI - GPU approach, and also a mandatory comparison between OpenAcc...
                                - Simplest ones: Game of life, Mandlebrot, Julia Set --> mandatory Hybrid MPI - GPU computing
                                
                       - Define the PROJECT which we want to work on!!!

                       - HOW TO RUN JOB ON COKA??:
                            - github Nvidia : the version is too new for Coka !! (the instructions don't work)
                            - latest version on CUDA: 11.7 for COKA
                            - we run with: ./
                            - to run python code to check code: load anaconda3(whatever) !!!
                            - download the release corresponding to the 11.8 version on Cuda
                            - go into the folder untared --> make -j 8 --> after the compiling, go to the bin --> arrive to the executable
                              --> run from a login node to get the result
                            - BUT WE HAVE A COMPILING ERROR!!!:
                                srun --partition=LONGRUN -N 1 gres=gpu
                                (ask chat gpt how to run an interactive section on a gpunode)
                                --> then we can do: ./devQuery??
                                IF WE DO IT FROM A NOT LOGIN NODE, WE GET: 
                                ./deviceQuery Starting...

                                 CUDA Device Query (Runtime API) version (CUDART static linking)

                                 cudaGetDeviceCount returned 100
                                 -> no CUDA-capable device is detected
                                 Result = FAIL
                            - WE DON'T CARE ABOUT THE COMPILING ERROR BECAUSE THE BIN FOLDER WAS GENERATED
                                - WE GO TO THE SUBDIRECTORY release
                                - we at first check the list of nodes: sinfo -l
                                - from the available partitions, we choose longrun, that we know it is GPU provided!!
                                - we access to a login node to have an interactive session with slurm:
                                
                                srun --partition=longrun -N 1 --gres=gpu:1 --pty bash
                                
                                - we do it in the folder "release"
                                - we run ./deviceQuery to get the informations about the GPU on the longrun partition
                                - now we can do exit (to come back to the login node) --> this time we start an interactive session Slurm on the
                                  partition Skyvolta:
                                
                                srun --partition=skyvolta -N 1 --gres=gpu:1 --pty bash
                                
                             - WE CAN REPEAT THE PROCEDURE ON LEONARDO!!
                             - in the script of the p100, we need to change 0 with 2!!!!
                                    - in EXPORT_......
                             - we at fist run the P100 --> so the flag sm_XX will be equal to: sm_60 !!
                                - after compiling --> we check what partition is needed checking the slurm script in the coka_script scripts
                                - we see that we need to run with: skyvolta !
                                - we login on the skyvolta node partition --> we run: ./axpy $((1024*1024*64))
                             - K80 flag:
                                - -gencode arch=compute_37,code=sum_37
                             - RUNNING THE P100 WITH SYNCHRONIZE AND THREADS= 512, WE GET THAT THE GPU HAS INCREDIBLE PERFORMANCES
                                - THIS HAPPENS BECAUSE THE FLAGS ARE NOT CORRECT FOR RUNNING THE CODE --> IT IS RUN THE V100 INSTEAD OF THE P100:
                                - IT IS LIKE THE KERNEL IS NOT LAUNCHED AT ALL!!!
                                
                            - vpn.sissa.it/sso
                            
                            - SECOND DAY: Memory Architecture and optimization:
                                - GPU consists of many streaming multiprocessors
                                - minimal computing unit: thread
                                - each thread block is mapped to a streaming multi processor (SM)
                                - CUDA can execute several threads simultaneously
                                - 32 consecutive elements form a warp
                                - threads within the same block can be assumed to run concurrently
                                - we can have many registers and many schedulers
                                - SM divided in processing blocks
                                - we want to access the global memory as little as possible
                                - register (per threads) --> shared memory (per block) --> L1 cache (per SM) --> L2 cache (global) --> global mem
                                  --> CPU host memory
                                - Warps: 
                                    - are distributed among the processing blocks
                                    - GPU use SMT : single instruction multiple threads
                                    - scheduler pick one of the thread warp for executing instruction
                                        --> can happen every thread in a warp execute the same instruction
                                        --> or some kind of branching or latency
                                        --> for example : if --> some threads execute an instruction, other ex. other ones
                                        - in gpus branching is fine (it is not like CPU) --> especially for modern GPUs
                                        - the branch is better if occurs at block level, and not at warp level
                                        - we are not talking about how we access the memory, but 
                                        - tipically it is preferable that the number of elements in a block being multiple of 32??
                                        - if a warp becomes idle --> it is like having a cache miss!!!
                                        - WHAT IS THE RELATION BETWEEN : SM, BLOCK, WARP ????
                                - Difference of branch divergence between warp level and block level:
                                    - Between Warp (block level) --> no penalty
                                    - Within Warp (warp level) --> latency ???
                                - Hardware: 
                                    - A100 and H100 are the most recent on Leonardo?
                                    - in any case, if data doesn't fit in memory --> must process in chuncks
                                    - latency: cost to initialize the transfer, the check in the memory and a lot of things
                                    - we are happy when memory accesses are coalesced --> why and what are the implication?
                                        - access patterns align with GPU hardware
                                        - first check in the cache then down and down to the global memory ??
                                        - GPU cache line : 128 Bytes
                                        - COALESCING: 
                                            - accesses are contiguous and aligned across warps
                                            - minimizes memory access
                                            - we assume each thread must access 4 bytes of data --> so for 1 warp (32 thread), we have 128 Bytes
                                              (a cache line)
                                                - there are many ways to exploit this
                                                - worst case: the grey boundary values are the threads??
                                                    - if we don't make access coalescing, we need to load an entire sector for each read???
                                                    - why in the other worst scenario the penalty is negligible??
                                                        - BECAUSE I HAVE STILL ONE TRANSACTION????
                                                        - THREADS ACCESS THE SAME DATA!!!
                                            - Threads in a warp should access contiguous addresses
                                            - access sizes align to cache line boundaries
                                            - avoid strided access
                                            - SO THE LACK OF PERFORMANCE IS ESSENTIALLY LINKED TO THE NUMBER OF ACCESSES TO MEMORY AND TO CACHE
                                              LINES!!!!
                                        - SHARED MEMORY:
                                            - we can use it to minimize the number of memory accesses to the global memory
                                            - up to 2 order of magnitudes faster than the global one 
                                            - bank conflict: same threads accessing the same bank??
                                            - memory is divided into 32 memory banks???
                                            - WHAT IS THE DIFFERENCE BETWEEN WARPS AND BANKS ????
                                            - avoiding bank conflicts: Padding
                                                - add padding and use stride 33 instead of 32 
                                            - HOW to allocate shared memory:
                                                - numBytesShMem
                                        - AoS vs Soa:
                                            - Structure of Arrays:
                                            - AoS : poor cache locality
                                            - we can combine it with shared memory
                                            - in LBM we can switch from SOA to AOS giving a boost of performance!!!!!
                                        - REGISTERS:
                                            - 5 - 10 % impact on performances!!
                                            - fastest memory
                                            - we want to avoid register spilling
                                            - the code is such that needs more registers than hardware limit
                                            - how to avoid it??
                                                - we can take action to avoid it happens!!
                                        - constant memory:
                                                - memory area where constants are stored
                                                - can use it to store constant quantities 
                                        - texture memory:
                                            - the compiler will use it!
                                        - Block-level sinchronization:
                                            - __syncthreads() 
                                            - Deadlock in CUDA????
                                            - Atomic operations!!!
                                                - AVOID USING IT!!
                                        - Reduction:
                                            - we apply operator to vector to reduce it!!!!
                                            - we can do better????
                                            - we can reduce within a block by using shared memory!!!
                                            - we can optimize also for warp divergence and coalesced memory access!!
                                                - no warp divergence
                                                    - no serialization within warps
                                                - coalesced memory access
                                        - WHAT IS THE WARP SCHEDULER????
                                        - WE CAN ESTIMATE OCCUPANCY!!
                                        - active warps: threads_x_block*SM_number?? / 32
                                        - I can use NVIDIA NSIGHT to estimate occupancy!!
                                        - If the theoretical occupancy is higher than the measured one --> probably the memory access is not 
                                          good!!!
                                        - 2 example in slides
                                            - implement kernels and try to optimize gradually it!!! 
                                - 3RD LESSON:
                                    - pinned (page-locked) host memory
                                    - cuda streams
                                    - cuda events
                                    - cuda graphs
                                    - integration with OpenACC
                                        - Synchronicity and Von Neumann model
                                            - traditional cpus assumes synchronous execution
                                            - we want to introduce for example overlapping between computation and memory transfer
                                            - blocking vs non-blocking calls:
                                                - we need to synchronize explicitly the memory to wait the host before doing the 
                                                  next calculation
                                        - CUDA streams:
                                            - queue of GPU operations exec. in a certain order
                                            - within a stream op. are serialized
                                            - Default stream: NULL stream --> synchronizes with all user streams
                                                - blocks all user streams
                                                - user streams can run concurrently with default stream
                                            - creating streams:
                                                - create streams
                                                - we specify that the kernel must run on a certain stream
                                                - D2H : device to host
                                                - we can direct a stream to a GPU and another to another GPU
                                                - so we will get asynchronous Data transfers
                                                - how host data is alocated??
                                                    - it is pageable:
                                                        - we want that device moves the data asynchronously --> so this approach is not good
                                                        - CPU cannot DMA from pageable memory
                                                        - GPU must instantiate an intermidiate buffer?? (2 transfers)
                                                        - How to solve this problem?
                                                            --> PINNED MEMORY! : - no CPU intervention
                                                                                 - GPU does a direct DMA
                                                        - why I don't pin all the memory??
                                                            - BECAUSE WE HAVE A MEMORY LIMIT!!!
                                                - we can have 2 concurrent streams for example
                                                - IN OUR PROJECT: MAYBE WE COULD APPLY IT TO THE PRINTING PART INSIDE THE ITERATION LOOP!!
                                                - if we have 2 streams, the time when they will be executed depends on the scheduler
                                                - example: cuBLAS with Streams (libraries) 
                                         - CUDA events:
                                            - synchronization marker in a stream:
                                                - creating dependencies
                                                - measure the time of exec. of a kernel without the need of a synchronization with host
                                                    - a kernel to intialize and one to stop
                                                - inter stream synchronization
                                         - CUDA Graphs:
                                            - snapshot of GPU work
                                       - CUDA Graphs:
                                        - snapshot of GPU work
                                        - good for latency? When we use them?? 
                                        - graph node is an asynchronous cuda operation
                                        - we can introduce dependencies between the nodes 
                                       - OpenACC and Streams:
                                            - we can integrate it with CUDA!!! 
                                            - we can apply it asynchronously!!!! 
                                            - async data transfers
                                       - We want to achieve a pipelined approach!!!
                                            - maximum overlap!!!!
                                       - Async everything: WHY????
                                       - overlapping communication and computation (when we have MPI!!!)
                                            - ghost nodes to store temporary stuffs!!!
                                            - MPI CUDA aware implementation
                                            - to check the performance: NVIDIA profiler !!!!
                                       - How can we debug Streams and Events????
                                            - 
                                - OpenMP:
                                    - directive based 
                                    - to offload the GPU computation
                                    - same code for CPU and GPU
                                    - less porting effort 
                                    - p100 and v100 are not supported by the latest version of OpenMP!!! --> we need to add the proper flag
                                      depending on the compiler!!
                                    - Diff. with OpenACC:
                                        - how we define the region: with "target" --> will specify a region executed on GPU
                                        - if we don't add additional constructs, we will have a single thread executing
                                        - the physical mapping depends on the compiler
                                        - we have threads per team????
                                        - I need to create teams and for each team a certain number of threads!!!
                                        - distribute: it distributes the loop iterations to thethreads!!!
                                        - there is a difference between Teams and Parallel !!!
                                        - from OpenMP 5.0 :
                                            - from extended pragma directive to a more compat one!!!!
                                    - IT IS REALLY IMPORTANT TO CHECK WHAT THE COMPILER IS ACTUALLY DOING!! (TO HAVE THE CONTROL ON IT!!!)
                                        - by adding the flag: -Minfo !!!!!
                                    - Data Mapping:
                                        - handle data movement between host and device
                                        - data map construct that allows to set how data should be handled in different regions of the code
                                    - target regions can be dependent or indipendent!! 
                                    - we can handle data also outside the data region (unstructured data regions)
                                    - we can also make use of the shared memory!!!
                                    - default: all the call to the kernels are blocking
                                        - asynchronous execution:
                                            - we need a nowait
                                    - we can define openMP tasks that will be executed concurrently!!
                                        - depend keyword
                                    - we can integrate with MPI!!!     
                                    


- PROJECT (LATTICE BOLTZMANN):
    - error: there is a missing ) in the file : it is due to the fact that the NX and NY variables are aleady defineed at the top of the file!!!!
    - the same happened also for the variable Q!!
    - we cannot call the kernels in the same way of c or c++ functions: we need to specify the threads configuration with: <<,>>
    - ATTENTION!!!!: ALSO THE grid array must be copied into the Device, because it is at the beginning of the process initialized with a proper
      function!!!
    - WE NEED TO CORRECT THE CALCULATION OF FLOATING POINT OPERATIONS AND AS WELL THE NUMBER OF INPUT-OUTPUT ACCESSES!!!!!                        
        - OTHERWISE THE ESTIMATION OF PERFORMANCES IS NOT CORRECT!!!
    - ATTENTION!!!:
        - When we call a kernel in CUDA, and we pass the parameters in "<<< >>>", we are passing the first as the number of blocks,
          and the second one as the number of threads per block : remember that this represents the logical thread topology and doesn't
          necessarily correspond to the data layout -->> and, more important, it doesn't correspond to the hardware configuration (rememeber the
          banks issue for Shared Memory!!)
    - IN OUR PROJECT AT FIRST WE DIDN'T INTRODUCED ANY BLOCK OR THREAD TOPOLOGY, SO WHAT HAPPENS IS THAT EACH THREADS EXECUTE THE ENTIRE 
      FOR LOOPS AND EACH THREADS ACCESS THE SAME MEMORY ADDRESS IN THE LOADED CACHE LINE FOR A SINGLE CACHE LINE TRANSACTION!!!
    - SO WHAT WE DO NOW IS TO INTRODUCE A COALESCED MEMORY ACCESS!!!
        - in particular in the stream and collision kernels !!!
        - but the coalesced or not coalesced access seems very linked to the difference between SOA and AOS
    - AFTER ALMOST A CENTURY, WE CAN MOVE TO THE COALESCED MEMORY ACCESS VERSION!!!:
    - NOW WE WANT TO INTRODUCE THE SoA paradigm:
        - this doesn't mean that we have a struct with an array inside and that we need to refer to the array with the dot notation as we do now
        - it means that the variables defined in the struct are directly accessible through pointers, without the need to load the entire
          struct instance !!!
        - so we define the variable inside the struct as a pointer
        - then what we do is to d
        - ATTENTIION!!! : IN C WE CANNOT CALL A FUNCTION INSIDE THE STRUCT DEFINITION!! --> WE NEED TO ALLOCATE ON THE HEAP WHEN WE INTIALIZE!!!
        - ATTENTION!!!!!!: at first we wanted to modify the struct on the host by introducing the static array as a pointer (array) in order
          to handle the allocation of of an array of NX*NY elements for each fi !!!
            - so we did a copy from the host to the device of the new defined grid --> since we pass from array of structs to struct of pointers,
              we defined the grid as a simple struct object with inside the arrays corresponding to each fi in every grid point
            - Unfortunately, it is not possible to copy a struct instance to the device and pretend that the GPU (the device) can manage
              them : this is totally wrong!!!
            - so we need to copy each f, as member of the struct instance, as an array of doubles of dimension NX*NY!!!
            - ATTENTION!!: WE NEED TO TAKE CARE PROPERLY OF THE DEALLOCATION!!!
            - ATTENTION!! PROBABLY THE SWAP MUST BE DONE JUST BETWEEN THE POINTERS MEMBER OF THE struct!!!
            - WE CREATED A NEW VERSION WITH THE init and Kinetic energy functions called by reference, BUT IT SEEMS SLOWER THAN THE 
              PREVIOUS ONE!!!
        - ATTENTION!!!:
            - WE CAN TAKE THE TIMES IN A BETTER WAY BY INTRODUCING THE TIMINGS BEFORE AND AFTER THE 2 CALLS TO THE KERNELS AND THEN ADD A 
              BARRIER TO MAKE SURE THE SYNCHRONIZATION IS CORRECT!!!!
        - ATTENTION!!!:
            - When I apply the SOA: 
                - The call to the kernels, must be done in such a way that the d_grid is passed by value, not by reference --> why?
                - We pass by value because d_grid and d_grid_new in this case are defined on the host --> but the pointers inside them
                  are obtained by copying from Host to Device 
                - So, if we make the code to do a copy of the struct instances, then it will be able to dereferntiate the pointers to GPU 
                  locations
                - If we pass by reference, we pass a Pointer to the Host to the GPU --> so it will not work!!!
           - For constant memory access:
                - the __const__ qualifier must be added to the gpu arrays, not to the Host ones, because the __constant__ variables
                  are physically allocated on a specific location in a special memory on the GPU!!!
                - REMEMBER: the __constant__ variables must live in the global scope, not in a host function scope (like main, for example)
                - ATTENTION!!!: WE CANNOT CALL ANYMORE THE collision and streming kernels passing the dex, dey and dw as arguments!!!
                  they are globally defined, so by explicitly passing them, we pass undefined arrays!!
                  - WE NEED TO REMOVE THE ARRAYS ALSO FROM THE KERNEL SIGNATURES AND FROM THE CUDA FREE LINES!!
           - we could make a comparison between block size 32*32 and 16*16!!
           - ATTENTION!!!: THERE IS A HUGE DIFFERENCE BETWEEN THE THREADS IN CPU AND GPU!!!
                - ON GPU, THE THREADS ARE UNIT OF EXECUTION!!!!!!!!!
           - WARNING!!!!:
                - WE SET THE NUMBER OF THREADS SO THAT EACH GRID POINT CORRESPONDS TO A THREAD!!!! 
                  --> THIS IS WHY WE HAVE 
                  NUM. OF BLOCK = grid_total_number_nodes/threads_per_block
                - BUT YOU NEED TO KEEP IN MIND THAT IT'S NOT A RULE, WE COULD ASSIGN TO A THREAD MORE ELEMENTS
           - SHARED MEMORY:
                - WE DEFINE THE DOMAIN AS MADE OF TILES!!!! --> EACH TILE SHARES MEMORY!!!
           - WE COULD PROBABLY MOVE THE FOR LOOP ON THE fi COMPONENTS OUTSIDE AND THE DOMAIN FOR LOOP INSIDE!!!
        - ATTENTION!!!
            - Our first implementation of the kernels is not with memory coalesced --> on the other hand, after applying SOA, we have 
              memory coalesced!!!
            - ReCALL THE CUDA STREAMS!!!!
















































